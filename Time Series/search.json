[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Visualization",
    "section": "",
    "text": "From the line graph of total petroleum consumption by sectors, we can clearly see the percentge of transportation petroleum consumption has increased from 52% (6,152 trillion Btu) to 70% (26,179 trillion Btu) in 73 years, which is due to the significant increase of automobile usage. The second largest petroleum consumption sector is industrial sector, which increased from 3461 trillion Btu to 9146 trillion Btu over the last 73 years. The third largest petroleum consumption sector is residential sector, which actually decreased from 1107 trillion Btu to 981 trillion Btu. Commercial sector and and electric power sector are the two least petroleum consumption sectors. Commercial sector increased from 735 trillion Btu to 909 trillion Btu, whereas electric power sector decreased from 415 trillion Btu to 243 trillion Btu. Residential sector and electric power sector are the only two sectors which decreased since 1949."
  },
  {
    "objectID": "dv.html#data-visualization-with-stock-data",
    "href": "dv.html#data-visualization-with-stock-data",
    "title": "Data Visualization",
    "section": "Data Visualization with Stock Data",
    "text": "Data Visualization with Stock Data\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"AAPL\",\"TSLA\",\"AMZN\" )\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-10-01\",\n             to = \"2022-12-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(AAPL$AAPL.Adjusted,\n                    TSLA$TSLA.Adjusted,\n                    AMZN$AMZN.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\nhead(stock)\n\n               AAPL     TSLA    AMZN      Dates       date\n2012-10-01 20.16033 1.944000 12.6005 2012-10-01 2012-10-01\n2012-10-02 20.21902 1.986667 12.5300 2012-10-02 2012-10-02\n2012-10-03 20.52905 1.953333 12.7960 2012-10-03 2012-10-03\n2012-10-04 20.38688 1.960000 13.0235 2012-10-04 2012-10-04\n2012-10-05 19.95243 1.926000 12.9255 2012-10-05 2012-10-05\n2012-10-08 19.51154 1.950000 12.9530 2012-10-08 2012-10-08\n\n\n\nggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n   labs(\n    title = \"Stock Prices for the Tech Companies\",\n    subtitle = \"From 2013-2022\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))+\n    guides(colour=guide_legend(title=\"Tech Companies\")) \n\n\n\n\n\nHover over the plot to see the difference.\n\ng4<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n   labs(\n    title = \"Stock Prices for the Tech Companies\",\n    subtitle = \"From 2013-2022\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Tech Companies\")) \n\n\nggplotly(g4) %>%\n  layout(hovermode = \"x\")"
  },
  {
    "objectID": "dv.html#stock-data",
    "href": "dv.html#stock-data",
    "title": "Data Visualization",
    "section": "Stock Data",
    "text": "Stock Data\n\n\n\n\n\n\n\n\n\n\n\nHover over the plot to see the difference.\n\n\n\n\n\n\n\nThe above graph, using data from Yahoo! Finance, depicts the stock prices of Google (blue), Bath and Body Works (red), and Ford (green). While all three companies’ stock prices observed an upward trend, stock prices were also subjected to seasonal trends consistent with the overall economic well being of the country. The clearest evidence of this is that in March 2020 all three stocks fell 30-50% in response to the outbreak of the COVID-19. As the country learned to live with the pandemic, all stocks rebounded quickly and continued to grow until fears of high inflation and a potential recession caused stocks to generally return to the mean, i.e. return to the trend they found themselves on before the pandemic. Ford, being the oldest and best established company depicted, was more resilient to seasonal stock fluctuations like Google and B&BW. B&BW, more so than the other companies listed, experienced noticeable lifts to its stock price every holiday season because the company’s products are often given as gifts."
  },
  {
    "objectID": "dv.html#bitcoin-plot-using-plotly",
    "href": "dv.html#bitcoin-plot-using-plotly",
    "title": "Data Visualization",
    "section": "Bitcoin plot using plotly",
    "text": "Bitcoin plot using plotly\n\nbitc <- getSymbols(\"BTC\",auto.assign = FALSE, from = \"2021-09-15\",src=\"yahoo\") \nhead(bitc)\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n\nstart(bitc)\n\n[1] \"2021-09-15\"\n\nend(bitc)\n\n[1] \"2023-01-18\"\n\n\n\nbitc=data.frame(bitc)\nbitc <- data.frame(bitc,rownames(bitc))\nhead(bitc)\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n           rownames.bitc.\n2021-09-15     2021-09-15\n2021-09-16     2021-09-16\n2021-09-17     2021-09-17\n2021-09-20     2021-09-20\n2021-09-21     2021-09-21\n2021-09-22     2021-09-22\n\ncolnames(bitc)[7] = \"date\"\nhead(bitc)\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n                 date\n2021-09-15 2021-09-15\n2021-09-16 2021-09-16\n2021-09-17 2021-09-17\n2021-09-20 2021-09-20\n2021-09-21 2021-09-21\n2021-09-22 2021-09-22\n\nbitc$date<-as.Date(bitc$date,\"%Y-%m-%d\")\nstr(bitc)\n\n'data.frame':   338 obs. of  7 variables:\n $ BTC.Open    : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.High    : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Low     : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.Close   : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Volume  : num  102 877 502 110 464 ...\n $ BTC.Adjusted: num  99.7 99.6 99.5 99.6 99.6 ...\n $ date        : Date, format: \"2021-09-15\" \"2021-09-16\" ...\n\n## ggplot\nbitc %>%\n  ggplot()+\n  geom_line(aes(y=BTC.Adjusted,x=date),color=\"blue\")\n\n\n\n## plotly\nfig <- plot_ly(bitc, x = ~date, y = ~BTC.Adjusted, type = 'scatter', mode = 'lines')\n\nfig <- fig %>% layout(title = \"Basic line Plot\")\nfig\n\n\n\n\n\n\ndf <- tail(bitc, 30)\n\nfigc <- df %>% plot_ly(x = ~date, type=\"candlestick\",\n          open = ~BTC.Open, close = ~BTC.Close,\n          high = ~BTC.High, low = ~BTC.Low) \nfigc <- figc %>% layout(title = \"Basic Candlestick Chart\")\n\nfigc"
  },
  {
    "objectID": "dv.html#google-stock-plot-using-plotly",
    "href": "dv.html#google-stock-plot-using-plotly",
    "title": "Data Visualization",
    "section": "Google Stock Plot Using Plotly",
    "text": "Google Stock Plot Using Plotly\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nThe graph above is the stock price of Google from Oct 2021 to Jan 2023. The trend of Google’s stock in this amount of time is shown as decreasing. However, combining the previous graph of the stock prices of Bath and Body Works and Ford, we know that all of them experienced decreasing stock price from Nov 2021. As discussed in the previous paragraph, as the country learned to live with the pandemic, all stocks rebounded quickly and continued to grow until fears of high inflation and a potential recession caused stocks to generally return to the mean."
  },
  {
    "objectID": "dv.html#climate-data-plot-using-plotly",
    "href": "dv.html#climate-data-plot-using-plotly",
    "title": "Data Visualization",
    "section": "Climate Data Plot Using Plotly",
    "text": "Climate Data Plot Using Plotly\n\n\n\n\n\n\n\n\n\n\nThe above graph shows the observed temperature at two locations in the D.C. area from the beginning of January 2021 to the end of September 2021. The temperatures at the time of observation are very similar because of the geographic proximity of the two locations, however the temperature observed at Dalecarlia Reservoir is consistently lower than the temperature at the National Arboretum. The best explanation for this is that the water in Dalecarlia Reservoir has regulatory affect on the surrounding air temperature. The temperature at these locations is largely determined by the regular change of seasons felt throughout the Northern Hemisphere and also is effected by the rapidly changing and hard to predict cycle of hot fronts, cold fronts, daily cloud cover, and extreme weather. The trend of this data is upwards with seasonal fluctuations of 30 or more degrees every 3-5 days."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Here is an article from National Geographic on greenhouse effect:\nHuman activities contribute to global warming by increasing the greenhouse effect. The greenhouse effect happens when certain gases—known as greenhouse gases—collect in Earth’s atmosphere. These gases, which occur naturally in the atmosphere, include carbon dioxide, methane, nitrogen oxide, and fluorinated gases sometimes known as chlorofluorocarbons (CFCs).\n\nGreenhouse gases let the sun’s light shine onto Earth’s surface, but they trap the heat that reflects back up into the atmosphere. In this way, they act like the insulating glass walls of a greenhouse. The greenhouse effect keeps Earth’s climate comfortable. Without it, surface temperatures would be cooler by about 33 degrees Celsius (60 degrees Fahrenheit), and many life forms would freeze.\n\nSince the Industrial Revolution in the late 1700s and early 1800s, people have been releasing large quantities of greenhouse gases into the atmosphere. That amount has skyrocketed in the past century. Greenhouse gas emissions increased 70 percent between 1970 and 2004. Emissions of carbon dioxide, the most important greenhouse gas, rose by about 80 percent during that time. The amount of carbon dioxide in the atmosphere today far exceeds the natural range seen over the last 650,000 years.\n\n\n\nMost of the carbon dioxide that people put into the atmosphere comes from burning fossil fuels such as oil, coal, and natural gas. Cars, trucks, trains, and planes all burn fossil fuels. Many electric power plants also burn fossil fuels.\n\nAnother way people release carbon dioxide into the atmosphere is by cutting down forests. This happens for two reasons. Decaying plant material, including trees, releases tons of carbon dioxide into the atmosphere. Living trees absorb carbon dioxide. By diminishing the number of trees to absorb carbon dioxide, the gas remains in the atmosphere.\n\n\n\n\nMost methane in the atmosphere comes from livestock farming, landfills, and fossil fuel production suchas coal mining and natural gas processing. Nitrous oxide comes from agricultural technology and fossil fuel burning.\n\n\n\n\n\nFluorinated gases include chlorofluorocarbons, hydrochlorofluorocarbons, and hydrofluorocarbons. These greenhouse gases are used in aerosol cans and refrigeration.\n\nAll of these human activities add greenhouse gases to the atmosphere, trapping more heat than usual and contributing to global warming.\n\n\n\nThe climate portal at MIT explains that radiative forcing is what happens when the amount of energy that enters the Earth’s atmosphere is different from the amount of energy that leaves it. Energy travels in the form of radiation: solar radiation entering the atmosphere from the sun, and infrared radiation exiting as heat. If more radiation is entering Earth than leaving—as is happening today—then the atmosphere will warm up. This is called radiative forcing because the difference in energy can force changes in the Earth’s climate.\n\n\n\nPublished September 25, 2020. This Explainer was adapted from “Explained: Radiative Forcing” by David Chandler, which originally appeared in MIT News.\n\n\n\n                   \n\nFrom the graph above, we know that carbon dioxide has the most contribution to global heating imbalance. Methane has the second most contribution to global heating imbalance. Then they are chlorofluorocarbons, nitrous oxide, hydrochlorofluorocarbons, and hydrofluorocarbons."
  },
  {
    "objectID": "intro.html#top-navigation",
    "href": "intro.html#top-navigation",
    "title": "Introduction",
    "section": "Top Navigation",
    "text": "Top Navigation"
  },
  {
    "objectID": "intro.html#business-questions",
    "href": "intro.html#business-questions",
    "title": "Introduction",
    "section": "Business Questions",
    "text": "Business Questions\n\nWhat is the trend of states’ petroleum consumption, price, and sales over the years?\nWhat is the trend of states’ coal consumption, price, and sales over the years?\nWhat is the trend of states’ electricity consumption, price, and sales over the years?\nWhat is the trend of states’ natural gas consumption, price, and sales over the years?\nWhat is the trend of states’ CO2 emissions over the years?\nWhat is the energy import and export situation nationally and internationally over the years?\nWhat is the trend of states’ cost and savings from energy efficiency programs?\nHow energy industry cause global warming?\nWhat information about energy would exploratory data analysis reveal?\nWhat information about energy would deep learning with time series reveal?\nWhat information and prediction would be revealed from the different time series models?\nWhat conclusion would be made based on the energy consumptions and model predictions?\n\n\n\n\nA coal-hauling truck with 240 tons of coal drives to the surface at the Buckskin Coal Mine in Gillette, Wyo., in 2004. Robert Nickelsberg/Getty Images"
  },
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "The majority of the data sources are gathered from the API of EIA.\nSee the code"
  },
  {
    "objectID": "data_sources.html#energy-information-administration",
    "href": "data_sources.html#energy-information-administration",
    "title": "Data Sources",
    "section": "Energy Information Administration",
    "text": "Energy Information Administration\n\n\n\n\n\nThe U.S. Energy Information Administration (EIA) is a principal agency of the U.S. Federal Statistical System responsible for collecting, analyzing, and disseminating energy information to promote sound policymaking, efficient markets, and public understanding of energy and its interaction with the economy and the environment. EIA programs cover data on coal, petroleum, natural gas, electric, renewable and nuclear energy. EIA is part of the U.S. Department of Energy.\n\n\nEIA API\nThe U.S. Energy Information Administration is committed to its free and open data by making it available through an Application Programming Interface (API) and its open data tools. EIA’s API is multi-facetted and contains the following time-series data sets organized by the main energy categories.\n\n\n\nU.S. Annual Petroleum Consumption\n\n\n\n\n\n\n\nperiod\nmsn\nseriesDescription\nvalue\nunit\n\n\n\n\n0\n2022\nPAICBUS\nIndustrial Sector\n9146.490\nTrillion Btu\n\n\n1\n2022\nPAEIBUS\nElectric Power Sector\n242.923\nTrillion Btu\n\n\n2\n2022\nPARCBUS\nResidential Sector\n980.627\nTrillion Btu\n\n\n3\n2022\nPAACBUS\nTransportation Sector\n26178.987\nTrillion Btu\n\n\n4\n2022\nPACCBUS\nCommercial Sector\n908.598\nTrillion Btu\n\n\n5\n2021\nPAACBUS\nTransportation Sector\n25778.031\nTrillion Btu\n\n\n\n\n\n\n\n\n\nState Annual Coal Consumption\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nlocation\nstateDescription\nsector\nsectorDescription\nconsumption\nprice\nconsumption.units\nprice.units\n\n\n\n\n0\n2021\n81\nMiddle Atlantic\n1\nElectric Utility\n0\nNA\nshort tons\ndollars per short ton\n\n\n1\n2021\nWY\nWyoming\n98\nElectric Power\n20013958\nNA\nshort tons\ndollars per short ton\n\n\n2\n2021\nWV\nWest Virginia\n98\nElectric Power\n24250094\nNA\nshort tons\ndollars per short ton\n\n\n3\n2021\nWI\nWisconsin\n98\nElectric Power\n15407584\nNA\nshort tons\ndollars per short ton\n\n\n4\n2021\nWA\nWashington\n98\nElectric Power\n2119522\nNA\nshort tons\ndollars per short ton\n\n\n5\n2021\nVA\nVirginia\n98\nElectric Power\n1481724\nNA\nshort tons\ndollars per short ton\n\n\n\n\n\n\n\n\n\nState Annual Coal Market Sales Price\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nstateRegionId\nstateRegionDescription\nmarketTypeId\nmarketTypeDescription\nprice\nsales\nprice.units\nsales.units\n\n\n\n\n0\n2021\nAPP\nAppalachia Total\nTOT\nTotal\n78.63\n156097454\ndollars per short ton\nshort tons\n\n\n1\n2021\nAPP\nAppalachia Total\nOM\nOpen Market\n58.67\n96954333\ndollars per short ton\nshort tons\n\n\n2\n2021\nAPP\nAppalachia Total\nCAP\nCaptive\n125.02\n2883742\ndollars per short ton\nshort tons\n\n\n3\n2021\nAK\nAlaska\nTOT\nTotal\n60.32\n1023740\ndollars per short ton\nshort tons\n\n\n4\n2021\nAK\nAlaska\nOM\nOpen Market\nw\n1023740\ndollars per short ton\nshort tons\n\n\n5\n2021\nPCN\nPacific Noncontiguous\nTOT\nTotal\n60.32\n1023740\ndollars per short ton\nshort tons\n\n\n\n\n\n\n\n\n\nState Annual Electricity Net Metering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nstate\nstateName\ntechnology\nsector\nsectorName\ncapacity\ncustomers\ncapacity.units\ncustomers.units\n\n\n\n\n0\n2021\nWA\nWashington\nAll Technologies\nIND\nindustrial\n0.661\n21\nmegawatts\nnumber of customers\n\n\n1\n2021\nWV\nWest Virginia\nAll Technologies\nCOM\ncommercial\n4.656\n174\nmegawatts\nnumber of customers\n\n\n2\n2021\nWI\nWisconsin\nAll Technologies\nCOM\ncommercial\n59.939\n1446\nmegawatts\nnumber of customers\n\n\n3\n2021\nWA\nWashington\nAll Technologies\nCOM\ncommercial\n39.339\n2187\nmegawatts\nnumber of customers\n\n\n4\n2021\nVT\nVermont\nAll Technologies\nCOM\ncommercial\n56.471\n689\nmegawatts\nnumber of customers\n\n\n5\n2021\nVA\nVirginia\nAll Technologies\nCOM\ncommercial\n69.769\n927\nmegawatts\nnumber of customers\n\n\n\n\n\n\n\n\n\nState Cost and Savings from Energy Efficiency Programs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nstate\nstateName\ntimePeriod\nsector\nsectorName\nall.other.costs\ncustomer.incentive\nenergy.savings\nall.other.costs.units\ncustomer.incentive.units\nenergy.savings.units\n\n\n\n\n0\n2021\nAK\nAlaska\nExpected Life Cycle of Programs\nRES\nresidential\n8\n0\n1243\nthousand dollars\nthousand dollars\nmegawatthours\n\n\n1\n2021\nMI\nMichigan\nExpected Life Cycle of Programs\nIND\nindustrial\n8432\n16549\n1825442\nthousand dollars\nthousand dollars\nmegawatthours\n\n\n2\n2021\nME\nMaine\nExpected Life Cycle of Programs\nIND\nindustrial\n0\n0\n0\nthousand dollars\nthousand dollars\nmegawatthours\n\n\n3\n2021\nMD\nMaryland\nExpected Life Cycle of Programs\nIND\nindustrial\n572\n1225\n71999\nthousand dollars\nthousand dollars\nmegawatthours\n\n\n4\n2021\nMA\nMassachusetts\nExpected Life Cycle of Programs\nIND\nindustrial\n4009\n14361\n909826\nthousand dollars\nthousand dollars\nmegawatthours\n\n\n5\n2021\nLA\nLouisiana\nExpected Life Cycle of Programs\nIND\nindustrial\n52\n293\n7433\nthousand dollars\nthousand dollars\nmegawatthours\n\n\n\n\n\n\n\n\n\nState Emissions from Energy Consumption\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nstateid\nstateDescription\nfuelid\nfuelDescription\nco2.rate.lbs.mwh\nco2.thousand.metric.tons\nso2.rate.lbs.mwh\nso2.short.tons\nco2.rate.lbs.mwh.units\nco2.thousand.metric.tons.units\nso2.rate.lbs.mwh.units\nso2.short.tons.units\n\n\n\n\n0\n2021\nUS\nUnited States\nNG\nNatural Gas\nNA\n695927\nNA\n4212\npounds per megawatthour\nthousand metric tons\npounds per megawatthour\nshort tons\n\n\n1\n2021\nUS\nUnited States\nCOL\nCoal\nNA\n919308\nNA\n981789\npounds per megawatthour\nthousand metric tons\npounds per megawatthour\nshort tons\n\n\n2\n2021\nTX\nTexas\nALL\nTotal\n941\n206175\n0.7\n163075\npounds per megawatthour\nthousand metric tons\npounds per megawatthour\nshort tons\n\n\n3\n2021\nTX\nTexas\nPET\nPetroleum\nNA\n434\nNA\n1152\npounds per megawatthour\nthousand metric tons\npounds per megawatthour\nshort tons\n\n\n4\n2021\nTX\nTexas\nOTH\nOther\nNA\n9\nNA\n6986\npounds per megawatthour\nthousand metric tons\npounds per megawatthour\nshort tons\n\n\n5\n2021\nTX\nTexas\nNG\nNatural Gas\nNA\n113147\nNA\n745\npounds per megawatthour\nthousand metric tons\npounds per megawatthour\nshort tons\n\n\n\n\n\n\n\n\n\nCrude Oil Import\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\noriginId\noriginName\noriginType\noriginTypeName\ndestinationId\ndestinationName\ndestinationType\ndestinationTypeName\ngradeId\ngradeName\nquantity\nquantity.units\n\n\n\n\n0\n2021\nCTY_CA\nCanada\nCTY\nCountry\nRF_168\nCOFFEYVILLE RESRCS REFG & MKTG / COFFEYVILLE / KS\nRF\nRefinery\nHSO\nHeavy Sour\n6601\nthousand barrels\n\n\n1\n2021\nWORLD\nWorld\nWORLD\nWorld\nUS\nUnited States\nUS\nUnited States\nMED\nMedium\n664743\nthousand barrels\n\n\n2\n2021\nWORLD\nWorld\nWORLD\nWorld\nUS\nUnited States\nUS\nUnited States\nLSW\nLight Sweet\n149120\nthousand barrels\n\n\n3\n2021\nWORLD\nWorld\nWORLD\nWorld\nUS\nUnited States\nUS\nUnited States\nLSO\nLight Sour\n54469\nthousand barrels\n\n\n4\n2021\nWORLD\nWorld\nWORLD\nWorld\nUS\nUnited States\nUS\nUnited States\nHSW\nHeavy Sweet\n96025\nthousand barrels\n\n\n5\n2021\nWORLD\nWorld\nWORLD\nWorld\nUS\nUnited States\nUS\nUnited States\nHSO\nHeavy Sour\n1273116\nthousand barrels\n\n\n\n\n\n\n\n\n\nAnnual Natural Gas Prices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nduoarea\narea.name\nproduct\nproduct.name\nprocess\nprocess.name\nseries\nseries.description\nvalue\nunits\n\n\n\n\n0\n2020\nSMS\nUSA-MS\nEPG0\nNatural Gas\nPRS\nPrice Delivered to Residential Consumers\nN3010MS3\nMississippi Price of Natural Gas Delivered to Residential Consumers (Dollars per Thousand Cubic Feet)\n11.51\n$/MCF\n\n\n1\n2020\nSWI\nUSA-WI\nEPG0\nNatural Gas\nVRX\nPercent Sold to The Commercial Sector\nNA1504_SWI_4\nWisconsin Natural Gas % of Total Residential - Sales (%)\n100.00\n%\n\n\n2\n2020\nSKS\nUSA-KS\nEPG0\nNatural Gas\nVRX\nPercent Sold to The Commercial Sector\nNA1504_SKS_4\nKansas Natural Gas % of Total Residential - Sales (%)\n100.00\n%\n\n\n3\n2020\nSCA\nCALIFORNIA\nEPG0\nNatural Gas\nVRX\nPercent Sold to The Commercial Sector\nNA1504_SCA_4\nCalifornia Natural Gas % of Total Residential - Sales (%)\n95.50\n%\n\n\n4\n2020\nSWV\nUSA-WV\nEPG0\nNatural Gas\nVRX\nPercent Sold to The Commercial Sector\nNA1504_SWV_4\nWest Virginia Natural Gas % of Total Residential - Sales (%)\n100.00\n%\n\n\n5\n2020\nSWA\nWASHINGTON\nEPG0\nNatural Gas\nVRX\nPercent Sold to The Commercial Sector\nNA1504_SWA_4\nWashington Natural Gas % of Total Residential - Sales (%)\n100.00\n%\n\n\n\n\n\n\n\n\n\nAnnual Natural Gas Import and Export by Country\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nduoarea\narea.name\nproduct\nproduct.name\nprocess\nprocess.name\nseries\nseries.description\nvalue\nunits\n\n\n\n\n0\n2020\nNUS-Z00\nU.S.\nEPG0\nNatural Gas\nENC\nCompressed Natural Gas Exports\nNGM_EPG0_ENC_NUS-Z00_MMCF\nCompressed U.S. Natural Gas Exports (Million Cubic Feet)\n386.00\nMMCF\n\n\n1\n2020\nNUS-Z00\nU.S.\nEPG0\nNatural Gas\nEVT\nExports by Vessel and Truck\nNGM_EPG0_EVT_NUS-Z00_MMCF\nLiquefied U.S. Natural Gas Exports by Vessel and Truck (MMcf)\n2386944.00\nMMCF\n\n\n2\n2020\nNUS-NLH\nLTU\nEPG0\nNatural Gas\nEVE\nExports by Vessel\nNGM_EPG0_EVE_NUS-NLH_DMCF\nPrice of Liquefied U.S. Natural Gas Exports by Vessel to Lithuania (Dollars per Thousand Cubic Feet)\n5.17\n\\(/MCF | |3 | 2020|NUS-NSN |SGP |EPG0 |Natural Gas |EVE |Exports by Vessel |NGM_EPG0_EVE_NUS-NSN_DMCF |Price of Liquefied U.S. Natural Gas Exports by Vessels to Singapore (Dollars per Thousand Cubic Feet) | 5.30|\\)/MCF\n\n\n4\n2020\nNUS-NFR\nFRA\nEPG0\nNatural Gas\nEVE\nExports by Vessel\nNGM_EPG0_EVE_NUS-NFR_MMCF\nU.S. Liquefied Natural Gas Exports by Vessel to France (Million Cubic Feet)\n90237.00\nMMCF\n\n\n5\n2020\nNUS-NKS\nKOR\nEPG0\nNatural Gas\nEVE\nExports by Vessel\nNGM_EPG0_EVE_NUS-NKS_MMCF\nLiquefied U.S. Natural Gas Exports by Vessels to South Korea (Million Cubic Feet)\n316227.00\nMMCF\n\n\n\n\n\n\n\n\n\nDaily U.S. Nuclear Outages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\ncapacity\noutage\npercentOutage\ncapacity.units\noutage.units\npercentOutage.units\n\n\n\n\n0\n31-DEC-22\n98185.4\n2687.667\n2.74\nmegawatts\nmegawatts\npercent\n\n\n1\n30-DEC-22\n98185.4\n2748.011\n2.80\nmegawatts\nmegawatts\npercent\n\n\n2\n29-DEC-22\n98185.4\n2977.810\n3.03\nmegawatts\nmegawatts\npercent\n\n\n3\n28-DEC-22\n98185.4\n3089.140\n3.15\nmegawatts\nmegawatts\npercent\n\n\n4\n27-DEC-22\n98185.4\n2992.790\n3.05\nmegawatts\nmegawatts\npercent\n\n\n5\n26-DEC-22\n98185.4\n3710.550\n3.78\nmegawatts\nmegawatts\npercent\n\n\n\n\n\n\n\n\n\nState Energy Data System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nseriesId\nseriesDescription\nstateId\nstateDescription\nvalue\nunit\n\n\n\n\n0\n2021\nESTCD\nElectricity average price, all sectors\nOH\nOhio\n28.74\nDollars per million Btu\n\n\n1\n2021\nNGTCD\nNatural gas average price, all sectors\nMT\nMontana\n7.51\nDollars per million Btu\n\n\n2\n2021\nNGCCB\nNatural gas consumed by (delivered to) the commercial sector\nUT\nUtah\n46053.00\nBillion Btu\n\n\n3\n2021\nNGTCV\nNatural gas total expenditures\nMO\nMissouri\n2546.40\nMillion dollars\n\n\n4\n2021\nSFEIB\nSupplemental gaseous fuels consumed by the electric power sector\nDC\nDistrict of Columbia\n0.00\nBillion Btu\n\n\n5\n2021\nNGACD\nNatural gas price in the transportation sector\nMO\nMissouri\n9.98\nDollars per million Btu\n\n\n\n\n\n\n\n\n\nState Annual CO2 Emissions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nseriesId\nseries.name\nsectorId\nsector.name\nfuelId\nfuel.name\nstateId\nstate.name\ncarbon.coefficient\nemissions\ncarbon.coefficient.units\nemissions.units\n\n\n\n\n0\n2018\nNGRCB\nResidential carbon dioxide emissions Natural Gas (Pipeline)\n3\nResidential Consumption\n6\nnatural gas (pipeline)\nFL\nFlorida\n53.05667\n932205.6\nKilograms of CO2 per million Btu\nCO2 metric tons\n\n\n1\n2018\nNGEIB\nElectric Power carbon dioxide emissions Natural Gas (Pipeline)\n1\nElectric Power Consumption\n6\nnatural gas (pipeline)\nME\nMaine\n53.05667\n766668.8\nKilograms of CO2 per million Btu\nCO2 metric tons\n\n\n2\n2018\nKSICB\nIndustrial carbon dioxide emissions Kerosene\n2\nIndustrial Consumption\n13\nkerosene\nRI\nRhode Island\n72.30667\n0.0\nKilograms of CO2 per million Btu\nCO2 metric tons\n\n\n3\n2018\nDFACB\nTransportation carbon dioxide emissions Distillate Fuel\n4\nTransportation Consumption\n11\ndistillate fuel\nSD\nSouth Dakota\n73.15000\n2480809.1\nKilograms of CO2 per million Btu\nCO2 metric tons\n\n\n4\n2018\nHLRCB\nResidential carbon dioxide emissions Lpg (Fuel Use)\n3\nResidential Consumption\n14\nlpg (fuel use)\nNM\nNew Mexico\n61.81593\n274339.1\nKilograms of CO2 per million Btu\nCO2 metric tons\n\n\n5\n2018\nRFICB\nIndustrial carbon dioxide emissions Residual Fuel\n2\nIndustrial Consumption\n19\nresidual fuel\nOR\nOregon\n78.79667\n7091.7\nKilograms of CO2 per million Btu\nCO2 metric tons\n\n\n\n\n\n\n\n\n\nState Annual CO2 Emissions by Fuel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperiod\nsectorId\nsector.name\nfuelId\nfuel.name\nstateId\nstate.name\nvalue\nvalue.units\n\n\n\n\n0\n2020\nRC\nResidential carbon dioxide emissions\nCO\nCoal\nWV\nWest Virginia\n0.000000\nmillion metric tons of CO2\n\n\n1\n2020\nTT\nTotal carbon dioxide emissions from all sectors\nNG\nNatural Gas\nND\nNorth Dakota\n8.057478\nmillion metric tons of CO2\n\n\n2\n2020\nTT\nTotal carbon dioxide emissions from all sectors\nPE\nPetroleum\nND\nNorth Dakota\n11.400245\nmillion metric tons of CO2\n\n\n3\n2020\nTT\nTotal carbon dioxide emissions from all sectors\nCO\nCoal\nND\nNorth Dakota\n34.793557\nmillion metric tons of CO2\n\n\n4\n2020\nTT\nTotal carbon dioxide emissions from all sectors\nTO\nAll Fuels\nND\nNorth Dakota\n54.251280\nmillion metric tons of CO2\n\n\n5\n2020\nIC\nIndustrial carbon dioxide emissions\nTO\nAll Fuels\nND\nNorth Dakota\n16.016110\nmillion metric tons of CO2"
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "About Me",
    "section": "",
    "text": "Rae Zhang\nData Scientist\n\n\n\n\n\nGraduate student at Georgetown University data science and analytics program, with strong attention to detail, curiosity, problem-solving abilities, and communication skills.\n\nWith an in-depth education in data science, my additional strengths in interdisciplinary academic background, interpersonal and leadership skills, and diverse cultural background would add extra lengths to my ability as a data scientist."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploring the elements of the average temperature time series is required to move on. So that further models may take these components into account, it is crucial to understand how each time series component affects the entire time series. Time series data primarily consist of four elements: level, trend, seasonality, and noise. Noise and unpredictability are nonsystematic components, whereas level, trend, and seasonality are systematic. A greater comprehension of a time series may be attained by breaking it down into these parts.\nIn this exploratory data analysis, the plots will be looking at a time series for the quarterly U.S. total coal consumption only. Even though the data collected has information on all of the 50 states (and that information will be used in future analysis), it makes sense to scope the analysis to only U.S. for simplification purposes. It also makes sense since all the states follow the same trend of coal consumption.\n\n\n\nFirstly, it is important to visualize the time series at its most basic level, which is a simple plot of data over time.\n\n\nCode\ncoal_df &lt;- read.csv(\"/Users/raezh1/Documents/Georgetown/ANLY560/website/Time Series/data/coal_us_consumption.csv\") |&gt;\n  group_by(period) |&gt;\n  summarise(Consumption = sum(consumption)) |&gt;\n  filter(!period %in% c('2000-Q1', '2000-Q2',  '2000-Q3', '2000-Q4'))\n\ncoal_df_ts &lt;- ts(coal_df$Consumption, start = c(2001,1), end = c(2021,1), frequency = 4)\n\n\n\n\nCode\ntheme_set(theme_gray(base_size=12,base_family=\"Palatino\"))\nautoplotly(coal_df_ts) +\n  ggtitle(\"U.S. Quarterly Coal Consumption\") +\n  xlab(\"Year (2001-2021)\") +\n  ylab(\"Short Ton\")\n\n\n\n\n\n\n\n\n\n\n\ndecompose() Functionstl() Function\n\n\nMultiplicative trend means the trend is not linear (curved line), and multiplicative seasonality means there are changes to widths or heights of seasonal periods over time. From the time series graph created above, we should use multiplicative model from decompose() to decompose the coal consumption.\n\n\nCode\ndecompose_coal = decompose(coal_df_ts, \"multiplicative\")\nautoplotly(decompose_coal) +\n  ggtitle(\"Decomposition of U.S. Total Coal Consumption\")\n\n\n\n\n\n\n\n\nIn R the stl() function performs decomposition of a time series into seasonal, trend and irregular components using loses. stl() will handle any type of seasonality, not only monthly and quarterly data.\n\n\nCode\ncoal_df_ts |&gt;\n  stl(s.window=\"periodic\", robust=TRUE) |&gt;\n  autoplotly()\n\n\n\n\n\n\n\n\n\nThe two plots of decomposition are very similar except the remainder from stl() function is less smoothed and more noisy than the remainder from decompose() function. They both show U.S. the U.S. quarterly coal consumption data has seasonality. Before 2010, the trend of the data is increasing. After 2010, the trend of the data is decreasing.\nSince the data has a trend, it doesn’t has stationarity as a stationary time series is one whose properties do not depend on the time at which the series is observed. The ACF graph below will show more of its non-stationary feature.\n\n\n\n\nA lag plot is a type of scatter plot where time series are plotted in pairs against itself some time units behind or ahead. Lag plots often reveal more information on the seasonality of the data, whether there is randomness in the data or an indication of autocorrelation in the data. Below is a lag plot for the quarterly U.S. coal consumption data.\n\n\nCode\nlibrary(wesanderson)\ngglagplot(coal_df_ts, do.lines=FALSE, set.lags = c(4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64)) + \n  xlab(\"Lags\") + \n  ggtitle(\"Lag Plot of U.S. Total Coal Consumption\") +\n  theme_minimal() +\n  ylab(\"Coal Consumption in short tons\") +\n  theme_light() +\n  theme(text=element_text(size=12, family=\"Palatino\")) +\n  labs(fill = \"Legend\") +\n  scale_color_brewer(palette=\"Set2\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\n\nFrom lag plots here we can see that the coal consumption data has serial correlation as the lag plots are showing a linear pattern, which suggests autocorrelation is present. This is a positive linear trend, so the data has positive autocorrelation. However, it’s hard to spot the seasonality from the lag plots, which makes me believe the data doesn’t have seasonality.\n\n\n\n\nThe function ACF computes (and by default plots) an estimate of the autocorrelation function of a univariate time series. Function PACF computes (and by default plots) an estimate of the partial autocorrelation function of a univariate time series.\nAutocorrelation and partial autocorrelation plots are heavily used in time series analysis and forecasting.\n\nACF PlotPACF Plot\n\n\n\n\nCode\nacf &lt;- ggAcf(coal_df_ts, 80) + ggtitle(\"ACF Plot of U.S. Total Coal Consumption\")\nggplotly(acf)\n\n\n\n\n\n\nNote that the ACF shows an oscillation, indicative of a seasonal series. Note the peaks occur at lags of 4th quarter, 8th quarter, and 12th quarter, etc. It means the data has yearly seasonality.\nA stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time.\nAs well as looking at the time plot of the data, the ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Our ACF graph decreases slowly and we can clearly see the data is not stationary.\n\n\n\n\nCode\npacf &lt;- ggPacf(coal_df_ts, 50) + ggtitle(\"PACF Plot of U.S. Total Coal Consumption\")\nggplotly(pacf)\n\n\n\n\n\n\nFrom the PACF plot, we can see a large spike at lag 1 that decreases after a few lags, which indicates a moving average term in the data.\n\n\n\n\n\n\n\n\n\n\nStationarity Test - Augmented Dickey-Fuller TestStationarity Test - KPSS TestSeasonality Test - nsdiffs() Test\n\n\nThe Augmented Dickey-Fuller test is a type of statistical test called a unit root test.\nADF test is conducted with the following assumptions.\nNull Hypothesis (HO): Series is non-stationary or series has a unit root.\nAlternate Hypothesis(HA): Series is stationary or series has no unit root.\nIf the null hypothesis is failed to be rejected, this test may provide evidence that the series is non-stationary.\nConditions to Reject Null Hypothesis(HO)\n\n\nCode\nadf.test(coal_df_ts |&gt; log())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(coal_df_ts)\nDickey-Fuller = -2.9525, Lag order = 4, p-value = 0.1858\nalternative hypothesis: stationary\n\n\nThe p value of ADF test is greater than 0.05, so we cannot reject null hypothesis and it means the U.S. total coal consumption data is not stationary. The rest result matches with the conclusion we had after observing decomposition plot and ACF plot.\nLet’s take the first differencing and do the ADF test again.\n\n\nCode\nadf.test(coal_df_ts |&gt; log() |&gt; diff())\n\n\nWarning in adf.test(diff(log(coal_df_ts))): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(log(coal_df_ts))\nDickey-Fuller = -4.3984, Lag order = 4, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that after first differencing, the p value is below 0.05 which means it’s stationary now and we don’t need the second differencing.\n\n\nIn this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required.\n\n\nCode\nlibrary(urca)\ncoal_df_ts |&gt; ur.kpss() |&gt; summary()\n\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 3 lags. \n\nValue of test-statistic is: 1.7562 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nThe test statistic is much bigger than the 5% critical value, indicating that the null hypothesis is rejected. That is, the data are not stationary. We can difference the data, and apply the test again.\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; diff(lag=4) |&gt; diff() |&gt; ur.kpss() |&gt; summary()\n\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 3 lags. \n\nValue of test-statistic is: 0.0871 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nThis time, the test statistic is tiny, and well within the range we would expect for stationary data, so we can conclude that the differenced data are stationary.\n\n\nA similar function for determining whether seasonal differencing is required is nsdiffs(), which uses the measure of seasonal strength introduced to determine the appropriate number of seasonal differences required. ndiffs() estimates the number of first differences. No seasonal differences are suggested if the result is less than 0.64, otherwise one seasonal difference is suggested.\nWe can apply nsdiffs() to the logged U.S. Total Coal Consumption data.\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; ndiffs()\n\n\n[1] 1\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; diff() |&gt; nsdiffs()\n\n\n[1] 1\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; diff(lag=4) |&gt; nsdiffs()\n\n\n[1] 0\n\n\nBecause nsdiffs() returns 1 on the original data(indicating one seasonal difference is required), we apply the function again to the seasonally differenced data. These two functions suggest we should do both a seasonal difference and a first difference.\n\n\n\nIn conclusion, after trying several differences and seasonal differences combinations, taking one seasonal difference and one difference is what the data needs to be stationary.\n\n\n\nTransformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\n\n\nCode\ncbind(\"Original\" = coal_df_ts,\n      \"Log Trans\" = log(coal_df_ts),\n      \"Sea. Diff\" = diff(log(coal_df_ts),lag=4),\n      \"1st & Sea. Diff\" = diff(diff(log(coal_df_ts),lag=4))) |&gt;\n  autoplotly(facets=TRUE) +\n    xlab(\"Year\") + ylab(\"\") +\n    ggtitle(\"Differencing of U.S. Total Coal Consumption\") \n\n\n\n\n\n\n\n\n\n\nFinally, the seasonality and correlation should be removed to make the time series stationary. A comparison of all of the methods is seen below. Once the transformations are applied, the series is stationary. This can also be seen in the plot below.\n\n\nCode\n`Log Transformation` &lt;- log(coal_df_ts)\n`Remove Seasonality` &lt;- diff(log(coal_df_ts), lag=4)\n`First Diff After Remove Seasonality` &lt;- diff(diff(log(coal_df_ts), lag=4))\n\na &lt;- ggAcf(coal_df_ts,70) + ggtitle(\"Original Data\")\nb &lt;- ggAcf(`Log Transformation`,70) + ggtitle(\"Log Transformation\")\nc &lt;- ggAcf(`Remove Seasonality`,70) + ggtitle(\"Remove Seasonality\")\nd &lt;- ggAcf(`First Diff After Remove Seasonality`,70) + ggtitle(\"First Diff After Remove Seasonality\")\n\n\n\n\n\nOriginal and Log TransformationDetrend and Remove Seasonality\n\n\n\n\nCode\nggarrange(a, b, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nggarrange(c, d, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\nThe first three graphs above are not as ideal as the last ACF graph which is First Diff After Remove Seasonality.\nFor white noise series and stationary series, we expect each autocorrelation to be close to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within ±2/√T where T is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise, and it’s not stationary either.\nThis ACF graph of detrended and differenced U.S. total coal consumption data shows almost all the spikes are inside the blue bounds, which proves the detrended and differenced data is stationary and ready to be utilized in future analysis."
  },
  {
    "objectID": "intro.html#energy-industry",
    "href": "intro.html#energy-industry",
    "title": "Introduction",
    "section": "Energy Industry",
    "text": "Energy Industry\n\nU.S. Energy-related Carbon Dioxide Emissions by Source\nAccording to U.S. Energy Information Administration, about half of U.S. energy-related CO2 emissions were from petroleum use in 2021.\nIn 2021, petroleum accounted for about 36% of U.S. energy consumption but petroleum was the source of 46% of total annual U.S. energy-related CO2 emissions. Natural gas also provided about 32% U.S. energy and accounted for 34% of total annual energy-related CO2 emissions. Coal was the source of about 12% of U.S. energy use and of about 21% of total annual energy-related CO2 emissions.\n\n\n\n\n\n\n\n\nClean Energy\nClean energy has been a hot topic recently as more and more people are concerned about global warming. WIth the emergence of electric vehicles and hybrid vehicles, the whole society has started to normalize EV as part of our lives. How ‘clean’ is the clean energy? Will the total carbon footprint of the supply chain of electric vehicles outweigh the traditional system? How much more electric consumption has been made as we normalize EVs?\n\n\n\niStock Images"
  },
  {
    "objectID": "intro.html#greenhouse-gas-emissions-by-sector",
    "href": "intro.html#greenhouse-gas-emissions-by-sector",
    "title": "Introduction",
    "section": "Greenhouse Gas Emissions by Sector",
    "text": "Greenhouse Gas Emissions by Sector\nAfter knowing what is global radiative forcing, I wanted to know what industries are the top contributors to greenhouse gas emissions.\nThe graph below from Our World in Data organization shows the greenhouse gas emissions by sector, where we can see energy industry takes up to 73% of global greenhouse gas emissions in 2016. The high amount of percentage from energy industry makes me wonder if we are putting efforts in transitioning from traditional energy to clean energy and limiting our carbon footprint overtime.\n\n\n\n\nSource: Our World in Data"
  },
  {
    "objectID": "dv.html#visualize-coal-consumption-data",
    "href": "dv.html#visualize-coal-consumption-data",
    "title": "Data Visualization",
    "section": "Visualize Coal Consumption Data",
    "text": "Visualize Coal Consumption Data\n\nChoropleth Map\n\nThe coal consumption dataset has American states’ coal consumption information from 2001. The unit of the coal consumption is in short tons, a unit of weight equal to 2,000 pounds.\nEach state has different sector of coal consumption, such as electric utility, electric power, independent power producers, coke plants, commercial and institutional, IPP CHP, IPP Non-CHP, and other industrial.\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nClick to view the graph\n\nThe choropleth map above shows the total coal consumption from 2001 to 2021 by state. The animated map shows Texas has been heavily rely on coal as it has the darkest color in the map.\nFrom 2001 to 2010, Texas, Illinois, Pennsylvania, Ohio, and Indiana had fairly darker colors than other states. In 2010, the total coal consumption of Texas was 261 million short tons, Illinois was 163 million short tons, Pennsylvania was 155 million short tons, Ohio and Indiana were both 123 million short tons.\nFortunately, since 2010, we can see the colors of these states but Texas started to get lighter and lighter, which shows they were trying to decrease the coal consumption in all the sectors and use other alternative energy sources to replace coal, which is a good thing.\n\n\n\nFacet Graphs\n\nAfter having a general idea of different states’ coal consumption throughout the years, facet graphs will help us view each states’ coal consumption in detail.\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nClick to view the graph\n\nFrom the facet graphs above, Texas’ coal consumption is way more than other states. Even Texas is trying to reduce their coal consumption every year, the amount of coal they use each year is still really high compared to other states.\n\n\n\nLine Chart\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nClick to view the graph\n\nThe line chart above shows the time series graph of each state’s coal consumption from 2001 to 2021. On average, Texas’ annual coal consumption is around 100 million short tons more than the second most coal using state Illinois.\nWe can also tell that majority of the states keep their annual coal consumption below 100 million short tons.\n\n\n\nBar Chart\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nClick to view the graph\n\nThe bar chart above shows the trend of American total coal consumption in the past ten years. As indicates from the choropleth map above, there’s a trend of decrease from 2010. Interestingly, there was an increase trend from 2001 to 2008. There are three major drop which are from 2009, 2012, and 2020.\nFrom this bar graph, we can also tell that, on average, the coal consumption of Texas is about one eighth of the total American coal usage."
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "When first attempting to model time series data, the first model considered is an autoregressive or moving average model. These AR/MA models are used to understand better the data and forecast future values based on historical data recorded previously. Before using these models, the time series must be stationary, meaning the mean and variance do not change over time. The most often cause of a violation of stationarity is a trend, where values slowly increase over time. So first, to check whether the data is stationary, we can use an ACF (autocorrelation function) and Dickey-Fuller Test to test for stationarity. Below is the Dickey-Fuller Test and the ACF plot for the coal consumption data explored and visualized in previous sections.\n\n\n\nCode\ntheme_set(theme_gray(base_size=12,base_family=\"Palatino\"))\ncoal_df &lt;- read.csv(\"data/coal_us_consumption.csv\") |&gt;\n  group_by(period) |&gt;\n  summarise(Consumption = sum(consumption)) |&gt;\n  filter(!period %in% c('2000-Q1', '2000-Q2',  '2000-Q3', '2000-Q4'))\n\ncoal_df_ts &lt;- ts(coal_df$Consumption, start = c(2001,1), end = c(2021,1), frequency = 4)\n\n`Log Transformation` &lt;- log(coal_df_ts)\n`Remove Seasonality` &lt;- diff(log(coal_df_ts), lag=4)\n`First Diff After Remove Seasonality` &lt;- diff(diff(log(coal_df_ts), lag=4))\n\na &lt;- ggAcf(coal_df_ts,70) + ggtitle(\"Original Data\")\nb &lt;- ggAcf(`Log Transformation`,70) + ggtitle(\"Log Transformation\")\nc &lt;- ggAcf(`Remove Seasonality`,70) + ggtitle(\"Remove Seasonality\")\nd &lt;- ggAcf(`First Diff After Remove Seasonality`,70) + ggtitle(\"First Diff After Remove Seasonality\")\n\n\n\n\n\nOriginal and Log TransformationDetrend and Remove Seasonality\n\n\n\n\nCode\nggarrange(a, b, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nggarrange(c, d, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\nThe ACF plots show the differences among original data, log transformation, remove seasonality, and first difference with remove seasonality.\nFor white noise series and stationary series, we expect each autocorrelation to be close to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within ±2/√T where T is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise, and it’s not stationary either.\nThis ACF graph of detrended and differenced US total coal consumption data (the forth one from above) shows almost all the spikes are inside the blue bounds, which proves the detrended and differenced data is stationary and ready to be utilized in future analysis.\n\n\n\n\n\n\n\n\nCode\nadf.test(`First Diff After Remove Seasonality`)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  First Diff After Remove Seasonality\nDickey-Fuller = -3.9404, Lag order = 4, p-value = 0.01703\nalternative hypothesis: stationary\n\n\nStationarity test is less than 0.05, which means first order differencing and removing seasonality would make the time series dataset stationary. The dataset is no longer needed to be differenced or detrended.\n\n\n\n\n\n\nCode\nisSeasonal(`First Diff After Remove Seasonality`, test = \"combined\", freq = NA)\n\n\n[1] FALSE\n\n\nThe isSeasonal function from seastests library shows our first-differenced and detrended data has no seasonality.\nNow we are confident that our differenced and detrended data is ready for the model fitting.\n\n\n\n\n\n\n\nCode\nlog(coal_df_ts) |&gt; diff() |&gt;diff(lag=4) |&gt; ggtsdisplay() #this is better\n\n\n\n\n\nCode\n#ggplotly(d) plotting the stationary data's ACF graph\n\n\nThe ACF and PACF graphs are showing that q = 0,1,2,3,4, d = 1, p = 0,1,2,3,4, Q = 0,1,2, P = 0,1,2,3, and D = 1\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*100),nrow=100)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\n\n\nCode\noutput &lt;- SARIMA.c(p1 = 1, p2 = 5, q1 = 1, q2 = 5, P1 = 1, P2 = 4, Q1 = 1, Q2 = 3, data = coal_df_ts) |&gt;\n  drop_na()\n\nminaic &lt;- output[which.min(output$AIC), ]\nminbic &lt;- output[which.min(output$BIC), ]\n\n\n\nParameters with Minimum AICParameters with Minimum BICUsing auto.arima()\n\n\n\n\nCode\nknitr::kable(minaic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n8\n0\n1\n0\n2\n1\n1\n2807.125\n2816.448\n2807.689\n\n\n\n\n\n\n\n\n\nCode\nknitr::kable(minbic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n0\n1\n0\n0\n1\n1\n2810.769\n2815.431\n2810.934\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(coal_df_ts)\n\n\nSeries: coal_df_ts \nARIMA(0,1,0)(2,1,1)[4] \n\nCoefficients:\n       sar1     sar2     sma1\n      0.069  -0.3507  -0.7618\ns.e.  0.143   0.1285   0.1291\n\nsigma^2 = 5.598e+14:  log likelihood = -1399.56\nAIC=2807.13   AICc=2807.69   BIC=2816.45\n\n\n\n\n\n\n\n\n\nFrom model fitting, we generated two models, ARIMA(0,1,0) x (2,1,1) and ARIMA(0,1,0) x (0,1,1). auto.arima() generated ARIMA(0,1,0) x (2,1,1) as well. ARIMA(0,1,0) x (2,1,1) has the lowest AIC while ARIMA(0,1,0) x (0,1,1) has the lowest BIC. Now let’s do two model diagnoses to analyze the result and find the better model to do forecast later.\n\n\n\n\nCode\nfit1 &lt;- Arima(coal_df_ts, order=c(0,1,0), seasonal = list(order = c(2,1,1), period = 4))\n\n\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(coal_df_ts,0,1,0,2,1,1,4))\n\n\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order = c(2,1,1), period = 4)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)(2,1,1)[4]\nQ* = 8.2062, df = 5, p-value = 0.1452\n\nModel df: 3.   Total lags used: 8\n\n\nThe Ljung-Box test uses the following hypotheses:\nH0: The residuals are independently distributed.\nHA: The residuals are not independently distributed; they exhibit serial correlation.\nIdeally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.\n\n\n\nThere isn’t any significant spikes in the ACF, and the model fails the Ljung-Box test. The model can still be used for forecasting.\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order=c(2,1,1), period=4))\n\n\nSeries: coal_df_ts \nARIMA(0,1,0)(2,1,1)[4] \n\nCoefficients:\n       sar1     sar2     sma1\n      0.069  -0.3507  -0.7618\ns.e.  0.143   0.1285   0.1291\n\nsigma^2 = 5.598e+14:  log likelihood = -1399.56\nAIC=2807.13   AICc=2807.69   BIC=2816.45\n\n\n\n\n\n\n\nCode\ncat(model_output[25:56], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n       sar1     sar2     sma1\n      0.069  -0.3507  -0.7618\ns.e.  0.143   0.1285   0.1291\n\nsigma^2 estimated as 5.377e+14:  log likelihood = -1399.56,  aic = 2807.13\n\n$degrees_of_freedom\n[1] 73\n\n$ttable\n     Estimate     SE t.value p.value\nsar1   0.0690 0.1430  0.4823  0.6310\nsar2  -0.3507 0.1285 -2.7300  0.0079\nsma1  -0.7618 0.1291 -5.9024  0.0000\n\n$AIC\n[1] 36.93586\n\n$AICc\n[1] 36.94024\n\n$BIC\n[1] 37.05853\n\n\n\n\n\n\n\n\nCode\nfit2 &lt;- Arima(coal_df_ts, order=c(0,1,0), seasonal = list(order = c(0,1,1), period = 4))\n\n\n\n\n\n\nCode\nset.seed(123)\nmodel_output2 &lt;- capture.output(sarima(coal_df_ts,0,1,0,0,1,1,4))\n\n\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order = c(0,1,1), period = 4)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)(0,1,1)[4]\nQ* = 16.558, df = 7, p-value = 0.02048\n\nModel df: 1.   Total lags used: 8\n\n\nThe Ljung-Box test uses the following hypotheses:\nH0: The residuals are independently distributed.\nHA: The residuals are not independently distributed; they exhibit serial correlation.\nIdeally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.\n\n\n\nThere are two significant spikes in the ACF, and the model fails the Ljung-Box test. The model can still be used for forecasting.\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order = c(0,1,1), period = 4))\n\n\nSeries: coal_df_ts \nARIMA(0,1,0)(0,1,1)[4] \n\nCoefficients:\n         sma1\n      -0.8690\ns.e.   0.0877\n\nsigma^2 = 6.029e+14:  log likelihood = -1403.38\nAIC=2810.77   AICc=2810.93   BIC=2815.43\n\n\n\n\n\n\n\nCode\ncat(model_output2[20:49], model_output2[length(model_output2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         sma1\n      -0.8690\ns.e.   0.0877\n\nsigma^2 estimated as 5.949e+14:  log likelihood = -1403.38,  aic = 2810.77\n\n$degrees_of_freedom\n[1] 75\n\n$ttable\n     Estimate     SE t.value p.value\nsma1   -0.869 0.0877 -9.9034       0\n\n$AIC\n[1] 36.98381\n\n$AICc\n[1] 36.98452\n\n$BIC\n[1] 37.04514\n\n\n\n\n\n\n\nFrom model diagnostics above, ARIMA(0,1,0) x (2,1,1) is also the one auto.arima() produced, and it has less spikes in the ACF and PACF plots of its residuals. The model also has smaller sigma squared means it has smaller variance, which means the estimators with a smaller variance is more efficient.\nTherefore, ARIMA(0,1,0) x (2,1,1) is the better model.\n\n\n\n\n\nLet’s forecast for the next three years using the model we just selected.\n\n\n\n\nCode\nc &lt;- fit1 |&gt; forecast(h=12) |&gt; autoplotly() + ggtitle(\"Quarterly Coal Consumption Three-Year-Forecasting\") + xlab(\"Year\") + ylab(\"Short Ton\")\nc |&gt;\n  layout(showlegend = F,\n         xaxis = list(rangeslider = list(visible = T)))\n\n\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(coal_df_ts, 12, 0,1,0,2,1,1,4)\n\n\n\n\n\n$pred\n          Qtr1      Qtr2      Qtr3      Qtr4\n2021           274828535 376893671 306317902\n2022 319770967 282898241 373853941 294614305\n2023 276483317 237626523 331810754 257541142\n2024 244525100                              \n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2021          23187703 32792364 40162280\n2022 46375406 55401734 63150819 70047839\n2023 76324135 81363930 86109260 90606402\n2024 94890651                           \n\n\n\n\n\n\n\n\n\nCode\nvalues &lt;- append(coal_df$Consumption, rep(NA,11))\ncoal_ts_df &lt;- data.frame(Year = seq(as.Date(\"2000/1/1\"), by = \"quarter\", length.out = 92), Short.tons=values)\n\ncoal_ts_df$meanf &lt;- append(rep(NA,80), meanf(coal_df_ts, h=12)$mean)\ncoal_ts_df$naive &lt;- append(rep(NA,80), naive(coal_df_ts, h=12)$mean)\ncoal_ts_df$snaive &lt;- append(rep(NA,80), snaive(coal_df_ts, h=12)$mean)\ncoal_ts_df$rwf &lt;- append(rep(NA,80), rwf(coal_df_ts, h=12, drift=TRUE)$mean)\ncoal_ts_df$fit &lt;- append(rep(NA,80), forecast(fit1, h=12)$mean)\n\np &lt;- ggplot(coal_ts_df) + \n  geom_line(aes(x=Year, y = Short.tons)) + \n  geom_line(aes(x=Year, y = meanf, color = \"Mean\")) +\n  geom_line(aes(x=Year, y = naive, color = \"Naïve\")) +\n  geom_line(aes(x=Year, y = snaive, color = \"SNaïve\")) +\n  geom_line(aes(x=Year, y = rwf, color = \"Drift\")) +\n  geom_line(aes(x=Year, y = fit, color = \"Model\")) +\n  ggtitle(\"Comparison of the Fitted Model and Benchmark Methods\") +\n  ylab(\"Short tons\")\n  \nggplotly(p)\n\n\n\n\n\n\nBenchmark method is for data scientists to keep their sanity when building models, they set a baseline — a score that the model must outperform. Normally, the state-of-the-art is used as the baseline but for problems with no existing solutions yet, one should build their own baseline.\n\n\nThis method simply takes the average (or “mean”) value of the entire historical data and use that to forecast future values. Very useful for data with small variance or whose value lies close to the mean.\n\n\n\nDrift is the amount of change observed from the data. In this method, drift is set to be the average change seen in the whole historical data and uses that to forecast values in the future. Basically, this just means drawing a straight line using the first and last values and extend that line into the future. This method works well on data that follows a general trend over time.\n\n\n\nThis method uses the most recent value as the forecasted value for the next time step. The assumption followed by this method is that its value tomorrow is equal to its value today.\n\n\n\nThe fitted model has outperformed the rest of the benchmark methods such us the mean method, naive methods, and drift method."
  },
  {
    "objectID": "models.html#parameters-with-minimum-aic",
    "href": "models.html#parameters-with-minimum-aic",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Parameters with Minimum AIC",
    "text": "Parameters with Minimum AIC\n\n\nCode\nknitr::kable(minaic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n8\n0\n1\n0\n2\n1\n1\n2807.125\n2816.448\n2807.689"
  },
  {
    "objectID": "models.html#parameters-with-minimum-bic",
    "href": "models.html#parameters-with-minimum-bic",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Parameters with Minimum BIC",
    "text": "Parameters with Minimum BIC\n\n\nCode\nknitr::kable(minbic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n0\n1\n0\n0\n1\n1\n2810.769\n2815.431\n2810.934"
  },
  {
    "objectID": "models.html#using-auto.arima",
    "href": "models.html#using-auto.arima",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Using auto.arima()",
    "text": "Using auto.arima()\n\n\nCode\nauto.arima(coal_df_ts)\n\n\nSeries: coal_df_ts \nARIMA(0,1,0)(2,1,1)[4] \n\nCoefficients:\n       sar1     sar2     sma1\n      0.069  -0.3507  -0.7618\ns.e.  0.143   0.1285   0.1291\n\nsigma^2 = 5.598e+14:  log likelihood = -1399.56\nAIC=2807.13   AICc=2807.69   BIC=2816.45"
  },
  {
    "objectID": "intro.html#the-big-picture",
    "href": "intro.html#the-big-picture",
    "title": "Introduction",
    "section": "The Big Picture",
    "text": "The Big Picture\nThis time series project is going to focus on the US energy consumption over the years and I hope I would be able to get clearer ideas on energy industry and its greenhouse gasses emissions and carbon footprints overtime.\n\n\n\nThe United States refines millions of barrels of oil every day. David McNew/Getty Images"
  },
  {
    "objectID": "eda.html#exploring-u.s.-co2-emission-data",
    "href": "eda.html#exploring-u.s.-co2-emission-data",
    "title": "Exploratory Data Analysis",
    "section": "Exploring U.S. CO2 Emission Data",
    "text": "Exploring U.S. CO2 Emission Data"
  },
  {
    "objectID": "eda.html#time-series-plot-1",
    "href": "eda.html#time-series-plot-1",
    "title": "Exploratory Data Analysis",
    "section": "Time Series Plot",
    "text": "Time Series Plot\n\n\nCode\nemissions_df &lt;- read.csv(\"/Users/raezh1/Documents/Georgetown/ANLY560/website/Time Series/data/df_total_monthly_CO2_emissions.csv\", skip=1, row.names = 1)\n\n#emissions_ts &lt;- ts(emission_df$sum, star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\nemissions_ts &lt;- ts(emissions_df$sum, start = c(1973,1), end = c(2022,12), frequency = 12)\n\nautoplotly(emissions_ts) +\n  xlab(\"Year\") + ylab(\"Million Metric Tons of Carbon Dioxide\") +\n  ggtitle(\"U.S. Monthly CO2 Emissions\")"
  },
  {
    "objectID": "eda.html#decomposition-1",
    "href": "eda.html#decomposition-1",
    "title": "Exploratory Data Analysis",
    "section": "Decomposition",
    "text": "Decomposition\n\ndecompose() Functionstl() Function\n\n\nMultiplicative trend means the trend is not linear (curved line), and multiplicative seasonality means there are changes to widths or heights of seasonal periods over time. From the time series graph created above, we should use multiplicative model from decompose() to decompose the coal consumption.\n\n\nCode\ndecompose_emissions = decompose(emissions_ts, \"multiplicative\")\nautoplotly(decompose_emissions) +\n  ggtitle(\"Decomposition of U.S. Total CO2 Emissions\")\n\n\n\n\n\n\n\n\nIn R the stl() function performs decomposition of a time series into seasonal, trend and irregular components using loses. stl() will handle any type of seasonality, not only monthly and quarterly data.\n\n\nCode\nemissions_ts |&gt;\n  stl(s.window=\"periodic\", robust=TRUE) |&gt;\n  autoplotly() +\n  ggtitle(\"Decomposition of U.S. Total CO2 Emissions\")\n\n\n\n\n\n\n\n\n\nThe two plots of decomposition are very similar except the remainder from stl() function is less smoothed and more noisy than the remainder from decompose() function. They both show U.S. the U.S. monthly CO2 emissions data has seasonality. Before 2008, the trend of the data is increasing. After 2008, the trend of the data is decreasing.\nSince the data has a trend, it doesn’t has stationarity as a stationary time series is one whose properties do not depend on the time at which the series is observed. The ACF graph below will show more of its non-stationary feature."
  },
  {
    "objectID": "eda.html#lag-plots-1",
    "href": "eda.html#lag-plots-1",
    "title": "Exploratory Data Analysis",
    "section": "Lag Plots",
    "text": "Lag Plots\nA lag plot is a type of scatter plot where time series are plotted in pairs against itself some time units behind or ahead. Lag plots often reveal more information on the seasonality of the data, whether there is randomness in the data or an indication of autocorrelation in the data. Below is a lag plot for the quarterly U.S. coal consumption data.\n\n\nCode\nlibrary(wesanderson)\ngglagplot(emissions_ts, do.lines=FALSE, set.lags = c(1,12,12*2,12*3,12*4,12*5,12*6,12*7,12*8,12*9,12*10,12*11,12*12,12*13,12*14,12*15)) + \n  xlab(\"Lags\") + \n  ggtitle(\"Lag Plot of U.S. CO2 Emissions\") +\n  theme_minimal() +\n  ylab(\"Million Metric Tons of Carbon Dioxide\") +\n  theme_light() +\n  theme(text=element_text(size=12, family=\"Palatino\")) +\n  labs(fill = \"Legend\") +\n  #scale_color_brewer(palette=\"Set2\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\n\nFrom lag plots here we can see that the CO2 emissions data has serial correlation as the lag plots are showing a linear pattern, which suggests autocorrelation is present. This is a positive linear trend, so the data has positive autocorrelation. However, it’s hard to spot the seasonality from the lag plots, which makes me believe the data doesn’t have seasonality."
  },
  {
    "objectID": "eda.html#autocorrelation-and-stationarity-1",
    "href": "eda.html#autocorrelation-and-stationarity-1",
    "title": "Exploratory Data Analysis",
    "section": "Autocorrelation and Stationarity",
    "text": "Autocorrelation and Stationarity\nThe function ACF computes (and by default plots) an estimate of the autocorrelation function of a univariate time series. Function PACF computes (and by default plots) an estimate of the partial autocorrelation function of a univariate time series.\nAutocorrelation and partial autocorrelation plots are heavily used in time series analysis and forecasting.\n\nACF PlotPACF Plot\n\n\n\n\nCode\nacf &lt;- ggAcf(emissions_ts, 80) + ggtitle(\"ACF Plot of U.S. Total Coal Consumption\")\nggplotly(acf)\n\n\n\n\n\n\nNote that the ACF shows an oscillation, indicative of a seasonal series. Note the peaks occur at lags of 12th, 24th, and 36th, etc. It means the data has yearly seasonality.\nA stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time.\nAs well as looking at the time plot of the data, the ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Our ACF graph decreases slowly and we can clearly see the data is not stationary.\n\n\n\n\nCode\npacf &lt;- ggPacf(emissions_ts, 80) + ggtitle(\"PACF Plot of U.S. Total CO2 Emissions\")\nggplotly(pacf)\n\n\n\n\n\n\nFrom the PACF plot, we can see a large spike at lag 1 that decreases after 13 lags, which indicates a moving average term in the data."
  },
  {
    "objectID": "eda.html#differencing-and-detrending-1",
    "href": "eda.html#differencing-and-detrending-1",
    "title": "Exploratory Data Analysis",
    "section": "Differencing and Detrending",
    "text": "Differencing and Detrending\n\nDifferences and Seasonal Differences Estimation with Stationarity and Seasonality Tests\n\nStationarity Test - Augmented Dickey-Fuller TestStationarity Test - KPSS TestSeasonality Test - nsdiffs() Test\n\n\nThe Augmented Dickey-Fuller test is a type of statistical test called a unit root test.\nADF test is conducted with the following assumptions.\nNull Hypothesis (HO): Series is non-stationary or series has a unit root.\nAlternate Hypothesis(HA): Series is stationary or series has no unit root.\nIf the null hypothesis is failed to be rejected, this test may provide evidence that the series is non-stationary.\nConditions to Reject Null Hypothesis(HO)\n\n\nCode\nadf.test(emissions_ts |&gt; log())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(emissions_ts)\nDickey-Fuller = -2.4005, Lag order = 8, p-value = 0.4088\nalternative hypothesis: stationary\n\n\nThe p value of ADF test is greater than 0.05, so we cannot reject null hypothesis and it means the U.S. total coal consumption data is not stationary. The rest result matches with the conclusion we had after observing decomposition plot and ACF plot.\nLet’s do the ADF test after first differencing.\n\n\nCode\nadf.test(emissions_ts |&gt; log() |&gt; diff())\n\n\nWarning in adf.test(diff(log(emissions_ts))): p-value smaller than printed\np-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(log(emissions_ts))\nDickey-Fuller = -14.873, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that after first differencing, the p value is below 0.05 which means it’s stationary now and we don’t need the second differencing.\n\n\nIn this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required.\n\n\nCode\nlibrary(urca)\nemissions_ts |&gt; log() |&gt; ur.kpss() |&gt; summary()\n\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 6 lags. \n\nValue of test-statistic is: 3.9813 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nThe test statistic is much bigger than the 5% critical value, indicating that the null hypothesis is rejected. That is, the data are not stationary. We can difference the data, and apply the test again.\n\n\nCode\nemissions_ts |&gt; log() |&gt; diff() |&gt; ur.kpss() |&gt; summary()\n\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 6 lags. \n\nValue of test-statistic is: 0.0218 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nThis time, the test statistic is small and well within the range we would expect for stationary data, so we can conclude that the differenced data are stationary.\n\n\nA similar function for determining whether seasonal differencing is required is nsdiffs(), which uses the measure of seasonal strength introduced to determine the appropriate number of seasonal differences required. No seasonal differences are suggested if the result is less than 0.64, otherwise one seasonal difference is suggested.\nWe can apply nsdiffs() to the logged U.S. Total CO2 Emissions data.\n\n\nCode\nemissions_ts |&gt; log() |&gt; ndiffs()\n\n\n[1] 1\n\n\nCode\nemissions_ts |&gt; log() |&gt; diff() |&gt; nsdiffs()\n\n\n[1] 1\n\n\nCode\nemissions_ts |&gt; log() |&gt; diff() |&gt; diff(lag=12) |&gt; nsdiffs()\n\n\n[1] 0\n\n\nBecause both nsdiff() and nsdiffs() returns 1 (indicating one seasonal difference and one difference are required), below we apply the ndiffs() function to the seasonally differenced data.\nlag=12 means taking a yearly seasonal difference, and the result is less than 0.64 which means our time series data doesn’t have seasonality anymore after taking a yearly seasonal difference\n\n\n\nIn conclusion, after trying several differences and seasonal differences combinations, taking one seasonal difference and one difference is what the this data needs to be stationary.\n\n\nDifferencing Results\n\n\nCode\ncbind(\"Original\" = emissions_ts,\n      \"Log Trans\" = log(emissions_ts),\n      \"Sea. Diff\" = diff(log(emissions_ts),lag=12),\n      \"1st & Sea. Diff\" = diff(diff(log(emissions_ts),lag=12))) %&gt;%\n  autoplotly(facets=TRUE) +\n    xlab(\"Year\") + ylab(\"\") +\n    ggtitle(\"Differencing of U.S. Total CO2 Emissions\")"
  },
  {
    "objectID": "eda.html#new-stationary-data-1",
    "href": "eda.html#new-stationary-data-1",
    "title": "Exploratory Data Analysis",
    "section": "New Stationary Data",
    "text": "New Stationary Data\nFinally, the seasonality and correlation should be removed to make the time series stationary. A comparison of all of the methods is seen below. Once the transformations are applied, the series is stationary. This can also be seen in the plot below.\n\n\nCode\n`Log Transformation` &lt;- log(emissions_ts)\n`Remove Seasonality` &lt;- diff(log(emissions_ts), lag=12)\n`First Diff After Remove Seasonality` &lt;- diff(diff(log(emissions_ts), lag=12))\n\na &lt;- ggAcf(emissions_ts,70) + ggtitle(\"Original Data\")\nb &lt;- ggAcf(`Log Transformation`,70) + ggtitle(\"Log Transformation\")\nc &lt;- ggAcf(`Remove Seasonality`,70) + ggtitle(\"Remove Seasonality\")\nd &lt;- ggAcf(`First Diff After Remove Seasonality`,70) + ggtitle(\"First Diff After Remove Seasonality\")\n\n\n\nACF Plots\n\nOriginal and Log TransformationDetrend and Remove Seasonality\n\n\n\n\nCode\nggarrange(a, b, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nggarrange(c, d, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\nThe first three graphs above are not as ideal as the last ACF graph which is First Diff After Remove Seasonality.\nFor white noise series and stationary series, we expect each autocorrelation to be close to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within ±2/√T where T is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise, and it’s not stationary either.\nThis ACF graph of detrended and differenced U.S. total CO2 Emissions data shows almost all the spikes are inside the blue bounds, which proves the detrended and differenced data is stationary and ready to be utilized in future analysis."
  },
  {
    "objectID": "models2.html",
    "href": "models2.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "From my introduction page we know that global warming and greenhouse effects have been a really concerning issues in recent years. Greenhouse effect is caused by the global radiative forcing as explained in the introduction page (Chandler Explained: Radiative forcing). What causes the global radiative forcing is the increasing amount of greenhouse gasses such as carbon dioxide, methane, nitrous oxide, and fluorinated gases. Carbon dioxide is the most predominant gas among the greenhouse gases (National Geographic Greenhouse effect).\nThe pie charts from EIA show U.S. carbon dioxide emissions comes from three sources which is petroleum 46%, natural gas 34%, and coal 21%. Those are all energy sources which takes up to 66% of the U.S. total CO2 emissions (U.S. Energy Information Administration - EIA - independent statistics and analysis). These energy sources also take up to 73.2% of the total greenhouse gas emissions (Ritchie et al. Emissions by sector). From the pie chart, we also know that there’s a 21% of the U.S. energy consumption is nonfossil which is nuclear and renewable energy (U.S. Energy Information Administration - EIA - independent statistics and analysis).\nOne of the project goals is trying to find out as total U.S. energy consumption are changing from traditional sources such as petroleum, natural gas, and coal to clean energies like nuclear and renewable energy, if the total U.S. CO2 emissions decreases and if we are able to predict future CO2 emissions based on energy sources variables. The total monthly petroleum, natural gas, coal consumption, and renewable energy consumption datasets were collected and shown in the previous tabs, and they will be treated as variables in regards to CO2 emissions in the following time series models."
  },
  {
    "objectID": "models.html#acf-plots-1",
    "href": "models.html#acf-plots-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ACF Plots",
    "text": "ACF Plots\n\nOriginal and Log TransformationDetrend and Remove Seasonality\n\n\n\n\nCode\nggarrange(a, b, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nggarrange(c, d, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\nThe ACF plots show the differences among original data, log transformation, remove seasonality, and first difference with remove seasonality.\nFor white noise series and stationary series, we expect each autocorrelation to be close to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within ±2/√T where T is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise, and it’s not stationary either.\nThis ACF graph of detrended and differenced US total coal consumption data (the forth one from above) shows almost all the spikes are inside the blue bounds, which proves the detrended and differenced data is stationary and ready to be utilized in future analysis."
  },
  {
    "objectID": "models.html#stationarity-test-seasonality-test-1",
    "href": "models.html#stationarity-test-seasonality-test-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Stationarity Test & Seasonality Test",
    "text": "Stationarity Test & Seasonality Test\n\nStationarity\n\n\nCode\nadf.test(`First Diff After Remove Seasonality`)\n\n\nWarning in adf.test(`First Diff After Remove Seasonality`): p-value smaller than\nprinted p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  First Diff After Remove Seasonality\nDickey-Fuller = -10.227, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nStationarity test is less than 0.05, which means first order differencing and removing seasonality would make the time series dataset stationary. The dataset is no longer needed to be differenced or detrended.\n\n\nSeasonality\n\n\n\nCode\nisSeasonal(`First Diff After Remove Seasonality`, test = \"combined\", freq = NA)\n\n\n[1] FALSE\n\n\nThe isSeasonal function from seastests library shows our first-differenced and detrended data has no seasonality.\nNow we are confident that our differenced and detrended data is ready for the model fitting."
  },
  {
    "objectID": "models.html#model-fitting-3",
    "href": "models.html#model-fitting-3",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Model Fitting",
    "text": "Model Fitting\n\n\nCode\nlog(emissions_ts) |&gt; diff() |&gt; diff(lag=12) |&gt; ggtsdisplay() #this is better\n\n\n\n\n\nCode\n#ggplotly(d) plotting the stationary data's ACF graph\n\n\nThe ACF and PACF graphs are showing that q = 0,1,2, d = 1,2, p = 0,1,2, Q = 0,1,2,3,4 P = 0,1,2,3,4, and D = 1,2\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d,D,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*100),nrow=100)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\n\n\nCode\n# q = 0,1,2, d = 1, p = 0,1,2, Q = 0,1,2,3,4 P = 0,1,2,3,4, and D = 1\noutput1 &lt;- SARIMA.c(p1 = 1, p2 = 3, q1 = 1, q2 = 3, P1 = 1, P2 = 4, Q1 = 1, Q2 = 4, d=1, D=1, data = emissions_ts) |&gt;\n  drop_na()\n\nminaic &lt;- output[which.min(output1$AIC), ]\nminbic &lt;- output[which.min(output1$BIC), ]\n\n\n\nParameters with Minimum AICParameters with Minimum BICUsing auto.arima()\n\n\n\n\nCode\nknitr::kable(minaic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n27\n1\n1\n1\n0\n1\n1\n2813.265\n2822.588\n2813.828\n\n\n\n\n\n\n\n\n\nCode\nknitr::kable(minbic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n27\n1\n1\n1\n0\n1\n1\n2813.265\n2822.588\n2813.828\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(emissions_ts)\n\n\nSeries: emissions_ts \nARIMA(1,1,1)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1    sma2\n      0.3995  -0.7933  0.5958  -0.2730  -1.3767  0.5187\ns.e.  0.0806   0.0575  0.1204   0.0493   0.1205  0.1012\n\nsigma^2 = 291.5:  log likelihood = -2504.01\nAIC=5022.02   AICc=5022.22   BIC=5052.65"
  },
  {
    "objectID": "models.html#model-diagnostics-1",
    "href": "models.html#model-diagnostics-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Model diagnostics",
    "text": "Model diagnostics\nFrom model fitting, we generated 1 model, ARIMA(1,1,1) x (0,1,1). auto.arima() generated ARIMA(1,1,1) x (2,1,2). It looks like ARIMA(1,1,1) x (2,1,2) has the lowest AIC, BIC, and AICc. Now let’s do two model diagnoses to analyze the result and find the better model to do forecast later.\n\nARIMA(1,1,1) x (2,1,2)\n\n\nCode\nfit1 &lt;- Arima(emissions_ts, order=c(1,1,1), seasonal = list(order = c(2,1,2), period = 12))\n\n\n\nModel Fitting Visual Results and Residuals\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(emissions_ts,1,1,1,2,1,2,12))\n\n\n\n\n\n\n\nCode\nemissions_ts |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order = c(2,1,2), period = 12)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)(2,1,2)[12]\nQ* = 26.72, df = 18, p-value = 0.0844\n\nModel df: 6.   Total lags used: 24\n\n\nThe Ljung-Box test uses the following hypotheses:\nH0: The residuals are independently distributed.\nHA: The residuals are not independently distributed; they exhibit serial correlation.\nIdeally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.\n\n\n\nThere is one significant spike in the ACF, and the model fails the Ljung-Box test. The model can still be used for forecasting.\n\n\nModel Fitting\n\n\nCode\nemissions_ts |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order=c(2,1,2), period=12))\n\n\nSeries: emissions_ts \nARIMA(1,1,1)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1    sma2\n      0.3995  -0.7933  0.5958  -0.2730  -1.3767  0.5187\ns.e.  0.0806   0.0575  0.1204   0.0493   0.1205  0.1012\n\nsigma^2 = 291.5:  log likelihood = -2504.01\nAIC=5022.02   AICc=5022.22   BIC=5052.65\n\n\n\n\nModel Output Diagnostics\n\n\nCode\ncat(model_output[52:86], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1    sma2\n      0.3995  -0.7933  0.5958  -0.2730  -1.3767  0.5187\ns.e.  0.0806   0.0575  0.1204   0.0493   0.1205  0.1012\n\nsigma^2 estimated as 288.5:  log likelihood = -2504.01,  aic = 5022.02\n\n$degrees_of_freedom\n[1] 581\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.3995 0.0806   4.9561       0\nma1   -0.7933 0.0575 -13.7845       0\nsar1   0.5958 0.1204   4.9484       0\nsar2  -0.2730 0.0493  -5.5347       0\nsma1  -1.3767 0.1205 -11.4227       0\nsma2   0.5187 0.1012   5.1276       0\n\n$AIC\n[1] 8.555405\n\n$AICc\n[1] 8.555652\n\n$BIC\n[1] 8.607577\n\n\n\n\nCode\nsummary(fit1)\n\n\nSeries: emissions_ts \nARIMA(1,1,1)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1    sma2\n      0.3995  -0.7933  0.5958  -0.2730  -1.3767  0.5187\ns.e.  0.0806   0.0575  0.1204   0.0493   0.1205  0.1012\n\nsigma^2 = 291.5:  log likelihood = -2504.01\nAIC=5022.02   AICc=5022.22   BIC=5052.65\n\nTraining set error measures:\n                      ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set -0.05728885 16.80051 12.80849 -0.06346395 2.182867 0.5794284\n                     ACF1\nTraining set -0.008209425\n\n\n\n\n\nARIMA(1,1,1) x (0,1,1)\n\n\nCode\nfit2 &lt;- Arima(emissions_ts, order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12))\n\n\n\nModel Fitting Visual Results and Residuals\n\n\nCode\nset.seed(123)\nmodel_output2 &lt;- capture.output(sarima(emissions_ts,1,1,1,0,1,1,12))\n\n\n\n\n\n\n\n\n\n\nCode\nemissions_ts |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)(0,1,1)[12]\nQ* = 46.721, df = 21, p-value = 0.001024\n\nModel df: 3.   Total lags used: 24\n\n\nThere is two significant spikes in the ACF, and the model didn’t fail the Ljung-Box test. The model can’t be used for forecasting.\n\n\nModel Fitting\n\n\nCode\nemissions_ts |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12))\n\n\nSeries: emissions_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4287  -0.8131  -0.8291\ns.e.  0.0727   0.0497   0.0218\n\nsigma^2 = 307.2:  log likelihood = -2519.7\nAIC=5047.41   AICc=5047.48   BIC=5064.91\n\n\n\n\nModel Output Diagnostics\n\n\nCode\ncat(model_output2[35:66], model_output2[length(model_output2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sma1\n      0.4287  -0.8131  -0.8291\ns.e.  0.0727   0.0497   0.0218\n\nsigma^2 estimated as 305.6:  log likelihood = -2519.7,  aic = 5047.41\n\n$degrees_of_freedom\n[1] 584\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.4287 0.0727   5.8949       0\nma1   -0.8131 0.0497 -16.3708       0\nsma1  -0.8291 0.0218 -37.9917       0\n\n$AIC\n[1] 8.598649\n\n$AICc\n[1] 8.598719\n\n$BIC\n[1] 8.628462\n\n\n\n\nCode\nsummary(fit2)\n\n\nSeries: emissions_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4287  -0.8131  -0.8291\ns.e.  0.0727   0.0497   0.0218\n\nsigma^2 = 307.2:  log likelihood = -2519.7\nAIC=5047.41   AICc=5047.48   BIC=5064.91\n\nTraining set error measures:\n                      ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set -0.04531625 17.29221 12.97933 -0.06676721 2.213159 0.5871565\n                    ACF1\nTraining set -0.00373604\n\n\n\n\n\n\nModel Selection\nFrom model diagnostics above, ARIMA(1,1,1) x (2,1,2) is also the one auto.arima() produced, and it has less spikes in the ACF and PACF plots of its residuals. The model also has smaller sigma squared means it has smaller variance, which means the estimators with a smaller variance is more efficient. Nontheless, all the training set error measures, especially RMSE, of first model are lower than the ones of the second model.\nTherefore, ARIMA(1,1,1) x (2,1,2) is the better model."
  },
  {
    "objectID": "models.html#forecasting-1",
    "href": "models.html#forecasting-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Forecasting",
    "text": "Forecasting\nLet’s forecast for the next three years using the model we just selected.\n\nforecast Method\n\n\nCode\na &lt;- fit1 |&gt; forecast(h=80) |&gt; autoplotly() + ggtitle(\"Monthy CO2 Emissions Five-Year-Forecasting\") + xlab(\"Year\") + ylab(\"Million Metric Tons of Carbon Dioxide\")\na %&gt;%\n  layout(showlegend = F, title='Time Series with Rangeslider',\n         xaxis = list(rangeslider = list(visible = T)))\n\n\n\n\n\n\n\n\nsarima.for() Method\n\n\nCode\nsarima.for(emissions_ts, 60, 1,1,1,2,1,2,12)\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023 618.7850 522.7436 523.7467 456.6446 484.6787 525.0852 592.4839 588.2336\n2024 595.4889 510.3660 509.0163 439.2943 467.0804 511.1400 581.6272 578.6118\n2025 581.2447 504.6408 496.4980 429.0742 455.7201 501.7074 571.3342 569.9160\n2026 574.3190 499.8097 488.2619 422.9227 448.9569 495.0954 563.3664 562.5627\n2027 569.2822 493.6951 481.9732 417.2486 443.2297 488.9319 556.6300 555.7564\n          Sep      Oct      Nov      Dec\n2023 518.0627 499.4276 511.9846 573.6319\n2024 507.6640 490.8693 500.3659 555.9238\n2025 496.9108 480.3016 490.3520 544.5836\n2026 488.5437 471.5427 482.7586 537.8624\n2027 481.6952 464.4100 476.1690 532.1546\n\n$se\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023 16.98534 19.86316 21.27671 22.26454 23.09347 23.85051 24.56782 25.25844\n2024 29.43616 30.53130 31.44341 32.27622 33.06763 33.83262 34.57761 35.30568\n2025 38.73656 39.38616 40.02545 40.65479 41.27458 41.88521 42.48707 43.08052\n2026 46.16912 46.85184 47.48856 48.10286 48.70392 49.29551 49.87923 50.45586\n2027 53.80342 54.64802 55.39405 56.09763 56.77985 57.44901 58.10852 58.75985\n          Sep      Oct      Nov      Dec\n2023 25.92822 26.58017 27.21614 27.83744\n2024 36.01857 36.71743 37.40316 38.07652\n2025 43.66591 44.24356 44.81376 45.37679\n2026 51.02584 51.58947 52.14699 52.69860\n2027 59.40374 60.04060 60.67073 61.29436"
  },
  {
    "objectID": "models.html#compare-with-benchmark-method-1",
    "href": "models.html#compare-with-benchmark-method-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Compare with benchmark method",
    "text": "Compare with benchmark method\n\n\nCode\nvalues &lt;- append(emissions_df$sum, rep(NA,80))\nemissions_ts_df &lt;- data.frame(Year = seq(as.Date(\"1973/1/1\"), by = \"month\", length.out = 680), Million.metric.tons=values)\n\nemissions_ts_df$meanf &lt;- append(rep(NA,600), meanf(emissions_ts, h=80)$mean)\nemissions_ts_df$naive &lt;- append(rep(NA,600), naive(emissions_ts, h=80)$mean)\nemissions_ts_df$snaive &lt;- append(rep(NA,600), snaive(emissions_ts, h=80)$mean)\nemissions_ts_df$rwf &lt;- append(rep(NA,600), rwf(emissions_ts, h=80, drift=TRUE)$mean)\nemissions_ts_df$fit &lt;- append(rep(NA,600), forecast(fit1, h=80)$mean)\n\np &lt;- ggplot(emissions_ts_df) + \n  geom_line(aes(x=Year, y = Million.metric.tons)) + \n  geom_line(aes(x=Year, y = meanf, color = \"Mean\")) +\n  geom_line(aes(x=Year, y = naive, color = \"Naïve\")) +\n  geom_line(aes(x=Year, y = snaive, color = \"SNaïve\")) +\n  geom_line(aes(x=Year, y = rwf, color = \"Drift\")) +\n  geom_line(aes(x=Year, y = fit, color = \"Model\")) +\n  ggtitle(\"Comparison of the Fitted Model and Benchmark Methods\") +\n  ylab(\"Million Metric Tons of Carbon Dioxide\")\n  \nggplotly(p)\n\n\n\n\n\n\nBenchmark method is for data scientists to keep their sanity when building models, they set a baseline — a score that the model must outperform. Normally, the state-of-the-art is used as the baseline but for problems with no existing solutions yet, one should build their own baseline.\n\nAverage Method\nThis method simply takes the average (or “mean”) value of the entire historical data and use that to forecast future values. Very useful for data with small variance or whose value lies close to the mean.\n\n\nDrift Method\nDrift is the amount of change observed from the data. In this method, drift is set to be the average change seen in the whole historical data and uses that to forecast values in the future. Basically, this just means drawing a straight line using the first and last values and extend that line into the future. This method works well on data that follows a general trend over time.\n\n\nNaïve Method\nThis method uses the most recent value as the forecasted value for the next time step. The assumption followed by this method is that its value tomorrow is equal to its value today.\n\n\nComparison Conclusion\nThe fitted model has outperformed the rest of the benchmark methods such us the mean method, naive methods, and drift method."
  },
  {
    "objectID": "models2.html#plotting-the-data",
    "href": "models2.html#plotting-the-data",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Plotting the Data",
    "text": "Plotting the Data\n\n\nCode\nemissions_df&lt;-read.csv(\"data/df_total_monthly_CO2_emissions.csv\", skip=1, row.names = 1)\nrenewable_df &lt;- read.csv(\"data/df_total_monthly_renewable_consumption.csv\", skip=1, row.names = 1)\nnaturalgas_df &lt;- read.csv(\"data/df_total_monthly_natural_gas_consumption.csv\", skip=1, row.names = 1)\ncoal_df &lt;- read.csv(\"data/df_total_monthly_coal_consumption.csv\", row.names = 1)\npetroleum_df &lt;- read.csv(\"data/df_total_monthly_df_monthly_petroleum_consumption.csv\", skip=1, row.names = 1)\n\ndd &lt;- data.frame(emissions_df$X.1,emissions_df$sum,renewable_df$sum,petroleum_df$sum,naturalgas_df$sum,coal_df$value)\n#dd&lt;-dd[,c(1,2,4,6)]\ncolnames(dd)&lt;-c(\"DATE\",\"CO2 Emissions\",\"Renewable\",\"Petroleum\",\"Natural Gas\",\"Coal\")\nknitr::kable(head(dd))\n\n\n\n\n\nDATE\nCO2 Emissions\nRenewable\nPetroleum\nNatural Gas\nCoal\n\n\n\n\n1973-01\n565.577\n403.982\n3185.269\n2395.856\n871.911\n\n\n1973-02\n514.236\n360.901\n2941.986\n2169.382\n692.252\n\n\n1973-03\n506.524\n400.161\n2942.485\n2056.331\n676.722\n\n\n1973-04\n461.479\n380.470\n2621.315\n1872.777\n785.359\n\n\n1973-05\n474.085\n392.142\n2836.568\n1764.857\n999.503\n\n\n1973-06\n469.615\n377.232\n2731.435\n1566.171\n1021.716\n\n\n\n\n\n\n\nCode\ndd.ts&lt;-ts(dd,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nautoplotly(dd.ts[,c(2:6)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Variables influencing CO2 Emissions in USA\")\n\n\n\n\n\n\nAs stated in literature review, we are interested in the impacts of U.S. major energy sources on U.S. total CO2 emissions. The energy data were collected and have been shown in the data sources tab. We further used these data and build models and predictions in ARMA/ARIMA/SARIMA Models tab. The unit of CO2 emissions is million metric tons of carbon dioxide, the unit of renewable energy, petroleum, natural gas, and coal is Trillion Btu. The time series starts from Jan 1973 and ends in yet includes Dec 2022."
  },
  {
    "objectID": "models2.html#fitting-the-model-using-auto.arima",
    "href": "models2.html#fitting-the-model-using-auto.arima",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting the model using auto.arima()",
    "text": "Fitting the model using auto.arima()\nUsing auto.arima() function here to fit the ARIMAX model. Here we are trying to predict CO2 Emissions using Renewable Energy, Petroleum, Natural gas, and Coal. All variables are time series and the exogenous variables in this case are Renewable Energy, Petroleum, Natural gas, and Coal.\n\n\nCode\n# \"CO2 Emissions\",\"Renewable\",\"Petroleum\",\"Natural Gas\",\"Coal\"\nxreg &lt;- cbind(Rnwbl = dd.ts[, \"Renewable\"],\n              Ptlm = dd.ts[, \"Petroleum\"],\n              Ntlgs = dd.ts[, \"Natural Gas\"],\n              Cl = dd.ts[, \"Coal\"])\n\nfit &lt;- auto.arima(dd.ts[, \"CO2 Emissions\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: dd.ts[, \"CO2 Emissions\"] \nRegression with ARIMA(4,0,0)(0,1,1)[12] errors \n\nCoefficients:\n         ar1     ar2     ar3     ar4     sma1    Rnwbl    Ptlm   Ntlgs       Cl\n      0.7384  0.0817  0.0343  0.1269  -0.7452  -0.0017  0.0728  0.1218  -0.0065\ns.e.  0.0417  0.0510  0.0512  0.0412   0.0287   0.0166  0.0045  0.0040   0.0069\n\nsigma^2 = 85.53:  log likelihood = -2143.09\nAIC=4306.19   AICc=4306.57   BIC=4349.95\n\nTraining set error measures:\n                     ME     RMSE      MAE         MPE    MAPE      MASE\nTraining set -0.1968328 9.084863 6.881898 -0.03815555 1.18767 0.3113222\n                     ACF1\nTraining set -0.003652219\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(4,0,0)(0,1,1)[12] errors\nQ* = 33.387, df = 19, p-value = 0.02168\n\nModel df: 5.   Total lags used: 24\n\n\nFrom the results above, we know that this is a regression model with ARIMA(4,0,0)(0,1,1)[12]."
  },
  {
    "objectID": "models2.html#fitting-the-model-manually",
    "href": "models2.html#fitting-the-model-manually",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting the model manually",
    "text": "Fitting the model manually\nHere we first have to fit the linear regression model predicting CO2 Emissions using Renewable Energy, Petroleum, Natural gas, and Coal.\nThen for the residuals, we will fit an ARIMA/SARIMA model.\n\nFirst fit the linear model\n\n\nCode\ndd$`CO2 Emissions` &lt;- ts(dd$`CO2 Emissions`, star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$Renewable &lt;- ts(dd$Renewable,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$Petroleum &lt;-ts(dd$Petroleum,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$`Natural Gas` &lt;-ts(dd$`Natural Gas`,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$Coal &lt;-ts(dd$Coal,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n# First fit the linear model\nfit.reg &lt;- lm(`CO2 Emissions` ~ Renewable + Petroleum + `Natural Gas` + Coal, data=dd)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = `CO2 Emissions` ~ Renewable + Petroleum + `Natural Gas` + \n    Coal, data = dd)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-107.596  -27.274   -1.138   24.142  126.391 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.015e+02  2.092e+01  -9.630  &lt; 2e-16 ***\nRenewable     -1.098e-01  1.316e-02  -8.348 4.88e-16 ***\nPetroleum      1.957e-01  6.690e-03  29.252  &lt; 2e-16 ***\n`Natural Gas`  5.652e-02  4.124e-03  13.705  &lt; 2e-16 ***\nCoal           1.088e-01  4.877e-03  22.308  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.52 on 595 degrees of freedom\nMultiple R-squared:  0.7462,    Adjusted R-squared:  0.7445 \nF-statistic: 437.3 on 4 and 595 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nThen look at the residuals\n\nOriginal Time Series Residuals\n\n\n\n\nCode\nres.fit &lt;- ts(residuals(fit.reg), star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n\nggtsdisplay(res.fit)\n\n\n\n\n\n\nFirst Diff\n\n\nCode\nres.fit |&gt; diff() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nFirst Diff and Seasonality Diff\n\n\nCode\nres.fit |&gt; diff() |&gt; diff(lag=12) |&gt; ggtsdisplay()\n\n\n\n\n\n\n\n\n\n\n\nStationarity and Seasonality Test\n\n\nCode\nres.fit |&gt; diff() |&gt; diff(lag=12) |&gt; adf.test()\n\n\nWarning in adf.test(diff(diff(res.fit), lag = 12)): p-value smaller than printed\np-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(diff(res.fit), lag = 12)\nDickey-Fuller = -10.178, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nCode\ndiff.res.fit &lt;- res.fit |&gt; diff() |&gt; diff(lag=12) \nisSeasonal(diff.res.fit, test = \"combined\", freq = NA)\n\n\n[1] FALSE\n\n\nAfter first differencing and seasonality differencing, the data is proved to be stationary. Now let’s find the model parameters of the time series linear regression residual data.\n\n\nFind the Model Parameters\n\n\nCode\n#q=1,2, Q=1,2 , p=1,2, P=0,1,2,3\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*200),nrow=200)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n       \n          if(p+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\n\n\nCode\n#q=1,2, Q=1,2 , p=0,1,2, P=0,1,2,3 \n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,data=res.fit) |&gt;\n  drop_na()\n\nminaic &lt;- output[which.min(output$AIC), ]\nminbic &lt;- output[which.min(output$BIC), ]\n\n\n\nParameters with Minimum AICParameters with Minimum BICUsing auto.arima()\n\n\n\n\nCode\nknitr::kable(minaic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n25\n1\n1\n1\n0\n1\n1\n4877.404\n4894.904\n4877.473\n\n\n\n\n\n\n\n\n\nCode\nknitr::kable(minbic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n25\n1\n1\n1\n0\n1\n1\n4877.404\n4894.904\n4877.473\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(res.fit)\n\n\nSeries: res.fit \nARIMA(2,0,1)(2,1,1)[12] \n\nCoefficients:\n        ar1      ar2      ma1    sar1    sar2     sma1\n      1.392  -0.4079  -0.7879  0.0674  -0.072  -0.8284\ns.e.  0.079   0.0732   0.0580  0.0506   0.049   0.0304\n\nsigma^2 = 226.2:  log likelihood = -2432.01\nAIC=4878.03   AICc=4878.22   BIC=4908.67\n\n\n\n\n\nBest model from the output is ARIMA(1,1,1)x(0,1,1)[12]. auto.arima() suggested ARIMA(2,0,1)(2,1,1)[12]\n\n\n\nModel diagnostics\nFrom model fitting, we generated 1 model, ARIMA((1,1,1)x(0,1,1). auto.arima() generated ARIMA(2,0,1) x (2,1,1). It looks like ARIMA(1,1,1)x(0,1,1) has the lower AIC, BIC, and AICc. Now let’s do two model diagnoses to analyze the result and find the better model to do forecast later.\n\nARIMA(2,0,1)(2,1,1)\n\n\nCode\nset.seed(1234)\nfit1 &lt;- Arima(res.fit, order=c(2,0,1), seasonal = list(order = c(2,1,1), period = 12))\n\n\n\n\nModel Fitting Visual Results and Residuals\n\n\nCode\nmodel_output &lt;- capture.output(sarima(res.fit,2,0,1,2,1,1,12))\n\n\n\n\n\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(2,0,1), seasonal = list(order = c(2,1,1), period = 12)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,0,1)(2,1,1)[12]\nQ* = 36.154, df = 18, p-value = 0.006743\n\nModel df: 6.   Total lags used: 24\n\n\nThe Ljung-Box test uses the following hypotheses:\nH0: The residuals are independently distributed.\nHA: The residuals are not independently distributed; they exhibit serial correlation.\nIdeally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.\n\n\n\nThere are two significant spikes in the ACF, and the model didn’t fail the Ljung-Box test.\n\n\nModel Fitting\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(2,0,1), seasonal = list(order=c(2,1,1), period=12))\n\n\nSeries: res.fit \nARIMA(2,0,1)(2,1,1)[12] \n\nCoefficients:\n        ar1      ar2      ma1    sar1    sar2     sma1\n      1.392  -0.4079  -0.7879  0.0674  -0.072  -0.8284\ns.e.  0.079   0.0732   0.0580  0.0506   0.049   0.0304\n\nsigma^2 = 226.2:  log likelihood = -2432.01\nAIC=4878.03   AICc=4878.22   BIC=4908.67\n\n\n\n\nModel Output Diagnostics\n\n\nCode\ncat(model_output[65:100], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1    sar1     sar2     sma1  constant\n      1.3922  -0.4080  -0.7880  0.0674  -0.0721  -0.8284    0.0019\ns.e.  0.0791   0.0733   0.0582  0.0506   0.0490   0.0304    0.1239\n\nsigma^2 estimated as 223.9:  log likelihood = -2432.01,  aic = 4880.03\n\n$degrees_of_freedom\n[1] 581\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        1.3922 0.0791  17.6037  0.0000\nar2       -0.4080 0.0733  -5.5655  0.0000\nma1       -0.7880 0.0582 -13.5486  0.0000\nsar1       0.0674 0.0506   1.3319  0.1834\nsar2      -0.0721 0.0490  -1.4715  0.1417\nsma1      -0.8284 0.0304 -27.2534  0.0000\nconstant   0.0019 0.1239   0.0154  0.9877\n\n$AIC\n[1] 8.29937\n\n$AICc\n[1] 8.299698\n\n$BIC\n[1] 8.358917\n\n\n\n\nCode\nsummary(fit1)\n\n\nSeries: res.fit \nARIMA(2,0,1)(2,1,1)[12] \n\nCoefficients:\n        ar1      ar2      ma1    sar1    sar2     sma1\n      1.392  -0.4079  -0.7879  0.0674  -0.072  -0.8284\ns.e.  0.079   0.0732   0.0580  0.0506   0.049   0.0304\n\nsigma^2 = 226.2:  log likelihood = -2432.01\nAIC=4878.03   AICc=4878.22   BIC=4908.67\n\nTraining set error measures:\n                      ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.006210697 14.81186 11.34788 -11.06374 110.0339 0.6029853\n                    ACF1\nTraining set -0.01093927\n\n\n\n\n\nARIMA(1,1,1)x(0,1,1)\n\n\nCode\nfit2 &lt;- Arima(res.fit , order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12))\n\n\n\nModel Fitting Visual Results and Residuals\n\n\nCode\nmodel_output2 &lt;- capture.output(sarima(res.fit,1,1,1,0,1,1,12))\n\n\n\n\n\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)(0,1,1)[12]\nQ* = 45.782, df = 21, p-value = 0.001366\n\nModel df: 3.   Total lags used: 24\n\n\nThere is one significant spike in the ACF, and the model didn’t fail the Ljung-Box test.\n\n\nModel Fitting\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12))\n\n\nSeries: res.fit \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4485  -0.8254  -0.8340\ns.e.  0.0637   0.0409   0.0232\n\nsigma^2 = 229.8:  log likelihood = -2434.7\nAIC=4877.4   AICc=4877.47   BIC=4894.9\n\n\n\n\nModel Output Diagnostics\n\n\nCode\ncat(model_output2[30:61], model_output2[length(model_output2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sma1\n      0.4485  -0.8254  -0.8340\ns.e.  0.0637   0.0409   0.0232\n\nsigma^2 estimated as 228.6:  log likelihood = -2434.7,  aic = 4877.4\n\n$degrees_of_freedom\n[1] 584\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.4485 0.0637   7.0433       0\nma1   -0.8254 0.0409 -20.2005       0\nsma1  -0.8340 0.0232 -35.8717       0\n\n$AIC\n[1] 8.309036\n\n$AICc\n[1] 8.309106\n\n$BIC\n[1] 8.338849\n\n\n\n\nCode\nsummary(fit2)\n\n\nSeries: res.fit \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4485  -0.8254  -0.8340\ns.e.  0.0637   0.0409   0.0232\n\nsigma^2 = 229.8:  log likelihood = -2434.7\nAIC=4877.4   AICc=4877.47   BIC=4894.9\n\nTraining set error measures:\n                     ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -0.2983109 14.95631 11.52741 -17.96732 111.7469 0.6125249\n                    ACF1\nTraining set -0.01297584\n\n\n\n\n\n\nModel Selection\nFrom two model diagnostics above, both ARIMA(1,1,1)(0,1,1) and ARIMA(1,1,1) x (2,1,2) model have similar number of spikes in the ACF and PACF plots of its residuals. All the training set error measures of the two models are similar. ARIMA(1,1,1)(0,1,1) model has a slightly smaller sigma squared which means it has smaller variance. The estimators with a smaller variance is more efficient.\n\n\nCross Validation\n\n\nCode\nn=length(res.fit)\nk=120\n \n #n-k=480; 480/12=40;\n \nrmse1 &lt;- matrix(NA, 40,12)\nrmse2 &lt;- matrix(NA,40,12)\nrmse3 &lt;- matrix(NA,40,12)\n\nst &lt;- tsp(res.fit)[1]+(k-1)/12 \n\nfor(i in 1:10)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(res.fit, end=st + i-1)\n  xtest &lt;- window(res.fit, start=st + (i-1) + 1/12, end=st + i)\n  \n  #ARIMA(1,1,2)x(0,1,0)[12]. auto.arima() - ARIMA(2,0,1)(2,1,1)[12]\n  \n  fit &lt;- Arima(xtrain, order=c(2,0,1), seasonal=list(order=c(2,1,1), period=12),\n                include.drift=TRUE, method=\"CSS\")\n  fcast &lt;- forecast(fit, h=40)\n  \n  fit2 &lt;- Arima(xtrain, order=c(1,1,2), seasonal=list(order=c(0,1,0), period=12),\n                include.drift=TRUE, method=\"CSS\")\n  fcast2 &lt;- forecast(fit2, h=40)\n  \n  fit3 &lt;- Arima(xtrain, order=c(2,1,1), seasonal=list(order=c(2,1,1), period=12),\n                include.drift=TRUE, method=\"CSS\")\n  fcast3 &lt;- forecast(fit3, h=40)\n  \n\n  rmse1[i,1:length(xtest)]  &lt;- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] &lt;- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] &lt;- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:12, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:12, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\nThis fit is good based on low RMSE: ARIMA(2,1,1)x(2,1,1)[12]\n\n\nModel Fitting: ARIMA(2,1,1)x(2,1,1)[12]\n\n\nCode\nxreg &lt;- cbind(RNWBL = dd[, \"Renewable\"],\n              PTRLM = dd[, \"Petroleum\"],\n              NTRLG = dd[, \"Natural Gas\"],\n              CL = dd[, \"Coal\"])\n\n\nfit &lt;- Arima(dd$`CO2 Emissions`,order=c(2,1,1),seasonal = c(2,1,1),xreg=xreg,method=\"CSS\")\nsummary(fit)\n\n\nSeries: dd$`CO2 Emissions` \nRegression with ARIMA(2,1,1)(2,1,1)[12] errors \n\nCoefficients:\n         ar1      ar2      ma1     sar1     sar2     sma1    RNWBL   PTRLM\n      0.4660  -0.0163  -0.7517  -0.0979  -0.1301  -0.6789  -0.0048  0.0749\ns.e.  0.0729   0.0479   0.0671   0.0587   0.0499   0.0446   0.0165  0.0048\n       NTRLG       CL\n      0.1204  -0.0080\ns.e.  0.0042   0.0072\n\nsigma^2 = 84.89:  log likelihood = -2144.72\n\nTraining set error measures:\n                     ME    RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.6001355 9.03536 6.662378 -0.1172099 1.143552 0.3013916\n                    ACF1\nTraining set 0.009465643"
  },
  {
    "objectID": "models2.html#forecasting-using-auto.arma",
    "href": "models2.html#forecasting-using-auto.arma",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting Using auto.arma()",
    "text": "Forecasting Using auto.arma()\nIn order to forecast CO2 Emissions variable, or the whole fit, we need to have forecasts of Renewable Energy, Petroleum, Natural gas, and Coal.\nHere we will be using auto.arima() to fit the Renewable Energy, Petroleum, Natural gas, and Coal variables.\n\nFitting ARIMA model to different energy consumption variables\n\nRenewable Energy\n\n\n\n\nCode\nRNWBL_fit &lt;- auto.arima(dd$Renewable) #fiting an ARIMA model to the CO2 emissions variable\nsummary(RNWBL_fit)\n\n\nSeries: dd$Renewable \nARIMA(0,1,2)(1,1,2)[12] \n\nCoefficients:\n          ma1      ma2    sar1     sma1    sma2\n      -0.2496  -0.1966  0.3548  -1.1398  0.2403\ns.e.   0.0408   0.0414  0.5676   0.5770  0.4817\n\nsigma^2 = 534.4:  log likelihood = -2681.06\nAIC=5374.11   AICc=5374.26   BIC=5400.36\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.2969799 22.76701 17.49176 -0.0738994 3.172679 0.4860948\n                    ACF1\nTraining set 0.006186365\n\n\n\n\nCode\nfRNWBL &lt;- forecast(RNWBL_fit)\n\n\n\nPetroleum\n\n\nCode\nPTRLM_fit &lt;- auto.arima(dd$Petroleum) #fiting an ARIMA model to the petroleum variable\nsummary(PTRLM_fit)\n\n\nSeries: dd$Petroleum \nARIMA(3,0,2)(2,1,1)[12] \n\nCoefficients:\n          ar1     ar2     ar3     ma1     ma2    sar1     sar2     sma1\n      -0.0239  0.2107  0.6388  0.7031  0.4326  0.0213  -0.0886  -0.8043\ns.e.   0.1010  0.0813  0.1060  0.1306  0.1478  0.0558   0.0511   0.0355\n\nsigma^2 = 7153:  log likelihood = -3446.92\nAIC=6911.83   AICc=6912.14   BIC=6951.22\n\nTraining set error measures:\n                   ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set 1.419845 83.15279 59.65203 -0.02073471 2.058544 0.5540171\n                    ACF1\nTraining set -0.03071941\n\n\n\n\nCode\nfPTRLM &lt;- forecast(PTRLM_fit)\n\n\n\n\nNatural Gas\n\n\nCode\nNTRLG_fit &lt;- auto.arima(dd$`Natural Gas`) #fiting an ARIMA model to the petroleum variable\nsummary(NTRLG_fit)\n\n\nSeries: dd$`Natural Gas` \nARIMA(2,1,1)(1,1,2)[12] \n\nCoefficients:\n         ar1      ar2      ma1     sar1     sma1     sma2\n      0.4262  -0.0133  -0.9085  -0.4149  -0.2781  -0.4480\ns.e.  0.0475   0.0455   0.0236   0.1921   0.1801   0.1392\n\nsigma^2 = 9115:  log likelihood = -3512.73\nAIC=7039.46   AICc=7039.66   BIC=7070.09\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 6.345775 93.94888 69.62737 0.09510603 3.560609 0.6725354\n                     ACF1\nTraining set -0.005717725\n\n\n\n\nCode\nfNTRLG &lt;- forecast(NTRLG_fit)\n\n\n\n\nCoal\n\n\nCode\nCL_fit &lt;- auto.arima(dd$Coal) #fiting an ARIMA model to the petroleum variable\nsummary(CL_fit)\n\n\nSeries: dd$Coal \nARIMA(2,1,2)(2,1,1)[12] \n\nCoefficients:\n          ar1     ar2      ma1      ma2    sar1     sar2     sma1\n      -0.1566  0.4381  -0.1342  -0.6282  0.0155  -0.1504  -0.7816\ns.e.   0.5999  0.2635   0.5924   0.4288  0.0520   0.0481   0.0331\n\nsigma^2 = 2982:  log likelihood = -3184.71\nAIC=6385.42   AICc=6385.67   BIC=6420.43\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -1.378761 53.69276 39.66915 -0.1088342 2.908254 0.5045871\n                     ACF1\nTraining set -0.001052996\n\n\n\n\nCode\nfCL &lt;- forecast(CL_fit)\n\n\n\n\n\n\n\n\nCode\nfxreg &lt;- cbind(RNWBL = fRNWBL$mean,\n               PTRLM = fPTRLM$mean,\n               NTRLG = fNTRLG$mean,\n               CL = fCL$mean)\n\nfcast &lt;- forecast(fit, xreg=fxreg) #fimp$mean gives the forecasted values\ng &lt;- autoplot(fcast) + \n  xlab(\"Year\") +\n  ylab(\"Million Metric Tons of Carbon Dioxide\") +\n  ggtitle(\"CO2 Emissions Forecast from ARIMAX Model\")\nggplotly(g)"
  },
  {
    "objectID": "fin_ts_models.html",
    "href": "fin_ts_models.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "In a slight detour in this project, this section concerns fitting time series data with statistical models that concern the variance in the data. These models are called AutoRegressive Conditional Heteroskedasticity (ARCH) and Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models and are used when there is an autoregressive term in the error variance. They are typically used when modeling time-series data that exhibit volatility, which happens most often in financial time series. Therefore, for this section, a set of stock data is retrieved and analyzed using a combination of ARCH/GARCH and ARIMA."
  },
  {
    "objectID": "fin_ts_models.html#stock-prices-over-the-years",
    "href": "fin_ts_models.html#stock-prices-over-the-years",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Stock Prices Over The Years",
    "text": "Stock Prices Over The Years\n\nGather NextEra Energy stocks from Yahoo Fiance\nNEE is the ticker symbol for NextEra Energy’s stock.\n\n\nCode\nNEE_ALL <- getSymbols(\"NEE\",auto.assign = FALSE, from = \"2019-01-01\",to = \"2023-04-13\",src=\"yahoo\")\nNEE_ALL=data.frame(NEE_ALL)\nNEE_ALL <- data.frame(NEE_ALL, rownames(NEE_ALL))\ncolnames(NEE_ALL)[7] = \"Year\"\nNEE_ALL$Year <- as.Date(NEE_ALL$Year,\"%Y-%m-%d\")\nknitr::kable(head(NEE_ALL,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNEE.Open\nNEE.High\nNEE.Low\nNEE.Close\nNEE.Volume\nNEE.Adjusted\nYear\n\n\n\n\n2019-01-02\n43.1700\n43.3250\n42.2525\n42.4575\n10549600\n38.75114\n2019-01-02\n\n\n2019-01-03\n42.4775\n42.7900\n42.1675\n42.3525\n9260800\n38.65530\n2019-01-03\n\n\n2019-01-04\n42.2875\n43.1475\n42.1650\n43.1325\n10848800\n39.36721\n2019-01-04\n\n\n\n\n\n\n\nVisualize NEE Stock\n\n\nCode\np <- NEE_ALL |>\n  ggplot() +\n  geom_line(aes(y=NEE.Adjusted, x=Year),color=\"aquamarine3\") +\n  ggtitle(\"NextEra Energy Stock Pirces\") +\n  ylab(\"USD\")\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCandlestick Plot\n\n\nCode\n#plotly\ntheme_set(theme_gray(base_size=12,base_family=\"Palatino\"))\nfig <- NEE_ALL %>% plot_ly(x = ~Year, type=\"candlestick\",\n          open = ~NEE.Open, close = ~NEE.Close,\n          high = ~NEE.High, low = ~NEE.Low, name=\"NEE\") \n\nfig <- fig %>% add_lines(x = ~Year, y = ma(NEE_ALL$NEE.Adjusted, 20), name=\"20-MA\", line = list(color = 'deeppink', width = 1), inherit = F)\n\nfig <- fig %>% add_lines(x = ~Year, y = ma(NEE_ALL$NEE.Adjusted, 80), name=\"80-MA\", line = list(color = 'cyan3', width = 1), inherit = F)\n\nfig <- fig %>% layout(title = \"Basic Candlestick Chart\", legend = list(orientation = 'h', x = 0.5, y = 1,xanchor = 'center', yref = 'paper', font = list(size = 10), bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nBeofrore doing ARCH/GARCH models on NextEra stock, let’s take a look at its one of the most beneficial company Tesla’s stock."
  },
  {
    "objectID": "fin_ts_models.html#squared-returns",
    "href": "fin_ts_models.html#squared-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Squared returns",
    "text": "Squared returns\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"TLSA\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2021-01-01\",\n             to = \"2023-03-31\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(TLSA$TLSA.Adjusted)\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\n\nMoving Average Plot\n\n\nCode\nts.stock &lt;- ts(stock$TLSA, star=decimal_date(as.Date(\"2021-01-01\",format = \"%Y-%m-%d\")),frequency = 365.25)\nautoplot(ts.stock, series=\"Data\") +\n  autolayer(ma(ts.stock, 20), series=\"20-MA\") +\n  autolayer(ma(ts.stock, 80), series=\"80-MA\") +\n  xlab(\"Date\") + ylab(\"Dollars\") +\n  ggtitle(\"Tesla Stock Price\")+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"20-MA\"=\"black\", \"80-MA\" = \"blue\"),\n                      breaks=c(\"Data\",\"20-MA\", \"80-MA\"))\n\n\n\n\n\n\n\nCandlestick Plot\n\n\nCode\ndf &lt;- data.frame(Date=index(TLSA),coredata(TLSA))\n\ntheme_set(theme_gray(base_size=12,base_family=\"Palatino\"))\nfig &lt;- df %&gt;% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~TLSA.Open, close = ~TLSA.Close,\n          high = ~TLSA.High, low = ~TLSA.Low, name=\"MSFT\") \n\nfig &lt;- fig %&gt;% add_lines(x = ~Date, y = ma(TLSA$TLSA.Adjusted, 20), name=\"20-MA\", line = list(color = 'black', width = 1), inherit = F)\n\nfig &lt;- fig %&gt;% add_lines(x = ~Date, y = ma(TLSA$TLSA.Adjusted, 80), name=\"80-MA\", line = list(color = 'blue', width = 1), inherit = F)\n\nfig &lt;- fig %&gt;% layout(title = \"Basic Candlestick Chart\", legend = list(orientation = 'h', x = 0.5, y = 1,xanchor = 'center', yref = 'paper', font = list(size = 10), bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\nCode\nTSLA &lt;- getSymbols(\"TSLA\",auto.assign = FALSE, from = \"2021-01-01\",src=\"yahoo\")\n#since russia invaded ukrain on Feb 24th\nchartSeries(TSLA, theme = chartTheme(\"white\"), # Theme\n            bar.type = \"hlc\",  # High low close \n            up.col = \"green\",  # Up candle color\n            dn.col = \"red\")   # Down candle color)\n\n\n\n\n\n\n\nLook at the whole dataset\n\n\nCode\nTSLA_ALL &lt;- getSymbols(\"TSLA\",auto.assign = FALSE, from = \"2018-01-01\",src=\"yahoo\")\nchartSeries(TSLA_ALL, theme = chartTheme(\"white\"), # Theme\n            bar.type = \"hlc\",  # High low close \n            up.col = \"green\",  # Up candle color\n            dn.col = \"red\")   # Down candle color)\n\n\n\n\n\n\n\nLog Differencing\n\n\nCode\nlog(TSLA_ALL$`TSLA.Adjusted`) %&gt;% diff() %&gt;% chartSeries()\n\n\n\n\n\n\n\nCode\nlog.TSLA=log(TSLA_ALL$`TSLA.Adjusted`)\nplot(log.TSLA,type='l',main='Log TSLA Stock')\n\n\n\n\n\n\n\nCode\n#str(bitc_ALL$`BTC-USD.Adjusted`)\nbts &lt;- ts(TSLA_ALL$`TSLA.Adjusted`, start=decimal_date(as.Date(\"2018-01-01\")), frequency = 365.25)\n\n\n\n\nCalculating Returns\n\n\nCode\nreturns = log(bts) %&gt;% diff()\nautoplot(returns) +ggtitle(\"Returns\")\n\n\n\n\n\n\n\nACF, PACF plots of the returns\n\n\nCode\nggAcf(returns)\n\n\n\n\n\nCode\nacf(returns)\n\n\n\n\n\n\n\nCode\nggPacf(returns)\n\n\n\n\n\nCode\npacf(returns)\n\n\n\n\n\n\n\nACF of absolute values of the returns and squared values\n\n\nCode\nacf(abs(returns))\n\n\n\n\n\n\n\nCode\nacf(returns^2)\n\n\n\n\n\n\n\nModel Fitting - ARCH Model\nLet’s look at the PACF to find p for ARCH(p)\n\n\nCode\npacf(returns^2) #p=1,2,3,4\n\n\n\n\n\nFit a few models\n\n\nCode\narch.fit1 &lt;- garchFit(~garch(1,0), data = returns, trace = F)\nsummary(arch.fit1)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n&lt;environment: 0x7fafba182240&gt;\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n0.0012021  0.0015321  0.0924612  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.202e-03   1.108e-03    1.085 0.278128    \nomega  1.532e-03   6.823e-05   22.456  &lt; 2e-16 ***\nalpha1 9.246e-02   2.699e-02    3.426 0.000613 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2371.129    normalized:  1.780127 \n\nDescription:\n Thu Apr 20 06:04:28 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  686.3096  0           \n Shapiro-Wilk Test  R    W      0.9563378 0           \n Ljung-Box Test     R    Q(10)  13.88928  0.1781022   \n Ljung-Box Test     R    Q(15)  18.17084  0.2537905   \n Ljung-Box Test     R    Q(20)  30.06613  0.06878914  \n Ljung-Box Test     R^2  Q(10)  93.45277  1.110223e-15\n Ljung-Box Test     R^2  Q(15)  124.364   0           \n Ljung-Box Test     R^2  Q(20)  153.8659  0           \n LM Arch Test       R    TR^2   77.6327   1.164857e-11\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.555750 -3.544051 -3.555760 -3.551366 \n\n\n\n\nCode\narch.fit2 &lt;- garchFit(~garch(2,0), data = returns, trace = F)\nsummary(arch.fit2)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 0), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 0)\n&lt;environment: 0x7fafab20f918&gt;\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1     alpha2  \n0.0021749  0.0012053  0.0859827  0.2056145  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     2.175e-03   1.054e-03    2.063 0.039083 *  \nomega  1.205e-03   6.734e-05   17.900  &lt; 2e-16 ***\nalpha1 8.598e-02   2.476e-02    3.472 0.000516 ***\nalpha2 2.056e-01   4.009e-02    5.129 2.91e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2404.942    normalized:  1.805512 \n\nDescription:\n Thu Apr 20 06:04:28 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  363.4213  0           \n Shapiro-Wilk Test  R    W      0.9658694 0           \n Ljung-Box Test     R    Q(10)  12.87265  0.2308795   \n Ljung-Box Test     R    Q(15)  18.591    0.2328694   \n Ljung-Box Test     R    Q(20)  28.39785  0.1003109   \n Ljung-Box Test     R^2  Q(10)  19.59424  0.03333266  \n Ljung-Box Test     R^2  Q(15)  41.71508  0.0002485932\n Ljung-Box Test     R^2  Q(20)  57.01482  2.04201e-05 \n LM Arch Test       R    TR^2   26.43916  0.009297785 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.605018 -3.589419 -3.605036 -3.599172 \n\n\n\n\nCode\narch.fit3 &lt;- garchFit(~garch(3,0), data = returns, trace = F)\nsummary(arch.fit3) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(3, 0), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(3, 0)\n&lt;environment: 0x7fafacad7c50&gt;\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1     alpha2     alpha3  \n0.0018883  0.0010802  0.0739590  0.2016414  0.0918580  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.888e-03   1.035e-03    1.825  0.06800 .  \nomega  1.080e-03   6.887e-05   15.684  &lt; 2e-16 ***\nalpha1 7.396e-02   2.466e-02    2.999  0.00271 ** \nalpha2 2.016e-01   3.903e-02    5.166 2.39e-07 ***\nalpha3 9.186e-02   2.968e-02    3.095  0.00197 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2414.967    normalized:  1.813039 \n\nDescription:\n Thu Apr 20 06:04:28 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  351.7103  0         \n Shapiro-Wilk Test  R    W      0.9681553 0         \n Ljung-Box Test     R    Q(10)  11.07031  0.3520662 \n Ljung-Box Test     R    Q(15)  16.47653  0.3511073 \n Ljung-Box Test     R    Q(20)  27.24647  0.1284711 \n Ljung-Box Test     R^2  Q(10)  7.550746  0.6726269 \n Ljung-Box Test     R^2  Q(15)  20.44479  0.1555259 \n Ljung-Box Test     R^2  Q(20)  31.73266  0.04623165\n LM Arch Test       R    TR^2   15.28409  0.2262666 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.618570 -3.599071 -3.618598 -3.611263 \n\n\n\n\nCode\narch.fit4 &lt;- garchFit(~garch(4,0), data = returns, trace = F)\nsummary(arch.fit4) #minimum AIC\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(4, 0), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(4, 0)\n&lt;environment: 0x7fafaee5b570&gt;\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1     alpha2     alpha3     alpha4  \n0.0019965  0.0010156  0.0726699  0.1935497  0.0647360  0.0720859  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.996e-03   1.029e-03    1.940  0.05242 .  \nomega  1.016e-03   7.037e-05   14.432  &lt; 2e-16 ***\nalpha1 7.267e-02   2.450e-02    2.966  0.00302 ** \nalpha2 1.935e-01   3.985e-02    4.857 1.19e-06 ***\nalpha3 6.474e-02   2.893e-02    2.237  0.02526 *  \nalpha4 7.209e-02   3.117e-02    2.313  0.02072 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2419.937    normalized:  1.81677 \n\nDescription:\n Thu Apr 20 06:04:28 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  365.6668  0         \n Shapiro-Wilk Test  R    W      0.9685425 0         \n Ljung-Box Test     R    Q(10)  12.4605   0.2554182 \n Ljung-Box Test     R    Q(15)  16.99111  0.3193961 \n Ljung-Box Test     R    Q(20)  28.06252  0.1079277 \n Ljung-Box Test     R^2  Q(10)  5.459804  0.8584262 \n Ljung-Box Test     R^2  Q(15)  17.20864  0.3065458 \n Ljung-Box Test     R^2  Q(20)  30.12726  0.06781762\n LM Arch Test       R    TR^2   13.29679  0.3478432 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.624531 -3.601132 -3.624571 -3.615762 \n\n\nOR\n\n\nCode\n#or\n\nARCH &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:4) {\nARCH[[cc]] &lt;- garch(returns,order=c(0,p),trace=F)\ncc &lt;- cc + 1\n} \n\n## get AIC values for model evaluation\nARCH_AIC &lt;- sapply(ARCH, AIC) ## model with lowest AIC is the best\nmin(ARCH_AIC)\n\n\n[1] -4810.263\n\n\n\n\nCode\nwhich(ARCH_AIC == min(ARCH_AIC))\n\n\n[1] 4\n\n\n\n\nCode\nARCH[[which(ARCH_AIC == min(ARCH_AIC))]]\n\n\n\nCall:\ngarch(x = returns, order = c(0, p), trace = F)\n\nCoefficient(s):\n      a0        a1        a2        a3        a4  \n0.001022  0.075614  0.190234  0.067205  0.068164  \n\n\nLowest AIC is for ARCH(4)\n\n\nCode\narch.fit4 &lt;- garchFit(~garch(4,0), data = returns, trace = F)\nsummary(arch.fit4) #minimum AIC\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(4, 0), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(4, 0)\n&lt;environment: 0x7fafb8f555f0&gt;\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1     alpha2     alpha3     alpha4  \n0.0019965  0.0010156  0.0726699  0.1935497  0.0647360  0.0720859  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.996e-03   1.029e-03    1.940  0.05242 .  \nomega  1.016e-03   7.037e-05   14.432  &lt; 2e-16 ***\nalpha1 7.267e-02   2.450e-02    2.966  0.00302 ** \nalpha2 1.935e-01   3.985e-02    4.857 1.19e-06 ***\nalpha3 6.474e-02   2.893e-02    2.237  0.02526 *  \nalpha4 7.209e-02   3.117e-02    2.313  0.02072 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2419.937    normalized:  1.81677 \n\nDescription:\n Thu Apr 20 06:04:29 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  365.6668  0         \n Shapiro-Wilk Test  R    W      0.9685425 0         \n Ljung-Box Test     R    Q(10)  12.4605   0.2554182 \n Ljung-Box Test     R    Q(15)  16.99111  0.3193961 \n Ljung-Box Test     R    Q(20)  28.06252  0.1079277 \n Ljung-Box Test     R^2  Q(10)  5.459804  0.8584262 \n Ljung-Box Test     R^2  Q(15)  17.20864  0.3065458 \n Ljung-Box Test     R^2  Q(20)  30.12726  0.06781762\n LM Arch Test       R    TR^2   13.29679  0.3478432 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.624531 -3.601132 -3.624571 -3.615762 \n\n\n\n\nForecast\n\n\nCode\npredict(arch.fit4, n.ahead=100, plot=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeanForecast\nmeanError\nstandardDeviation\nlowerInterval\nupperInterval\n\n\n\n\n0.0019965\n0.0333851\n0.0333851\n-0.0634371\n0.0674301\n\n\n0.0019965\n0.0348916\n0.0348916\n-0.0663898\n0.0703828\n\n\n0.0019965\n0.0370448\n0.0370448\n-0.0706100\n0.0746029\n\n\n0.0019965\n0.0381996\n0.0381996\n-0.0728734\n0.0768664\n\n\n0.0019965\n0.0393241\n0.0393241\n-0.0750773\n0.0790703\n\n\n0.0019965\n0.0398369\n0.0398369\n-0.0760824\n0.0800754\n\n\n0.0019965\n0.0402938\n0.0402938\n-0.0769780\n0.0809710\n\n\n0.0019965\n0.0405711\n0.0405711\n-0.0775215\n0.0815145\n\n\n0.0019965\n0.0407878\n0.0407878\n-0.0779462\n0.0819391\n\n\n0.0019965\n0.0409214\n0.0409214\n-0.0782081\n0.0822010\n\n\n0.0019965\n0.0410227\n0.0410227\n-0.0784065\n0.0823995\n\n\n0.0019965\n0.0410893\n0.0410893\n-0.0785371\n0.0825301\n\n\n0.0019965\n0.0411378\n0.0411378\n-0.0786320\n0.0826250\n\n\n0.0019965\n0.0411702\n0.0411702\n-0.0786957\n0.0826887\n\n\n0.0019965\n0.0411935\n0.0411935\n-0.0787413\n0.0827343\n\n\n0.0019965\n0.0412094\n0.0412094\n-0.0787724\n0.0827654\n\n\n0.0019965\n0.0412206\n0.0412206\n-0.0787945\n0.0827874\n\n\n0.0019965\n0.0412284\n0.0412284\n-0.0788096\n0.0828026\n\n\n0.0019965\n0.0412338\n0.0412338\n-0.0788203\n0.0828132\n\n\n0.0019965\n0.0412376\n0.0412376\n-0.0788276\n0.0828206\n\n\n0.0019965\n0.0412402\n0.0412402\n-0.0788328\n0.0828258\n\n\n0.0019965\n0.0412420\n0.0412420\n-0.0788364\n0.0828294\n\n\n0.0019965\n0.0412433\n0.0412433\n-0.0788389\n0.0828319\n\n\n0.0019965\n0.0412442\n0.0412442\n-0.0788406\n0.0828336\n\n\n0.0019965\n0.0412448\n0.0412448\n-0.0788419\n0.0828348\n\n\n0.0019965\n0.0412452\n0.0412452\n-0.0788427\n0.0828357\n\n\n0.0019965\n0.0412455\n0.0412455\n-0.0788433\n0.0828363\n\n\n0.0019965\n0.0412458\n0.0412458\n-0.0788437\n0.0828367\n\n\n0.0019965\n0.0412459\n0.0412459\n-0.0788440\n0.0828370\n\n\n0.0019965\n0.0412460\n0.0412460\n-0.0788442\n0.0828372\n\n\n0.0019965\n0.0412461\n0.0412461\n-0.0788443\n0.0828373\n\n\n0.0019965\n0.0412461\n0.0412461\n-0.0788444\n0.0828374\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788445\n0.0828375\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788445\n0.0828375\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376\n\n\n0.0019965\n0.0412462\n0.0412462\n-0.0788446\n0.0828376"
  },
  {
    "objectID": "fin_ts_models.html#arimaarcharmaarch-using-logged-transformed-values-or-using-differenced-log-values",
    "href": "fin_ts_models.html#arimaarcharmaarch-using-logged-transformed-values-or-using-differenced-log-values",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ARIMA+ARCH/ARMA+ARCH using logged transformed values or using differenced log values",
    "text": "ARIMA+ARCH/ARMA+ARCH using logged transformed values or using differenced log values\n(You can follow the analysis as we did in the Lab for “APPL”)\n\n\nCode\ndiff.TSLA=diff(TSLA_ALL$`TSLA.Adjusted`)\n\n#Plot differences of original series\nplot(diff.TSLA,type='l',main='Difference TSLA')\n\n\n\n\n\n\nTake log of original series and plot the log price\n\n\nCode\nlog.TSLA=log(TSLA_ALL$`TSLA.Adjusted`)\nplot(log.TSLA,type='l',main='Log TSLA')\n\n\n\n\n\n\n\nDifferencing log price and plot\n\n\nCode\ndifflog.TSLA=diff(log.TSLA)\nplot(difflog.TSLA,type='l',main='Difference Log Apple')\n\n\n\n\n\nDifferences of Tesla stock prices: It can be seen that the series is price-dependent; in other words, the variance of the series increases as the level of original series increases, and therefore, is not stationary.\nlog price of Tesla: The series is less variable compared to the original one.\nDifferences of log price of Tesla: The series seems more mean- reverting, and variance is cloe to being constant and does not significantly change as level of original series changes.\nAnother point that makes the differences of time series is more of interest than price series is that people often look at the returns of the stock rather than the its prices. Differences of log prices represent the returns and are similar to percentage changes of stock prices.\n\n\nACF and PACF of Log Tesla\n\n\nCode\nacf.TSLA = acf(log.TSLA,main='ACF TSLA',lag.max=100)\n\n\n\n\n\n\n\nCode\npacf.TSLA=pacf(log.TSLA,main='PACF TSLA',lag.max=100,ylim=c(-0.5,1))\n\n\n\n\n\nACF of Log Tesla stock price, showing the ACF slowly decreases (not dies down). It is probably that the model needs difference.\nPACF of Log Tesla, indicating significant value at lag 0 and then PACF cuts off. Therefore, The model for Log Apple stock price might be ARIMA; p=0,q=0.\n\n\nACF and PACF of Differenced log Tesla\n\n\nCode\nacf.TSLA=acf(difflog.TSLA,main='ACF Difference Log Tesla',na.action=na.pass)\n\n\n\n\n\n\n\nCode\npacf.TSLA=pacf(difflog.TSLA,main='PACF Difference Log Tesla',na.action=na.pass)\n\n\n\n\n\nACF of differences of log Apple with no significant lags (do not take into account lag 0).\nPACF of differences of log Tesla, reflecting no significant lags. The model for “differenced log Tesla” series is thus a white noise, and the “original model” resembles random walk model ARIMA(0,1,0).\n\n\nCheck for different combinations\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*33),nrow=33) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=1,2,\n{\n  for(q in 1:4)# q=1,2,\n  {\n    for(d in 0:1)# \n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(log.TSLA,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n2212.3459\n2227.9314\n2212.3639\n\n\n0\n1\n0\n-4711.6345\n-4701.2456\n-4711.6254\n\n\n0\n0\n1\n504.2939\n525.0746\n504.3240\n\n\n0\n1\n1\n-4710.5684\n-4694.9850\n-4710.5503\n\n\n0\n0\n2\n-776.1761\n-750.2001\n-776.1309\n\n\n0\n1\n2\n-4710.8081\n-4690.0304\n-4710.7780\n\n\n0\n0\n3\n-1676.5742\n-1645.4031\n-1676.5108\n\n\n0\n1\n3\n-4710.9230\n-4684.9508\n-4710.8777\n\n\n1\n0\n0\n-4707.8894\n-4687.1086\n-4707.8592\n\n\n1\n1\n0\n-4710.6427\n-4695.0594\n-4710.6246\n\n\n1\n0\n1\n-4706.6878\n-4680.7119\n-4706.6426\n\n\n1\n1\n1\n-4709.2362\n-4688.4585\n-4709.2061\n\n\n1\n0\n2\n-4707.1667\n-4675.9955\n-4707.1033\n\n\n1\n1\n2\n-4710.0827\n-4684.1105\n-4710.0375\n\n\n1\n0\n3\n-4707.3505\n-4670.9842\n-4707.2660\n\n\n1\n1\n3\n-4708.9341\n-4677.7675\n-4708.8707\n\n\n2\n0\n0\n-4706.7472\n-4680.7712\n-4706.7020\n\n\n2\n1\n0\n-4710.6180\n-4689.8402\n-4710.5878\n\n\n2\n0\n1\n-4705.3572\n-4674.1860\n-4705.2938\n\n\n2\n1\n1\n-4709.8991\n-4683.9269\n-4709.8539\n\n\n2\n0\n2\n-4706.5444\n-4670.1781\n-4706.4599\n\n\n2\n1\n2\n-4715.2396\n-4684.0730\n-4715.1762\n\n\n2\n0\n3\n-4704.2735\n-4662.7120\n-4704.1648\n\n\n2\n1\n3\n-4713.2905\n-4676.9295\n-4713.2060\n\n\n3\n0\n0\n-4706.8538\n-4675.6827\n-4706.7905\n\n\n3\n1\n0\n-4710.5654\n-4684.5932\n-4710.5201\n\n\n3\n0\n1\n-4706.3427\n-4669.9764\n-4706.2582\n\n\n3\n1\n1\n-4708.6158\n-4677.4491\n-4708.5524\n\n\n3\n0\n2\n-4711.5452\n-4669.9837\n-4711.4365\n\n\n3\n1\n2\n-4713.2933\n-4676.9323\n-4713.2087\n\n\n3\n0\n3\n-4703.3410\n-4656.5843\n-4703.2050\n\n\n3\n1\n3\n-4711.3921\n-4669.8366\n-4711.2832\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),] #2,1,2\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n22\n2\n1\n2\n-4715.24\n-4684.073\n-4715.176\n\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$BIC),] #0,1,0\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n0\n1\n0\n-4711.634\n-4701.246\n-4711.625\n\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$AICc),] #2,1,2\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n22\n2\n1\n2\n-4715.24\n-4684.073\n-4715.176\n\n\n\n\n\n\n\n\nCode\n#Diagnostics\nsarima(log.TSLA,2,1,2) #good fit\n\n\ninitial  value -3.188373 \niter   2 value -3.188458\niter   3 value -3.189536\niter   4 value -3.189538\niter   5 value -3.189541\niter   6 value -3.189547\niter   7 value -3.189565\niter   8 value -3.189609\niter   9 value -3.189724\niter  10 value -3.189909\niter  11 value -3.190242\niter  12 value -3.190567\niter  13 value -3.190707\niter  14 value -3.190739\niter  15 value -3.190757\niter  16 value -3.190809\niter  17 value -3.190879\niter  18 value -3.190911\niter  19 value -3.190931\niter  20 value -3.190961\niter  21 value -3.191042\niter  22 value -3.191265\niter  23 value -3.191364\niter  24 value -3.191491\niter  25 value -3.191505\niter  26 value -3.191546\niter  27 value -3.191634\niter  28 value -3.191708\niter  29 value -3.191746\niter  30 value -3.191765\niter  31 value -3.191798\niter  32 value -3.191868\niter  33 value -3.191973\niter  34 value -3.192136\niter  35 value -3.192240\niter  36 value -3.192260\niter  37 value -3.192284\niter  38 value -3.192444\niter  39 value -3.192592\niter  40 value -3.192727\niter  41 value -3.192803\niter  42 value -3.192906\niter  43 value -3.193008\niter  44 value -3.193074\niter  45 value -3.193128\niter  46 value -3.193131\niter  47 value -3.193137\niter  48 value -3.193142\niter  49 value -3.193152\niter  50 value -3.193154\niter  51 value -3.193155\niter  52 value -3.193155\niter  53 value -3.193156\niter  54 value -3.193156\niter  54 value -3.193156\niter  54 value -3.193156\nfinal  value -3.193156 \nconverged\ninitial  value -3.193423 \niter   2 value -3.193424\niter   3 value -3.193425\niter   4 value -3.193426\niter   5 value -3.193427\niter   6 value -3.193428\niter   7 value -3.193428\niter   7 value -3.193428\niter   7 value -3.193428\nfinal  value -3.193428 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1     ma2  constant\n      0.6330  -0.8692  -0.6577  0.9144    0.0016\ns.e.  0.0567   0.0676   0.0472  0.0544    0.0011\n\nsigma^2 estimated as 0.001683:  log likelihood = 2363.62,  aic = -4715.24\n\n$degrees_of_freedom\n[1] 1327\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        0.6330 0.0567  11.1590  0.0000\nar2       -0.8692 0.0676 -12.8670  0.0000\nma1       -0.6577 0.0472 -13.9302  0.0000\nma2        0.9144 0.0544  16.8230  0.0000\nconstant   0.0016 0.0011   1.4135  0.1577\n\n$AIC\n[1] -3.53997\n\n$AICc\n[1] -3.539936\n\n$BIC\n[1] -3.516571\n\n\n\n\nCode\nsarima(log.TSLA,0,1,0)\n\n\ninitial  value -3.189072 \niter   1 value -3.189072\nfinal  value -3.189072 \nconverged\ninitial  value -3.189072 \niter   1 value -3.189072\nfinal  value -3.189072 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0016\ns.e.    0.0011\n\nsigma^2 estimated as 0.001698:  log likelihood = 2357.82,  aic = -4711.63\n\n$degrees_of_freedom\n[1] 1331\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0016 0.0011  1.4186  0.1562\n\n$AIC\n[1] -3.537263\n\n$AICc\n[1] -3.537261\n\n$BIC\n[1] -3.529464\n\n\n\n\nCode\nTSLA.autoarima.fit=auto.arima(log.TSLA)\nsummary(TSLA.autoarima.fit) #also 2,1,2\n\n\nSeries: log.TSLA \nARIMA(2,1,2) with drift \n\nCoefficients:\n         ar1      ar2      ma1     ma2   drift\n      0.6330  -0.8692  -0.6577  0.9144  0.0016\ns.e.  0.0567   0.0676   0.0472  0.0544  0.0011\n\nsigma^2 = 0.00169:  log likelihood = 2363.62\nAIC=-4715.24   AICc=-4715.18   BIC=-4684.07\n\nTraining set error measures:\n                        ME       RMSE        MAE          MPE     MAPE\nTraining set -1.157689e-05 0.04101409 0.02892536 -0.006019341 0.708551\n                  MASE         ACF1\nTraining set 0.9968526 -0.005208895\n\n\n\n\nCode\narima1=arima(log.TSLA,order=c(2,1,2))\narima1\n\n\n\nCall:\narima(x = log.TSLA, order = c(2, 1, 2))\n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      0.6343  -0.8692  -0.6587  0.9146\ns.e.  0.0567   0.0673   0.0471  0.0541\n\nsigma^2 estimated as 0.001686:  log likelihood = 2362.64,  aic = -4715.28\n\n\n\n\nPlot the Squared residuals of the model, ACF, and PACF of the squared residual\n\n\nCode\narima1=Arima(log.TSLA,order=c(2,1,2))\nres.arima1=arima1$res\nsquared.res.arima1=res.arima1^2\n\n\n\nplot(squared.res.arima1,main='Squared Residuals')\n\n\n\n\n\n\n\nCode\nacf.squared1=acf(squared.res.arima1,main='ACF Squared Residuals')\n\n\n\n\n\n\n\nCode\npacf.squared1=pacf(squared.res.arima1,main='PACF Squared Residuals')\n\n\n\n\n\nSquared residuals plot shows cluster of volatility. ACF seems to die down.But significant spikes at lag 1 to 5.\nPACF cuts off after lag 4(significant spike at lag 2).\nThe residuals therefore show some patterns that might be modeled.\nARCH/GARCH is necessary to model the volatility of the series. As indicated by its name, this method concerns with the conditional variance of the series.\nTherefore, I will try ARCH(1)- ARCH(5) (which is GARCH(0,5)) and compare it’s AIC values.\n\n\nFitting different ARCH models\n\n\nCode\nARCH &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:5) {\nARCH[[cc]] &lt;- garch(res.arima1,order=c(0,p),trace=F)\ncc &lt;- cc + 1\n} \n\n## get AIC values for model evaluation\nARCH_AIC &lt;- sapply(ARCH, AIC) ## model with lowest AIC is the best\nmin(ARCH_AIC)\n\n\n[1] -4818.082\n\n\n\n\nCode\nwhich(ARCH_AIC == min(ARCH_AIC))\n\n\n[1] 4\n\n\n\n\nCode\nARCH[[which(ARCH_AIC == min(ARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res.arima1, order = c(0, p), trace = F)\n\nCoefficient(s):\n      a0        a1        a2        a3        a4  \n0.001028  0.074585  0.179825  0.069115  0.068518  \n\n\nThis shows that ARCH(4) is much better.\n\n\nCode\narch04=garch(res.arima1,order=c(0,4),trace=F)\nsummary(arch04)\n\n\n\nCall:\ngarch(x = res.arima1, order = c(0, 4), trace = F)\n\nModel:\nGARCH(0,4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3717 -0.4827  0.0304  0.5554  4.9841 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 1.028e-03   4.302e-05   23.907  &lt; 2e-16 ***\na1 7.459e-02   2.071e-02    3.601 0.000317 ***\na2 1.798e-01   2.629e-02    6.839 7.95e-12 ***\na3 6.911e-02   2.469e-02    2.799 0.005130 ** \na4 6.852e-02   2.611e-02    2.624 0.008681 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 360.62, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.034863, df = 1, p-value = 0.8519\n\n\n\n\nCode\nAIC(arch04)\n\n\n[1] -4818.082\n\n\nAIC of ARCH(4) is much lower. Therefore, I choose ARCH(4).\nIn addition, p-value of Box-Ljung test is greater than 0.05, and so The model adequately represents the residuals.\n\n\nForecast\n\n\nCode\nfit = garchFit(~ garch(4, 0), data = res.arima1, trace = FALSE)\npredict(fit, n.ahead = 100, plot=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeanForecast\nmeanError\nstandardDeviation\nlowerInterval\nupperInterval\n\n\n\n\n0.0020279\n0.0334364\n0.0334364\n-0.0635062\n0.0675620\n\n\n0.0020279\n0.0350331\n0.0350331\n-0.0666356\n0.0706914\n\n\n0.0020279\n0.0369907\n0.0369907\n-0.0704725\n0.0745284\n\n\n0.0020279\n0.0381702\n0.0381702\n-0.0727843\n0.0768402\n\n\n0.0020279\n0.0391988\n0.0391988\n-0.0748004\n0.0788562\n\n\n0.0020279\n0.0396956\n0.0396956\n-0.0757741\n0.0798299\n\n\n0.0020279\n0.0401151\n0.0401151\n-0.0765962\n0.0806520\n\n\n0.0020279\n0.0403796\n0.0403796\n-0.0771146\n0.0811704\n\n\n0.0020279\n0.0405775\n0.0405775\n-0.0775026\n0.0815584\n\n\n0.0020279\n0.0407019\n0.0407019\n-0.0777463\n0.0818022\n\n\n0.0020279\n0.0407939\n0.0407939\n-0.0779267\n0.0819825\n\n\n0.0020279\n0.0408551\n0.0408551\n-0.0780466\n0.0821025\n\n\n0.0020279\n0.0408987\n0.0408987\n-0.0781320\n0.0821879\n\n\n0.0020279\n0.0409280\n0.0409280\n-0.0781895\n0.0822454\n\n\n0.0020279\n0.0409488\n0.0409488\n-0.0782302\n0.0822861\n\n\n0.0020279\n0.0409630\n0.0409630\n-0.0782580\n0.0823138\n\n\n0.0020279\n0.0409729\n0.0409729\n-0.0782774\n0.0823333\n\n\n0.0020279\n0.0409797\n0.0409797\n-0.0782907\n0.0823466\n\n\n0.0020279\n0.0409844\n0.0409844\n-0.0783000\n0.0823559\n\n\n0.0020279\n0.0409877\n0.0409877\n-0.0783064\n0.0823623\n\n\n0.0020279\n0.0409900\n0.0409900\n-0.0783109\n0.0823668\n\n\n0.0020279\n0.0409915\n0.0409915\n-0.0783140\n0.0823698\n\n\n0.0020279\n0.0409926\n0.0409926\n-0.0783161\n0.0823720\n\n\n0.0020279\n0.0409934\n0.0409934\n-0.0783176\n0.0823734\n\n\n0.0020279\n0.0409939\n0.0409939\n-0.0783186\n0.0823745\n\n\n0.0020279\n0.0409942\n0.0409942\n-0.0783193\n0.0823752\n\n\n0.0020279\n0.0409945\n0.0409945\n-0.0783198\n0.0823757\n\n\n0.0020279\n0.0409947\n0.0409947\n-0.0783201\n0.0823760\n\n\n0.0020279\n0.0409948\n0.0409948\n-0.0783204\n0.0823762\n\n\n0.0020279\n0.0409949\n0.0409949\n-0.0783205\n0.0823764\n\n\n0.0020279\n0.0409949\n0.0409949\n-0.0783207\n0.0823765\n\n\n0.0020279\n0.0409950\n0.0409950\n-0.0783207\n0.0823766\n\n\n0.0020279\n0.0409950\n0.0409950\n-0.0783208\n0.0823766\n\n\n0.0020279\n0.0409950\n0.0409950\n-0.0783208\n0.0823767\n\n\n0.0020279\n0.0409950\n0.0409950\n-0.0783209\n0.0823767\n\n\n0.0020279\n0.0409950\n0.0409950\n-0.0783209\n0.0823767\n\n\n0.0020279\n0.0409950\n0.0409950\n-0.0783209\n0.0823767\n\n\n0.0020279\n0.0409950\n0.0409950\n-0.0783209\n0.0823767\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n0.0020279\n0.0409951\n0.0409951\n-0.0783209\n0.0823768\n\n\n\n\n\n\n\n\nFull model of ARIMA(2,1,2) + ARCH(4)\n\n\nCode\narima1=Arima(log.TSLA,order=c(2,1,2))\nsummary(arima1)\n\n\nSeries: log.TSLA \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      0.6343  -0.8692  -0.6587  0.9146\ns.e.  0.0567   0.0673   0.0471  0.0541\n\nsigma^2 = 0.001691:  log likelihood = 2362.64\nAIC=-4715.28   AICc=-4715.24   BIC=-4689.31\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.001576005 0.04104422 0.02894389 0.03376781 0.7085964 0.9974914\n                     ACF1\nTraining set -0.005564882\n\n\n\n\nCode\narch04=garchFit(data=res.arima1,order=c(0,4),trace=F)\nsummary(arch04)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(data = res.arima1, trace = F, order = c(0, 4)) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n&lt;environment: 0x7fafbad5a780&gt;\n [data = res.arima1]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n1.4548e-03  9.3559e-05  7.4729e-02  8.6886e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.455e-03   1.021e-03    1.425 0.154247    \nomega  9.356e-05   2.769e-05    3.379 0.000727 ***\nalpha1 7.473e-02   1.522e-02    4.909 9.18e-07 ***\nbeta1  8.689e-01   2.792e-02   31.124  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2432.124    normalized:  1.824549 \n\nDescription:\n Thu Apr 20 06:04:33 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  478.096   0        \n Shapiro-Wilk Test  R    W      0.9673765 0        \n Ljung-Box Test     R    Q(10)  9.716473  0.4657104\n Ljung-Box Test     R    Q(15)  11.49959  0.7164429\n Ljung-Box Test     R    Q(20)  19.21523  0.5078777\n Ljung-Box Test     R^2  Q(10)  6.07118   0.809248 \n Ljung-Box Test     R^2  Q(15)  12.22416  0.6619889\n Ljung-Box Test     R^2  Q(20)  15.96616  0.7187214\n LM Arch Test       R    TR^2   10.2364   0.5952296\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-3.643096 -3.627507 -3.643114 -3.637254 \n\n\n\n\nCompare results using AIC,BIC values and model diagnostics\n\n\nCode\n#summary(arch.fit4)\n#summary(arch04)\n\n\n\n\nCode\nAIC.case1 = -3.621\nBIC.case1 = -3.597\n\n\n\n\nCode\nAIC.case2 = -3.638\nBIC.case2 = -3.623\n\n\nBoth AIC and BIC from case 2 is smaller than those from case 1, which means the second model ARIMA(2,1,2) + ARCH(4) is better than the first model ARCH(4)."
  },
  {
    "objectID": "fin_ts_models.html#stock-prices",
    "href": "fin_ts_models.html#stock-prices",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Stock Prices",
    "text": "Stock Prices"
  },
  {
    "objectID": "deep_learning.html",
    "href": "deep_learning.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "There is often a tradeoff between flexibility and interpretability when modeling data using deep learning techniques like neural networks. As models become more flexible, the interpretability of the model often drops. This leads to models becoming very confusing when trying to understand how it is making decisions (hence the interpretability piece). When modeling time-series data, specifically temperature data, interpretability is less critical. There is less concern for how the model works versus whether it gives accurate predictions. Thus, time series modeling using deep learning techniques such as different types of neural networks is a viable option.\nIn this analysis, the U.S. Monthly CO2 Emissions data is used as input into three different types of neural networks. Each network was trained, and accuracies are calculated. From this, forecasts were made for CO2 emissions in the near future. A recurrent, GRU, and Long Short Term Memory(LSTM) neural network were used in this analysis. These models are used more often for sequential data like text, and since time series are sequential, these are good choices to begin with."
  },
  {
    "objectID": "intro.html#global-warming-and-greenhouse-effects",
    "href": "intro.html#global-warming-and-greenhouse-effects",
    "title": "Introduction",
    "section": "",
    "text": "Here is an article from National Geographic on greenhouse effect:\nHuman activities contribute to global warming by increasing the greenhouse effect. The greenhouse effect happens when certain gases—known as greenhouse gases—collect in Earth’s atmosphere. These gases, which occur naturally in the atmosphere, include carbon dioxide, methane, nitrogen oxide, and fluorinated gases sometimes known as chlorofluorocarbons (CFCs).\n\nGreenhouse gases let the sun’s light shine onto Earth’s surface, but they trap the heat that reflects back up into the atmosphere. In this way, they act like the insulating glass walls of a greenhouse. The greenhouse effect keeps Earth’s climate comfortable. Without it, surface temperatures would be cooler by about 33 degrees Celsius (60 degrees Fahrenheit), and many life forms would freeze.\n\nSince the Industrial Revolution in the late 1700s and early 1800s, people have been releasing large quantities of greenhouse gases into the atmosphere. That amount has skyrocketed in the past century. Greenhouse gas emissions increased 70 percent between 1970 and 2004. Emissions of carbon dioxide, the most important greenhouse gas, rose by about 80 percent during that time. The amount of carbon dioxide in the atmosphere today far exceeds the natural range seen over the last 650,000 years.\n\n\n\nMost of the carbon dioxide that people put into the atmosphere comes from burning fossil fuels such as oil, coal, and natural gas. Cars, trucks, trains, and planes all burn fossil fuels. Many electric power plants also burn fossil fuels.\n\nAnother way people release carbon dioxide into the atmosphere is by cutting down forests. This happens for two reasons. Decaying plant material, including trees, releases tons of carbon dioxide into the atmosphere. Living trees absorb carbon dioxide. By diminishing the number of trees to absorb carbon dioxide, the gas remains in the atmosphere.\n\n\n\n\nMost methane in the atmosphere comes from livestock farming, landfills, and fossil fuel production suchas coal mining and natural gas processing. Nitrous oxide comes from agricultural technology and fossil fuel burning.\n\n\n\n\n\nFluorinated gases include chlorofluorocarbons, hydrochlorofluorocarbons, and hydrofluorocarbons. These greenhouse gases are used in aerosol cans and refrigeration.\n\nAll of these human activities add greenhouse gases to the atmosphere, trapping more heat than usual and contributing to global warming.\n\n\n\nThe climate portal at MIT explains that radiative forcing is what happens when the amount of energy that enters the Earth’s atmosphere is different from the amount of energy that leaves it. Energy travels in the form of radiation: solar radiation entering the atmosphere from the sun, and infrared radiation exiting as heat. If more radiation is entering Earth than leaving—as is happening today—then the atmosphere will warm up. This is called radiative forcing because the difference in energy can force changes in the Earth’s climate.\n\n\n\nPublished September 25, 2020. This Explainer was adapted from “Explained: Radiative Forcing” by David Chandler, which originally appeared in MIT News.\n\n\n\n                   \n\nFrom the graph above, we know that carbon dioxide has the most contribution to global heating imbalance. Methane has the second most contribution to global heating imbalance. Then they are chlorofluorocarbons, nitrous oxide, hydrochlorofluorocarbons, and hydrofluorocarbons."
  },
  {
    "objectID": "models.html#acf-plots",
    "href": "models.html#acf-plots",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Original and Log TransformationDetrend and Remove Seasonality\n\n\n\n\nCode\nggarrange(a, b, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nggarrange(c, d, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\nThe ACF plots show the differences among original data, log transformation, remove seasonality, and first difference with remove seasonality.\nFor white noise series and stationary series, we expect each autocorrelation to be close to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within ±2/√T where T is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise, and it’s not stationary either.\nThis ACF graph of detrended and differenced US total coal consumption data (the forth one from above) shows almost all the spikes are inside the blue bounds, which proves the detrended and differenced data is stationary and ready to be utilized in future analysis."
  },
  {
    "objectID": "models.html#stationarity-test-seasonality-test",
    "href": "models.html#stationarity-test-seasonality-test",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Code\nadf.test(`First Diff After Remove Seasonality`)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  First Diff After Remove Seasonality\nDickey-Fuller = -3.9404, Lag order = 4, p-value = 0.01703\nalternative hypothesis: stationary\n\n\nStationarity test is less than 0.05, which means first order differencing and removing seasonality would make the time series dataset stationary. The dataset is no longer needed to be differenced or detrended.\n\n\n\n\n\n\nCode\nisSeasonal(`First Diff After Remove Seasonality`, test = \"combined\", freq = NA)\n\n\n[1] FALSE\n\n\nThe isSeasonal function from seastests library shows our first-differenced and detrended data has no seasonality.\nNow we are confident that our differenced and detrended data is ready for the model fitting."
  },
  {
    "objectID": "models.html#model-fitting",
    "href": "models.html#model-fitting",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Code\nlog(coal_df_ts) |&gt; diff() |&gt;diff(lag=4) |&gt; ggtsdisplay() #this is better\n\n\n\n\n\nCode\n#ggplotly(d) plotting the stationary data's ACF graph\n\n\nThe ACF and PACF graphs are showing that q = 0,1,2,3,4, d = 1, p = 0,1,2,3,4, Q = 0,1,2, P = 0,1,2,3, and D = 1\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*100),nrow=100)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\n\n\nCode\noutput &lt;- SARIMA.c(p1 = 1, p2 = 5, q1 = 1, q2 = 5, P1 = 1, P2 = 4, Q1 = 1, Q2 = 3, data = coal_df_ts) |&gt;\n  drop_na()\n\nminaic &lt;- output[which.min(output$AIC), ]\nminbic &lt;- output[which.min(output$BIC), ]\n\n\n\nParameters with Minimum AICParameters with Minimum BICUsing auto.arima()\n\n\n\n\nCode\nknitr::kable(minaic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n8\n0\n1\n0\n2\n1\n1\n2807.125\n2816.448\n2807.689\n\n\n\n\n\n\n\n\n\nCode\nknitr::kable(minbic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n0\n1\n0\n0\n1\n1\n2810.769\n2815.431\n2810.934\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(coal_df_ts)\n\n\nSeries: coal_df_ts \nARIMA(0,1,0)(2,1,1)[4] \n\nCoefficients:\n       sar1     sar2     sma1\n      0.069  -0.3507  -0.7618\ns.e.  0.143   0.1285   0.1291\n\nsigma^2 = 5.598e+14:  log likelihood = -1399.56\nAIC=2807.13   AICc=2807.69   BIC=2816.45"
  },
  {
    "objectID": "models.html#model-diagnostics",
    "href": "models.html#model-diagnostics",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "From model fitting, we generated two models, ARIMA(0,1,0) x (2,1,1) and ARIMA(0,1,0) x (0,1,1). auto.arima() generated ARIMA(0,1,0) x (2,1,1) as well. ARIMA(0,1,0) x (2,1,1) has the lowest AIC while ARIMA(0,1,0) x (0,1,1) has the lowest BIC. Now let’s do two model diagnoses to analyze the result and find the better model to do forecast later.\n\n\n\n\nCode\nfit1 &lt;- Arima(coal_df_ts, order=c(0,1,0), seasonal = list(order = c(2,1,1), period = 4))\n\n\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(coal_df_ts,0,1,0,2,1,1,4))\n\n\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order = c(2,1,1), period = 4)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)(2,1,1)[4]\nQ* = 8.2062, df = 5, p-value = 0.1452\n\nModel df: 3.   Total lags used: 8\n\n\nThe Ljung-Box test uses the following hypotheses:\nH0: The residuals are independently distributed.\nHA: The residuals are not independently distributed; they exhibit serial correlation.\nIdeally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.\n\n\n\nThere isn’t any significant spikes in the ACF, and the model fails the Ljung-Box test. The model can still be used for forecasting.\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order=c(2,1,1), period=4))\n\n\nSeries: coal_df_ts \nARIMA(0,1,0)(2,1,1)[4] \n\nCoefficients:\n       sar1     sar2     sma1\n      0.069  -0.3507  -0.7618\ns.e.  0.143   0.1285   0.1291\n\nsigma^2 = 5.598e+14:  log likelihood = -1399.56\nAIC=2807.13   AICc=2807.69   BIC=2816.45\n\n\n\n\n\n\n\nCode\ncat(model_output[25:56], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n       sar1     sar2     sma1\n      0.069  -0.3507  -0.7618\ns.e.  0.143   0.1285   0.1291\n\nsigma^2 estimated as 5.377e+14:  log likelihood = -1399.56,  aic = 2807.13\n\n$degrees_of_freedom\n[1] 73\n\n$ttable\n     Estimate     SE t.value p.value\nsar1   0.0690 0.1430  0.4823  0.6310\nsar2  -0.3507 0.1285 -2.7300  0.0079\nsma1  -0.7618 0.1291 -5.9024  0.0000\n\n$AIC\n[1] 36.93586\n\n$AICc\n[1] 36.94024\n\n$BIC\n[1] 37.05853\n\n\n\n\n\n\n\n\nCode\nfit2 &lt;- Arima(coal_df_ts, order=c(0,1,0), seasonal = list(order = c(0,1,1), period = 4))\n\n\n\n\n\n\nCode\nset.seed(123)\nmodel_output2 &lt;- capture.output(sarima(coal_df_ts,0,1,0,0,1,1,4))\n\n\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order = c(0,1,1), period = 4)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)(0,1,1)[4]\nQ* = 16.558, df = 7, p-value = 0.02048\n\nModel df: 1.   Total lags used: 8\n\n\nThe Ljung-Box test uses the following hypotheses:\nH0: The residuals are independently distributed.\nHA: The residuals are not independently distributed; they exhibit serial correlation.\nIdeally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.\n\n\n\nThere are two significant spikes in the ACF, and the model fails the Ljung-Box test. The model can still be used for forecasting.\n\n\n\n\n\nCode\ncoal_df_ts |&gt;\n  Arima(order=c(0,1,0), seasonal = list(order = c(0,1,1), period = 4))\n\n\nSeries: coal_df_ts \nARIMA(0,1,0)(0,1,1)[4] \n\nCoefficients:\n         sma1\n      -0.8690\ns.e.   0.0877\n\nsigma^2 = 6.029e+14:  log likelihood = -1403.38\nAIC=2810.77   AICc=2810.93   BIC=2815.43\n\n\n\n\n\n\n\nCode\ncat(model_output2[20:49], model_output2[length(model_output2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         sma1\n      -0.8690\ns.e.   0.0877\n\nsigma^2 estimated as 5.949e+14:  log likelihood = -1403.38,  aic = 2810.77\n\n$degrees_of_freedom\n[1] 75\n\n$ttable\n     Estimate     SE t.value p.value\nsma1   -0.869 0.0877 -9.9034       0\n\n$AIC\n[1] 36.98381\n\n$AICc\n[1] 36.98452\n\n$BIC\n[1] 37.04514\n\n\n\n\n\n\n\nFrom model diagnostics above, ARIMA(0,1,0) x (2,1,1) is also the one auto.arima() produced, and it has less spikes in the ACF and PACF plots of its residuals. The model also has smaller sigma squared means it has smaller variance, which means the estimators with a smaller variance is more efficient.\nTherefore, ARIMA(0,1,0) x (2,1,1) is the better model."
  },
  {
    "objectID": "models.html#forecasting",
    "href": "models.html#forecasting",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Let’s forecast for the next three years using the model we just selected.\n\n\n\n\nCode\nc &lt;- fit1 |&gt; forecast(h=12) |&gt; autoplotly() + ggtitle(\"Quarterly Coal Consumption Three-Year-Forecasting\") + xlab(\"Year\") + ylab(\"Short Ton\")\nc |&gt;\n  layout(showlegend = F,\n         xaxis = list(rangeslider = list(visible = T)))\n\n\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(coal_df_ts, 12, 0,1,0,2,1,1,4)\n\n\n\n\n\n$pred\n          Qtr1      Qtr2      Qtr3      Qtr4\n2021           274828535 376893671 306317902\n2022 319770967 282898241 373853941 294614305\n2023 276483317 237626523 331810754 257541142\n2024 244525100                              \n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2021          23187703 32792364 40162280\n2022 46375406 55401734 63150819 70047839\n2023 76324135 81363930 86109260 90606402\n2024 94890651"
  },
  {
    "objectID": "models.html#compare-with-benchmark-method",
    "href": "models.html#compare-with-benchmark-method",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Code\nvalues &lt;- append(coal_df$Consumption, rep(NA,11))\ncoal_ts_df &lt;- data.frame(Year = seq(as.Date(\"2000/1/1\"), by = \"quarter\", length.out = 92), Short.tons=values)\n\ncoal_ts_df$meanf &lt;- append(rep(NA,80), meanf(coal_df_ts, h=12)$mean)\ncoal_ts_df$naive &lt;- append(rep(NA,80), naive(coal_df_ts, h=12)$mean)\ncoal_ts_df$snaive &lt;- append(rep(NA,80), snaive(coal_df_ts, h=12)$mean)\ncoal_ts_df$rwf &lt;- append(rep(NA,80), rwf(coal_df_ts, h=12, drift=TRUE)$mean)\ncoal_ts_df$fit &lt;- append(rep(NA,80), forecast(fit1, h=12)$mean)\n\np &lt;- ggplot(coal_ts_df) + \n  geom_line(aes(x=Year, y = Short.tons)) + \n  geom_line(aes(x=Year, y = meanf, color = \"Mean\")) +\n  geom_line(aes(x=Year, y = naive, color = \"Naïve\")) +\n  geom_line(aes(x=Year, y = snaive, color = \"SNaïve\")) +\n  geom_line(aes(x=Year, y = rwf, color = \"Drift\")) +\n  geom_line(aes(x=Year, y = fit, color = \"Model\")) +\n  ggtitle(\"Comparison of the Fitted Model and Benchmark Methods\") +\n  ylab(\"Short tons\")\n  \nggplotly(p)\n\n\n\n\n\n\nBenchmark method is for data scientists to keep their sanity when building models, they set a baseline — a score that the model must outperform. Normally, the state-of-the-art is used as the baseline but for problems with no existing solutions yet, one should build their own baseline.\n\n\nThis method simply takes the average (or “mean”) value of the entire historical data and use that to forecast future values. Very useful for data with small variance or whose value lies close to the mean.\n\n\n\nDrift is the amount of change observed from the data. In this method, drift is set to be the average change seen in the whole historical data and uses that to forecast values in the future. Basically, this just means drawing a straight line using the first and last values and extend that line into the future. This method works well on data that follows a general trend over time.\n\n\n\nThis method uses the most recent value as the forecasted value for the next time step. The assumption followed by this method is that its value tomorrow is equal to its value today.\n\n\n\nThe fitted model has outperformed the rest of the benchmark methods such us the mean method, naive methods, and drift method."
  },
  {
    "objectID": "eda.html#time-series-plot",
    "href": "eda.html#time-series-plot",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Firstly, it is important to visualize the time series at its most basic level, which is a simple plot of data over time.\n\n\nCode\ncoal_df &lt;- read.csv(\"/Users/raezh1/Documents/Georgetown/ANLY560/website/Time Series/data/coal_us_consumption.csv\") |&gt;\n  group_by(period) |&gt;\n  summarise(Consumption = sum(consumption)) |&gt;\n  filter(!period %in% c('2000-Q1', '2000-Q2',  '2000-Q3', '2000-Q4'))\n\ncoal_df_ts &lt;- ts(coal_df$Consumption, start = c(2001,1), end = c(2021,1), frequency = 4)\n\n\n\n\nCode\ntheme_set(theme_gray(base_size=12,base_family=\"Palatino\"))\nautoplotly(coal_df_ts) +\n  ggtitle(\"U.S. Quarterly Coal Consumption\") +\n  xlab(\"Year (2001-2021)\") +\n  ylab(\"Short Ton\")"
  },
  {
    "objectID": "eda.html#decomposition",
    "href": "eda.html#decomposition",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "decompose() Functionstl() Function\n\n\nMultiplicative trend means the trend is not linear (curved line), and multiplicative seasonality means there are changes to widths or heights of seasonal periods over time. From the time series graph created above, we should use multiplicative model from decompose() to decompose the coal consumption.\n\n\nCode\ndecompose_coal = decompose(coal_df_ts, \"multiplicative\")\nautoplotly(decompose_coal) +\n  ggtitle(\"Decomposition of U.S. Total Coal Consumption\")\n\n\n\n\n\n\n\n\nIn R the stl() function performs decomposition of a time series into seasonal, trend and irregular components using loses. stl() will handle any type of seasonality, not only monthly and quarterly data.\n\n\nCode\ncoal_df_ts |&gt;\n  stl(s.window=\"periodic\", robust=TRUE) |&gt;\n  autoplotly()\n\n\n\n\n\n\n\n\n\nThe two plots of decomposition are very similar except the remainder from stl() function is less smoothed and more noisy than the remainder from decompose() function. They both show U.S. the U.S. quarterly coal consumption data has seasonality. Before 2010, the trend of the data is increasing. After 2010, the trend of the data is decreasing.\nSince the data has a trend, it doesn’t has stationarity as a stationary time series is one whose properties do not depend on the time at which the series is observed. The ACF graph below will show more of its non-stationary feature."
  },
  {
    "objectID": "eda.html#lag-plots",
    "href": "eda.html#lag-plots",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "A lag plot is a type of scatter plot where time series are plotted in pairs against itself some time units behind or ahead. Lag plots often reveal more information on the seasonality of the data, whether there is randomness in the data or an indication of autocorrelation in the data. Below is a lag plot for the quarterly U.S. coal consumption data.\n\n\nCode\nlibrary(wesanderson)\ngglagplot(coal_df_ts, do.lines=FALSE, set.lags = c(4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64)) + \n  xlab(\"Lags\") + \n  ggtitle(\"Lag Plot of U.S. Total Coal Consumption\") +\n  theme_minimal() +\n  ylab(\"Coal Consumption in short tons\") +\n  theme_light() +\n  theme(text=element_text(size=12, family=\"Palatino\")) +\n  labs(fill = \"Legend\") +\n  scale_color_brewer(palette=\"Set2\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\n\nFrom lag plots here we can see that the coal consumption data has serial correlation as the lag plots are showing a linear pattern, which suggests autocorrelation is present. This is a positive linear trend, so the data has positive autocorrelation. However, it’s hard to spot the seasonality from the lag plots, which makes me believe the data doesn’t have seasonality."
  },
  {
    "objectID": "eda.html#autocorrelation-and-stationarity",
    "href": "eda.html#autocorrelation-and-stationarity",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The function ACF computes (and by default plots) an estimate of the autocorrelation function of a univariate time series. Function PACF computes (and by default plots) an estimate of the partial autocorrelation function of a univariate time series.\nAutocorrelation and partial autocorrelation plots are heavily used in time series analysis and forecasting.\n\nACF PlotPACF Plot\n\n\n\n\nCode\nacf &lt;- ggAcf(coal_df_ts, 80) + ggtitle(\"ACF Plot of U.S. Total Coal Consumption\")\nggplotly(acf)\n\n\n\n\n\n\nNote that the ACF shows an oscillation, indicative of a seasonal series. Note the peaks occur at lags of 4th quarter, 8th quarter, and 12th quarter, etc. It means the data has yearly seasonality.\nA stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time.\nAs well as looking at the time plot of the data, the ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Our ACF graph decreases slowly and we can clearly see the data is not stationary.\n\n\n\n\nCode\npacf &lt;- ggPacf(coal_df_ts, 50) + ggtitle(\"PACF Plot of U.S. Total Coal Consumption\")\nggplotly(pacf)\n\n\n\n\n\n\nFrom the PACF plot, we can see a large spike at lag 1 that decreases after a few lags, which indicates a moving average term in the data."
  },
  {
    "objectID": "eda.html#differencing-and-detrending",
    "href": "eda.html#differencing-and-detrending",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Stationarity Test - Augmented Dickey-Fuller TestStationarity Test - KPSS TestSeasonality Test - nsdiffs() Test\n\n\nThe Augmented Dickey-Fuller test is a type of statistical test called a unit root test.\nADF test is conducted with the following assumptions.\nNull Hypothesis (HO): Series is non-stationary or series has a unit root.\nAlternate Hypothesis(HA): Series is stationary or series has no unit root.\nIf the null hypothesis is failed to be rejected, this test may provide evidence that the series is non-stationary.\nConditions to Reject Null Hypothesis(HO)\n\n\nCode\nadf.test(coal_df_ts |&gt; log())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(coal_df_ts)\nDickey-Fuller = -2.9525, Lag order = 4, p-value = 0.1858\nalternative hypothesis: stationary\n\n\nThe p value of ADF test is greater than 0.05, so we cannot reject null hypothesis and it means the U.S. total coal consumption data is not stationary. The rest result matches with the conclusion we had after observing decomposition plot and ACF plot.\nLet’s take the first differencing and do the ADF test again.\n\n\nCode\nadf.test(coal_df_ts |&gt; log() |&gt; diff())\n\n\nWarning in adf.test(diff(log(coal_df_ts))): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(log(coal_df_ts))\nDickey-Fuller = -4.3984, Lag order = 4, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe can see that after first differencing, the p value is below 0.05 which means it’s stationary now and we don’t need the second differencing.\n\n\nIn this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required.\n\n\nCode\nlibrary(urca)\ncoal_df_ts |&gt; ur.kpss() |&gt; summary()\n\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 3 lags. \n\nValue of test-statistic is: 1.7562 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nThe test statistic is much bigger than the 5% critical value, indicating that the null hypothesis is rejected. That is, the data are not stationary. We can difference the data, and apply the test again.\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; diff(lag=4) |&gt; diff() |&gt; ur.kpss() |&gt; summary()\n\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 3 lags. \n\nValue of test-statistic is: 0.0871 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nThis time, the test statistic is tiny, and well within the range we would expect for stationary data, so we can conclude that the differenced data are stationary.\n\n\nA similar function for determining whether seasonal differencing is required is nsdiffs(), which uses the measure of seasonal strength introduced to determine the appropriate number of seasonal differences required. ndiffs() estimates the number of first differences. No seasonal differences are suggested if the result is less than 0.64, otherwise one seasonal difference is suggested.\nWe can apply nsdiffs() to the logged U.S. Total Coal Consumption data.\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; ndiffs()\n\n\n[1] 1\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; diff() |&gt; nsdiffs()\n\n\n[1] 1\n\n\nCode\ncoal_df_ts |&gt; log() |&gt; diff(lag=4) |&gt; nsdiffs()\n\n\n[1] 0\n\n\nBecause nsdiffs() returns 1 on the original data(indicating one seasonal difference is required), we apply the function again to the seasonally differenced data. These two functions suggest we should do both a seasonal difference and a first difference.\n\n\n\nIn conclusion, after trying several differences and seasonal differences combinations, taking one seasonal difference and one difference is what the data needs to be stationary.\n\n\n\nTransformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\n\n\nCode\ncbind(\"Original\" = coal_df_ts,\n      \"Log Trans\" = log(coal_df_ts),\n      \"Sea. Diff\" = diff(log(coal_df_ts),lag=4),\n      \"1st & Sea. Diff\" = diff(diff(log(coal_df_ts),lag=4))) |&gt;\n  autoplotly(facets=TRUE) +\n    xlab(\"Year\") + ylab(\"\") +\n    ggtitle(\"Differencing of U.S. Total Coal Consumption\")"
  },
  {
    "objectID": "eda.html#new-stationary-data",
    "href": "eda.html#new-stationary-data",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Finally, the seasonality and correlation should be removed to make the time series stationary. A comparison of all of the methods is seen below. Once the transformations are applied, the series is stationary. This can also be seen in the plot below.\n\n\nCode\n`Log Transformation` &lt;- log(coal_df_ts)\n`Remove Seasonality` &lt;- diff(log(coal_df_ts), lag=4)\n`First Diff After Remove Seasonality` &lt;- diff(diff(log(coal_df_ts), lag=4))\n\na &lt;- ggAcf(coal_df_ts,70) + ggtitle(\"Original Data\")\nb &lt;- ggAcf(`Log Transformation`,70) + ggtitle(\"Log Transformation\")\nc &lt;- ggAcf(`Remove Seasonality`,70) + ggtitle(\"Remove Seasonality\")\nd &lt;- ggAcf(`First Diff After Remove Seasonality`,70) + ggtitle(\"First Diff After Remove Seasonality\")\n\n\n\n\n\nOriginal and Log TransformationDetrend and Remove Seasonality\n\n\n\n\nCode\nggarrange(a, b, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nggarrange(c, d, ncol = 1, nrow = 2)\n\n\n\n\n\n\n\n\nThe first three graphs above are not as ideal as the last ACF graph which is First Diff After Remove Seasonality.\nFor white noise series and stationary series, we expect each autocorrelation to be close to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within ±2/√T where T is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise, and it’s not stationary either.\nThis ACF graph of detrended and differenced U.S. total coal consumption data shows almost all the spikes are inside the blue bounds, which proves the detrended and differenced data is stationary and ready to be utilized in future analysis."
  },
  {
    "objectID": "deep_learning.html#time-series-plot",
    "href": "deep_learning.html#time-series-plot",
    "title": "Deep Learning for TS",
    "section": "Time Series Plot",
    "text": "Time Series Plot\nFirstly, it is essential to remember what this time series data looks like. Here below is a plot of the CO2 Emissions data, it has the U.S. monthly carbon dioxide emissions from Jan 1973 to Dec 2022 in the unit of million metric tons.\n\n\nCode\n## Load libraries ##\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense, SimpleRNN, LSTM ,GRU\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\n\nimport plotly.io as pio\n\nimport plotly.express as px\n\nimport plotly.graph_objects as go\n\n## Read data CO2 emissions ##\n\ndf = pd.read_csv(\"data/df_total_monthly_CO2_emissions.csv\",\n                 header=1, names=[\"1\", \"Month\", \"Emissions\", \"Unit\"], usecols=[\"Month\", 'Emissions'])\n\nX = np.array(df[\"Emissions\"].values.astype('float32')).reshape(df.shape[0],1)\n\nprint(X.shape)\n## Visualize ##\n\npio.renderers.default = \"notebook_connected\"\n\ndef plotly_line_plot(t, y, x_label=\"t: time (months)\", y_label=\"y(t): Million Metric Tons of Carbon Dioxide\",\n                     names=[\"\",\"\"], title=\"\"):\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=t[0], y=y[0],name=names[0]))\n    for i in range(1, len(y)):\n        if len(t[i]) == 1:\n            # print(t[i],y[i])\n            fig.add_trace(go.Scatter(x=t[i], y=y[i], name=names[i]))\n        else:\n            fig.add_trace(go.Scatter(x=t[i], y=y[i], name=names[i]))\n    fig.update_layout(\n        font=dict(\n            family=\"Palatino\",\n            size=14),\n        title=title,\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"seaborn\",\n        showlegend=True\n\n    )\n    fig.show()\n# Original data\n\nt = [*range(0, len(X))]\n\nplotly_line_plot([t], [X[:, 0]], title=\"CO2 Emissions per month since 1973-01\", names=[\"Original\"])\n\n\n(600, 1)"
  },
  {
    "objectID": "deep_learning.html#deep-learning-models",
    "href": "deep_learning.html#deep-learning-models",
    "title": "Deep Learning for TS",
    "section": "Deep Learning Models",
    "text": "Deep Learning Models\nNext, the time series data is split into a training and testing set. Since this is time-series data, the training and testing sets must be split in such a way that time is accounted for. Here, a 90,10 split for training and testing sets is used. Each of the three models is constructed and trained using the training data. Below are the results for these models.\n\nTrain and Test Plot\n\n\nCode\n## Test-train split ##\n# Parameter split_percent defines the ratio of training examples\n\ndef get_train_test(data, split_percent=0.9):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n    # Point for splitting data into train and test\n\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\ntrain_data, test_data, data = get_train_test(X)\n#print(train_data.shape)\n#print(test_data.shape)\n## Visualize: training-test split ##\n# SINGLE SERIES\n\nt1=[*range(0,len(train_data))]\n\nt2=len(train_data)+np.array([*range(0, len(test_data))])\n\nplotly_line_plot([t1, t2], [train_data, test_data], title=\"Train-test Split: CO2 Emissions per month since 1973-01\", names=[\"Train Data\", \"Test Data\"])\n\n\n\n                                                \n\n\n\n\nCO2 Emissions Partition Plot\n\n\nCode\n## Re-format data into required shape ##\n# PREPARE THE INPUT X AND TARGET Y\n\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind,X,Y_ind,Y #use for plotting later\n\n    # INDICES OF TARGET ARRAY\n\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps); #print(Y_ind); exit()\n    Y = dat[Y_ind]\n    # PREPARE X\n\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    del X_ind[::time_steps] # if time_steps=10 remove every 10th entry\n\n    X = dat[X_ind];\n    # PLOT\n\n    if(plot_data_partition):\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show();\n    # RESHAPE INTO KERAS FORMAT\n\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n    return X1, Y\n\n# PARTITION DATA\n\np=3  # simpilar to AR(p) given time_steps data points, predict time_steps+1 point (make prediction one month in future)\n\ntestX, testY = get_XY(test_data, p)\n\ntrainX, trainY = get_XY(train_data, p)\n#print(testX.shape,testY.shape)\n#print(trainX.shape,trainY.shape)\n#print(type(trainX))\n## Visualize ##\n## Build list\n\ntmp1=[]; tmp2=[]; tmp3=[]; count=0\n\nfor i in range(0,trainX.shape[0]):\n    # tmp1.append()\n    tmp1.append(count+np.array([*range(0,trainX[i,:,0].shape[0])]))\n    tmp1.append([count+trainX[i,:,0].shape[0]]); #print(([count+trainX[i,:,0].shape[0]]))\n    # tmp1.append([count+trainX[i,:,0].shape[0]+1])\n    tmp2.append(trainX[i,:,0])\n    tmp2.append([trainY[i]]); #print([trainY[i]])\n    # tmp2.append([trainY[i]])\n    count+=trainX[i,:,0].shape[0]+1\n\n    # print(i,trainX[i,:,0].shape)\n# print(tmp1)\n# print(tmp2)\n\ndef plotly_line_plot2(t, y, x_label=\"t: time (months)\", y_label=\"y(t): Million Metric Tons of Carbon Dioxide\", title=\"CO2 Emissions Partition Plot\"):\n    # GENERATE PLOTLY FIGURE\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=t[0], y=y[0]))\n    for i in range(1, len(y)):\n        if len(t[i]) == 1:\n            # print(t[i],y[i])\n            fig.add_trace(go.Scatter(x=t[i], y=y[i]))\n        else:\n            fig.add_trace(go.Scatter(x=t[i], y=y[i],))\n    fig.update_layout(\n        font=dict(\n            family=\"Palatino\",\n            size=14\n\n        ),\n        title=title,\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"seaborn\",\n        showlegend=True\n\n    )\n    fig.show()\n\nplotly_line_plot2(tmp1, tmp2, title=\"Train-test Split with Timestamp: CO2 Emissions per month since 1973-01\")\n\n\n\n                                                \n\n\n\n\nRecurrent Neural Networks\nA simple recurrent neural network is constructed and trained using TensorFlow and Keras. This is a deep neural network with three hidden layers, one dense layer, and the activation function is a hyperbolic tangent. Next, a second recurrent neural network is constructed using the exact same parameters, except the second model utilizes kernel regularization, which attempts to compensate for the potential of overfitting. If the regularization worked, the training error for the non-regularized network might be lower, but for the regularized network, the testing error should be lower. Below are the validation loss plots for each neural network, one regularized and one not.\n\nSimple RNN with L1 L2 regularization Results\n\n\nCode\n## Model and training parameters ##\n#USER PARAM\n\nrecurrent_hidden_units=3\n\nepochs=200\n\nf_batch=0.1    #fraction used for batch size\n\noptimizer=\"RMSprop\"\n\nvalidation_split=0.3\n\nprint(trainX.shape,p,trainY.shape)\n# trainY=trainY.reshape(trainY.shape[0],1)\n# testY=testY.reshape(testY.shape[0],1)\n\nprint(p,trainX.shape,testX.shape,trainY.shape,testY.shape)\n## Create Model ##\n\nfrom tensorflow.keras import regularizers\n\n# CREATE MODEL\nmodel = Sequential()\n# COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(SimpleRNN(\n#model.add(GRU(\n    recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=(trainX.shape[1], trainX.shape[2]),\n    # recurrent_dropout=0.8,\n    recurrent_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n    activation='tanh')\n)\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n# COMPILE THE MODEL\nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n## Train Model ##\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY,\nepochs=epochs,\nbatch_size=int(f_batch*trainX.shape[0]),\nvalidation_split=validation_split,\nverbose=0)\n## Visualize fitting history ##\n#HISTORY PLOT\nepochs_steps = [*range(0, len(history.history['loss']))]\n# MAKE PREDICTIONS\n\ntrain_predict = model.predict(trainX).squeeze()\n\ntest_predict = model.predict(testX).squeeze()\n\nprint(trainX.shape, train_predict.shape,trainY.shape,testX.shape, test_predict.shape,testY.shape)\n#COMPUTE RMSE\n\nprint(trainY.shape, train_predict.shape)\n\ntrain_rmse = np.sqrt(mean_squared_error(trainY, train_predict))\n\ntest_rmse = np.sqrt(mean_squared_error(testY, test_predict))\n\nprint(np.mean((trainY-train_predict)**2.0))\n\nprint(np.mean((testY-test_predict)**2.0))\n\nprint('Train MSE = %.5f RMSE = %.5f' % (train_rmse**2.0,train_rmse))\n\nprint('Test MSE = %.5f RMSE = %.5f' % (test_rmse**2.0,test_rmse))\n\n\n(179, 2, 1) 3 (179,)\n3 (179, 2, 1) (19, 2, 1) (179,) (19,)\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n                                                                 \n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n1/6 [====&gt;.........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/6 [==============================] - 0s 1ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 12ms/step\n(179, 2, 1) (179,) (179,) (19, 2, 1) (19,) (19,)\n(179,) (179,)\n0.010527734\n0.018228224\nTrain MSE = 0.01053 RMSE = 0.10260\nTest MSE = 0.01823 RMSE = 0.13501\n\n\n2023-05-04 02:44:16.817275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\nSimple RNN Parity Plot\n\n\nCode\n## Visualize parity plot ##\n# GET DATA\n\n# GENERATE PLOTLY FIGURE\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=trainY,y=train_predict,mode=\"markers\",name=\"Train Predict\"))\n\nfig.add_trace(go.Scatter(x=testY,y=test_predict,mode=\"markers\",name=\"Test Predict\"))\n\nfig.add_trace(go.Scatter(x=trainY,y=trainY,mode='lines',name=\"Ground Truth\"))\n\nfig.update_layout(\n    title=\"Parity Plot\",\n    xaxis_title=\"y_pred\",\n    yaxis_title=\"y_data\",\n    template=\"plotly_white\",\n    showlegend=True\n\n)\nfig.show()\n\n\n\n                                                \n\n\n\n\nPrediction Visualization\n\n\nCode\n## Visualize Predictions ##\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=Y_ind,y=Y,mode=\"markers\", marker=dict(\n                symbol=\"circle\",\n                color=\"steelblue\",\n                size=9),name=\"Target\"))\n\nfig.add_trace(go.Scatter(x=X_ind,y=X,mode=\"markers\", marker=dict(\n                symbol=\"circle\",\n                color=\"orange\",\n                size=6),name=\"Training Points\"))\n\nfig.add_trace(go.Scatter(x=Y_ind,y=train_predict,mode='markers', marker=dict(\n                symbol=\"circle\",\n                color=\"red\",\n                size=6), name=\"Prediction\"))\n\nfig.add_trace(go.Scatter(x=Y_ind, y=train_predict,mode='lines', marker=dict(\n                symbol=\"circle\",\n                color=\"tomato\",\n                size=6), name=\"Prediction\"))\n#fig.add_trace(go.Scatter(x=t1, y=train_data, mode='lines'))\n\nfig.update_layout(\n    font=dict(\n        family=\"Palatino\",\n        size=14),\n    title=\"Actual and Predicted Values\",\n    xaxis_title=\"Observation Number after given Time Steps\",\n    yaxis_title=\"Million Metric Tons of Carbon Dioxide Scaled\",\n    template=\"plotly_white\",\n    showlegend=True\n\n)\n\nfig.show()"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Greenhouse gas emissions have been one of the primary causes of climate change over the past few decades. These emissions are primarily produced by the burning of fossil fuels such as coal, oil, and natural gas. This has led to a global push towards renewable energy sources such as wind, solar, and hydropower. In this research paper, we will examine whether clean energy is helping to reduce greenhouse gas emissions, and we will use statistics and case studies to support our analysis."
  },
  {
    "objectID": "conclusions.html#project-introduction",
    "href": "conclusions.html#project-introduction",
    "title": "Conclusions",
    "section": "",
    "text": "Greenhouse gas emissions have been one of the primary causes of climate change over the past few decades. These emissions are primarily produced by the burning of fossil fuels such as coal, oil, and natural gas. This has led to a global push towards renewable energy sources such as wind, solar, and hydropower. In this research paper, we will examine whether clean energy is helping to reduce greenhouse gas emissions, and we will use statistics and case studies to support our analysis."
  },
  {
    "objectID": "conclusions.html#background",
    "href": "conclusions.html#background",
    "title": "Conclusions",
    "section": "Background",
    "text": "Background\nThe Intergovernmental Panel on Climate Change (IPCC) has warned that greenhouse gas emissions must be reduced by around 45% by 2030 and reach net-zero by 2050 in order to keep global warming below 1.5°C. This target is ambitious, and achieving it will require significant efforts from governments, industries, and individuals alike.\nThe adoption of renewable energy sources is a crucial part of these efforts. Renewable energy sources are clean and do not produce greenhouse gases during operation. As such, they are seen as a potential solution to the problem of greenhouse gas emissions.\n\n\n\nTWI-Global"
  },
  {
    "objectID": "conclusions.html#my-statistics",
    "href": "conclusions.html#my-statistics",
    "title": "Conclusions",
    "section": "My Statistics",
    "text": "My Statistics\nAccording to a report by the International Energy Agency (IEA), renewable energy accounted for over 80% of global electricity capacity additions in 2020. This was despite the challenges posed by the COVID-19 pandemic. The report also noted that renewable energy is expected to play a key role in meeting the world’s energy needs in the coming years.\nAnother report by the IEA found that renewable energy sources could provide 86% of global power generation by 2050. This would require significant investments in renewable energy infrastructure, but it is an achievable target.\nIn addition, a study by the National Renewable Energy Laboratory found that renewable energy sources could provide up to 80% of US electricity generation by 2050. This would require a significant shift away from fossil fuels, but it is also achievable with the right investments and policies."
  },
  {
    "objectID": "conclusions.html#case-studies",
    "href": "conclusions.html#case-studies",
    "title": "Conclusions",
    "section": "Case Studies",
    "text": "Case Studies\nLet us now look at some case studies to see how renewable energy is helping to reduce greenhouse gas emissions.\n\nDenmark:\nDenmark has set a goal of becoming carbon neutral by 2050. One of the ways it is achieving this goal is through the adoption of wind energy. Wind turbines currently provide around 47% of Denmark’s electricity needs, and the country has set a target of reaching 50% by 2030.\nAs a result of this push towards wind energy, Denmark has reduced its greenhouse gas emissions by around 34% since 1990. This has been achieved despite an increase in the country’s population and GDP over the same period.\n\n\nCalifornia:\nCalifornia has set a goal of achieving 100% clean energy by 2045. To achieve this goal, the state is investing heavily in solar energy, wind energy, and energy storage. As a result, California’s greenhouse gas emissions have been reduced by around 14% since 2004.\nIn addition, the state’s renewable energy sector has created over 150,000 jobs, which has had a positive impact on the local economy.\n\n\nChina:\nChina is the world’s largest emitter of greenhouse gases. However, the country is also investing heavily in renewable energy sources such as wind and solar.\nIn 2020, China added more renewable energy capacity than any other country in the world. This has helped to reduce the country’s reliance on coal and other fossil fuels, and it has led to a reduction in greenhouse gas emissions."
  },
  {
    "objectID": "conclusions.html#benefits-of-clean-energy",
    "href": "conclusions.html#benefits-of-clean-energy",
    "title": "Conclusions",
    "section": "Benefits of Clean Energy",
    "text": "Benefits of Clean Energy\nThere are several benefits to the adoption of clean energy sources. First and foremost, clean energy is sustainable and does not produce greenhouse gas emissions. This means that it is a key solution to the problem of climate change.\nIn addition, clean energy is often cheaper than fossil fuels in the long term. This is because renewable energy sources"
  },
  {
    "objectID": "conclusions.html#international-case-studies",
    "href": "conclusions.html#international-case-studies",
    "title": "Conclusions",
    "section": "International Case Studies",
    "text": "International Case Studies\nLet us now look at some case studies to see how renewable energy is helping to reduce greenhouse gas emissions.\n\nGermany: Energiewende and the Pursuit of a Low-Carbon Future\nGermany’s ambitious Energiewende program, initiated in 2010, aims to increase the share of renewable energy in the power mix to at least 80% by 2050 and reduce GHG emissions by 80-95% compared to 1990 levels. By 2020, renewable energy accounted for 46% of Germany’s electricity production. In 2019, Germany’s GHG emissions dropped by 6.3% compared to the previous year, reaching their lowest levels since 1990. The decline in emissions is attributed to the rapid expansion of renewable energy capacity, reduced coal consumption, and increased energy efficiency measures.\n\n\nDenmark: A Small Country with Big Renewable Energy Ambitions\nDenmark has consistently been a frontrunner in the renewable energy sector, particularly in wind power. In 2020, Denmark reached a milestone, with wind and solar energy combined providing 50% of the country’s electricity consumption. The Danish government has set an ambitious target to reduce GHG emissions by 70% by 2030, compared to 1990 levels, and to become carbon-neutral by 2050. Between 1990 and 2018, Denmark managed to reduce its GHG emissions by 32%, showcasing the effectiveness of the country’s clean energy initiatives.\n\n\nCosta Rica: Pursuing Carbon Neutrality through Clean Energy\nCosta Rica, a small Central American country, has made significant strides in renewable energy development. In 2020, renewable energy sources, including hydropower, geothermal, wind, and solar, accounted for approximately 99% of Costa Rica’s electricity generation. Costa Rica’s commitment to clean energy has resulted in a substantial reduction in GHG emissions from its energy sector. The country aims to become carbon-neutral by 2050, and its focus on renewable energy plays a crucial role in achieving this goal.\n\n\nSweden: A Leader in Clean Energy and Decarbonization\nSweden has been a global leader in clean energy adoption and decarbonization efforts. In 2020, 70% of Sweden’s electricity production came from renewable sources, with hydropower and wind power being the dominant contributors. As a result of its clean energy initiatives, Sweden managed to reduce its GHG emissions by 27% between 1990 and 2019, even as its economy grew by 78% during the same period. Sweden aims to achieve net-zero GHG emissions by 2045, demonstrating its commitment to combating climate change.\n\n\nAustralia: Embracing Solar Power and Reducing Emissions\nAustralia has vast potential for renewable energy, particularly solar power, due to its abundant sunlight and vast land area. In 2020, renewable energy sources accounted for 27.7% of Australia’s electricity generation, with solar and wind power contributing significantly to this figure. Australia’s clean energy initiatives have led to a decrease in GHG emissions from its electricity sector. Between 2005 and 2020, the country’s GHG emissions from the electricity sector decreased by 21%, demonstrating the positive impact of clean energy adoption on emissions reduction.\n\n\nChina: Wind and Solar Energy\nChina is the world’s largest emitter of greenhouse gases. However, the country is also investing heavily in renewable energy sources such as wind and solar.\nIn 2020, China added more renewable energy capacity than any other country in the world. This has helped to reduce the country’s reliance on coal and other fossil fuels, and it has led to a reduction in greenhouse gas emissions."
  },
  {
    "objectID": "conclusions.html#conclusion",
    "href": "conclusions.html#conclusion",
    "title": "Conclusions",
    "section": "Conclusion",
    "text": "Conclusion\nClean energy has shown to be an effective solution to reducing greenhouse gas emissions in the United States. These case studies demonstrate how states with clean energy policies and investments in renewable energy have seen significant reductions in greenhouse gas emissions. Solar and wind power have proven to be reliable alternatives to fossil fuels, while energy storage technologies have helped balance the electricity grid.\nAs the United States aims to achieve a net-zero carbon economy by 2050, more investment in clean energy and policies to encourage its adoption will be essential. The successful examples of California, Texas, New York, Hawaii, and Iowa show that clean energy can be an effective solution in the fight against climate change."
  },
  {
    "objectID": "fin_ts_models.html#gather-nextera-energy-stocks-from-yahoo-fiance",
    "href": "fin_ts_models.html#gather-nextera-energy-stocks-from-yahoo-fiance",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Gather NextEra Energy stocks from Yahoo Fiance",
    "text": "Gather NextEra Energy stocks from Yahoo Fiance\nNEE is the ticker symbol for NextEra Energy’s stock.\n\n\nCode\nNEE_ALL &lt;- getSymbols(\"NEE\",auto.assign = FALSE, from = \"2019-01-01\",to = \"2023-04-13\",src=\"yahoo\")\nNEE_ALL=data.frame(NEE_ALL)\nNEE_ALL &lt;- data.frame(NEE_ALL, rownames(NEE_ALL))\ncolnames(NEE_ALL)[7] = \"Year\"\nNEE_ALL$Year &lt;- as.Date(NEE_ALL$Year,\"%Y-%m-%d\")\nknitr::kable(head(NEE_ALL,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNEE.Open\nNEE.High\nNEE.Low\nNEE.Close\nNEE.Volume\nNEE.Adjusted\nYear\n\n\n\n\n2019-01-02\n43.1700\n43.3250\n42.2525\n42.4575\n10549600\n38.75114\n2019-01-02\n\n\n2019-01-03\n42.4775\n42.7900\n42.1675\n42.3525\n9260800\n38.65531\n2019-01-03\n\n\n2019-01-04\n42.2875\n43.1475\n42.1650\n43.1325\n10848800\n39.36721\n2019-01-04"
  },
  {
    "objectID": "fin_ts_models.html#visualize-nee-stock",
    "href": "fin_ts_models.html#visualize-nee-stock",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Visualize NEE Stock",
    "text": "Visualize NEE Stock\n\nCandlestick PlotTime Series Plot\n\n\n\n\nCode\n# plotly\n# change font family and size\nt &lt;- list(\n  family = \"Palatino\",\n  size = 14)\n\ntheme_set(theme_gray(base_size=12,base_family=\"Palatino\"))\nfig &lt;- NEE_ALL |&gt; plot_ly(x = ~Year, type=\"candlestick\",\n          open = ~NEE.Open, close = ~NEE.Close,\n          high = ~NEE.High, low = ~NEE.Low, name=\"NEE\") \n\nfig &lt;- fig |&gt; add_lines(x = ~Year, y = ma(NEE_ALL$NEE.Adjusted, 20), name=\"20-MA\", line = list(color = 'deeppink', width = 1), inherit = F)\n\nfig &lt;- fig |&gt; add_lines(x = ~Year, y = ma(NEE_ALL$NEE.Adjusted, 80), name=\"80-MA\", line = list(color = 'cyan3', width = 1), inherit = F)\n\nfig &lt;- fig |&gt; layout(title = \"Candlestick Chart of NextEra Energy Stock Prices\",\n              font=t, \n              legend = list(orientation = 'h', \n                            x = 0.5, \n                            y = 1,\n                            xanchor = 'center', \n                            yref = 'paper', \n                            font = list(size = 10), \n                            bgcolor = 'transparent'),\n              xaxis = list(\n                type = 'date',\n                tickformat = \"%d %B (%a)&lt;br&gt;%Y\"\n                ),\n              plot_bgcolor = \"#eeeeee\"\n              )\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- NEE_ALL |&gt;\n  ggplot() +\n  geom_line(aes(y=NEE.Adjusted, x=Year),color=\"aquamarine3\") +\n  ggtitle(\"NextEra Energy Stock Pirces\") +\n  ylab(\"USD\") +\n  xlab(\"Month\") +\n  scale_x_date(date_breaks = \"1 month\",\n                             labels = scales::date_format(format = \"%Y-%m\")) +\n  # rotate labels for visibility\n  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust=1))\n\nggplotly(p)\n\n\n\n\n\n\n\n\n\nFrom the above figures, we can see the spikes of volatility in the price, especially around certain points where the price dramatically drops and rises (see around mid 2020). This is a good first indicator that there will be an ARCH/GARCH component when fitting the data. Another way to look at this data is by taking a look at price versus volume. This just gives us a better sense of the price and a different perspective, but it doesn’t necessarily provide us with more insight.\n\nPrice Volumn Chart\n\n\nCode\nNEE_ALL &lt;- getSymbols(\"NEE\",auto.assign = FALSE, from = \"2019-01-01\",to = \"2023-04-13\",src=\"yahoo\")\nchartSeries(NEE_ALL, theme = chartTheme(\"white\"), # Theme\n            bar.type = \"hlc\",  # High low close \n            up.col = \"green\",  # Up candle color\n            dn.col = \"red\",\n            name = \"NEE Stock\")   # Down candle color)\n\n\n\n\n\nAgain, the above figure shows us more information on the price trends versus the volume, but the overall conclusion is the same. However, the next figure shows the volatility better as the log of the returns is taken and plotted.\n\n\nCode\nreturns &lt;- diff(log(NEE_ALL$`NEE.Adjusted`), lag = 1)\nreturns %&gt;% chartSeries(theme = chartTheme(\"white\"), name = \"NEE Stock Daily Log Returns\")\n\n\n\n\n\nThe above figure shows the daily log of the returns, which gives a much better sense of the volatility of the returns for NextEra Energy. Here, since there is clear volatility, an ARCH/GARCH model will be needed to fit the data. However, before fitting an ARCH model immediately, it is important to check the ACF/PACF plots to check if an ARIMA model should be fit first before fitting an ARCH model on the residuals. First, since we took the log of the returns, we should plot the ACF to check if we should difference the data."
  },
  {
    "objectID": "fin_ts_models.html#acf-and-pacf-plots",
    "href": "fin_ts_models.html#acf-and-pacf-plots",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF and PACF Plots",
    "text": "ACF and PACF Plots\n\n\nCode\nlog(NEE_ALL$'NEE.Adjusted') |&gt; ggAcf() +\n  ggtitle(\"ACF of Log Transformed NextEra Energy Prices\")\n\n\n\n\n\nFrom the above figure, it is clear that a difference must be done to account for the correlation in the data. After differencing the data once, the ACF and PACF plots are created and shown below.\n\nACFPACF\n\n\n\n\nCode\nacf &lt;- ggAcf(returns, lag.max = 100) +\n  ggtitle(\"ACF of Log Transformed and Differenced NextEra Energy Prices\")\nggplotly(acf)\n\n\n\n\n\n\n\n\n\n\nCode\npacf &lt;- ggPacf(returns, lag.max = 100) +\n  ggtitle(\"PACF of Log Transformed and Differenced NextEra Energy Prices\")\nggplotly(pacf)\n\n\n\n\n\n\n\n\n\n\nSeasonality Test\n\n\nCode\nisSeasonal(returns, test = \"combined\", freq = NA)\n\n\n[1] FALSE\n\n\nAccording to the seasonality test, NextEra stock price is not seasonal time series data."
  },
  {
    "objectID": "fin_ts_models.html#model-fitting---arima-model",
    "href": "fin_ts_models.html#model-fitting---arima-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Model Fitting - ARIMA Model",
    "text": "Model Fitting - ARIMA Model\nNow that the data is differenced, the ACF and PACF plots clearly show a need for an ARIMA model to fit the data. Once an ARIMA model is fit, there will probably be a need to fit an ARCH model on the residuals, but that will depend on the residuals. Let’s fit an ARIMA model first. From the above ACF and PACF plots, it would be appropriate to check all Arima models with p values ranging from 1-7 and q values ranging from 1-7. For the ARIMA model, d is set to 1 as we differenced the data once. From that, the following results were achieved.\n\n\nCode\nd = 1\ni = 1\ntemp = data.frame()\nls = matrix(rep(NA,6*43),nrow=43)\n\nfor (p in 1:7)\n{\n  for(q in 1:7)\n  {\n    if(p + d + q &lt;= 12)\n    {\n      model &lt;- Arima(log(NEE_ALL$'NEE.Adjusted'),order=c(p,d,q), include.drift = TRUE)\n      ls[i,] = c(p,d,q,model$aic,model$bic,model$aicc)\n      i = i + 1\n    }\n  }\n}\n\ntemp = as.data.frame(ls)\nnames(temp) = c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n\n\n\nCode\n#knitr::kable(temp)\nknitr::kable(temp[which.min(temp$AIC),])\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n26\n4\n1\n5\n-5615.411\n-5560.62\n-5615.163\n\n\n\n\n\nCode\nknitr::kable(temp[which.min(temp$BIC),])\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n9\n2\n1\n2\n-5607.54\n-5577.654\n-5607.462\n\n\n\n\n\nCode\n# auto.arima()\nauto.arima(log(NEE_ALL$'NEE.Adjusted'))\n\n\nSeries: log(NEE_ALL$NEE.Adjusted) \nARIMA(4,1,5) \n\nCoefficients:\n          ar1     ar2      ar3      ar4     ma1      ma2     ma3     ma4\n      -0.1793  0.7322  -0.3307  -0.7621  0.1423  -0.7033  0.3190  0.6553\ns.e.   0.0753  0.0750   0.0547   0.0549  0.0799   0.0814  0.0658  0.0646\n         ma5\n      0.0980\ns.e.  0.0365\n\nsigma^2 = 0.0003135:  log likelihood = 2817.93\nAIC=-5615.87   AICc=-5615.66   BIC=-5566.06\n\n\nBy sorting the table, the smallest AIC has the parameters of 4,1,5, while by sorting using the BIC criterion, the model has parameters of 2,1,2. Next, a function allows for an auto-selection of an ARIMA model. According to the auto.arima() method in R, the parameters for the model should be 2,1,2. Therefore, the 2,1,2 model makes the most sense.\nNext, the model should undergo some diagnostics to make sure the fit looks good. Below are some basic diagnostics run using the 2,1,2 model.\n\n\nCode\nsarimafit &lt;- sarima(log(NEE_ALL$'NEE.Adjusted'), 2, 1, 2, details = FALSE)\n\n\n\n\n\n\n\nGiven the figure above, the following observations can be made. First, when looking at the plot of standardized residuals, the mean should be around 0, with the variance at about 1. As seen above, this is generally true, with the mean close to 0. However, the variance is probably slightly more than 1 and there is clusters of higher variance, which indicates a second model should be fit for the errors. Generally, if the mean or variance is significantly different than expected, this would be a sign of a poor model fit, but we don’t really see that here. Next, when inspecting the ACF of residuals, the plot shows no significant lags, which is a very encouraging sign considering the logic above. Next, when looking at the qq-plot, the hope is to see some signs of normality. Here, we see some signs of normality in the data, but with a bit of skew. Finally, when looking at the p-values for the Ljung-Box test, many of the p-values are above 0.05, which is a decent sign of a good model.\nNext, we can do an initial check on the absolute returns and squared returns to check if an ARCH model is needed. Below is a set of visualizations for the ACF and PACF plots for the absolute and squared returns.\n\nSquared Residuals ACFSquared Residuals PACF\n\n\n\n\nCode\nfit &lt;- arima(log(NEE_ALL$'NEE.Adjusted'), order = c(2, 1, 2))\nres.arima &lt;- fit$res\nsquared.res.arima &lt;- res.arima^2\nacf_residuals &lt;- ggAcf(squared.res.arima, na.action = na.pass) + ggtitle(\"ACF Squared Residuals\")\nggplotly(acf_residuals)\n\n\n\n\n\n\n\n\n\n\nCode\nfit &lt;- arima(log(NEE_ALL$'NEE.Adjusted'), order = c(2, 1, 2))\nres.arima &lt;- fit$res\nsquared.res.arima &lt;- res.arima^2\npacf_residuals &lt;- ggPacf(squared.res.arima, na.action = na.pass) + ggtitle(\"ACF Squared Residuals\")\nggplotly(pacf_residuals)\n\n\n\n\n\n\n\n\n\nFrom the above plots and the previous diagnostics, it is pretty clear an ARCH model is needed to fit the residuals of the ARIMA model. Using the above plots, the p-value should range between 1-9. Then, using a similar technique to fitting the ARIMA model, a set of ARCH models can be created and evaluated to find the best fit for the residuals. Below is another table that shows the fitted ARCH models with their associated AICs."
  },
  {
    "objectID": "fin_ts_models.html#model-fitting---arima212-arch-model",
    "href": "fin_ts_models.html#model-fitting---arima212-arch-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Model Fitting - ARIMA(2,1,2) + ARCH Model",
    "text": "Model Fitting - ARIMA(2,1,2) + ARCH Model\n\n\nCode\nARCH &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:9) {\n  ARCH[[cc]] &lt;- garch(res.arima, order = c(0, p), trace = F)\n  cc &lt;- cc + 1\n}\n\n## get AIC values for model evaluation\nARCH_AIC &lt;- sapply(ARCH, AIC) ## model with lowest AIC is the best\nmin(ARCH_AIC)\n\n\n[1] -5907.563\n\n\nCode\nwhich(ARCH_AIC == min(ARCH_AIC))\n\n\n[1] 6\n\n\nCode\nARCH[[which(ARCH_AIC == min(ARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res.arima, order = c(0, p), trace = F)\n\nCoefficient(s):\n       a0         a1         a2         a3         a4         a5         a6  \n0.0000901  0.1575585  0.1380603  0.1605526  0.0433259  0.0436153  0.1958591  \n\n\nCode\narch.df &lt;- data.frame(p = 1:9, q = 0, AIC = ARCH_AIC)\nknitr::kable(arch.df)\n\n\n\n\n\np\nq\nAIC\n\n\n\n\n1\n0\n-5749.417\n\n\n2\n0\n-5818.602\n\n\n3\n0\n-5886.832\n\n\n4\n0\n-5882.985\n\n\n5\n0\n-5881.615\n\n\n6\n0\n-5907.563\n\n\n7\n0\n-5905.209\n\n\n8\n0\n-5887.420\n\n\n9\n0\n-5886.691\n\n\n\n\n\nBy sorting the table, the smallest AIC has the parameters of 6,0. Therefore, we should fit the residuals with a 6,0 ARCH model. Now that we have fit the data with an ARCH/ARIMA model, the next step is to check the fit. One option is to check the residuals. First, we can plot the residuals and evaluate the results. Additionally, we can check whether the residuals are normal.\n\nDiagnosticsNormality TestsLjung-Box Test\n\n\n\n\nCode\nfit2 &lt;- garch(res.arima, order = c(0,6), trace = FALSE)\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 15.323, df = 10, p-value = 0.1207\n\nModel df: 0.   Total lags used: 10\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test(fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.22758, df = 1, p-value = 0.6333\n\n\n\n\n\nFrom above, we can see a couple of important diagnostic plots. The first set of plots show a set of 3 plots of the residuals, the ACF of the residuals, and a distribution of the residuals. The residuals seem to now have a constant mean and variance according to the first plot. Next, the ACF shows strong signs of stationarity in the residuals. Finally, the distribution of the residuals is relatively normal. This can be confirmed using the second tab, which is a qqnorm plot of the residuals. Using this plot, the normality of the residuals can be confirmed. These plots all point towards strong signs the model is a good fit for the data. Finally, a Ljung-Box test outputs a p-value greater than 0.05, indicating that the residuals are independent, encouraging the suitable model. Here, after doing the model diagnostics, the model above can be used for forecasting, and the model is ARCH(6,0) + ARIMA(2,1,2)."
  },
  {
    "objectID": "fin_ts_models.html#forecast",
    "href": "fin_ts_models.html#forecast",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Forecast",
    "text": "Forecast\n\n\nCode\nfit3 = garchFit(~ arma(2,2) + garch(6, 0), data = res.arima, trace = FALSE)\npredict(fit3, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeanForecast\nmeanError\nstandardDeviation\nlowerInterval\nupperInterval\n\n\n\n\n0.0020155\n0.0103173\n0.0103173\n-0.0182060\n0.0222370\n\n\n0.0015082\n0.0121532\n0.0121429\n-0.0223117\n0.0253281\n\n\n0.0015581\n0.0116165\n0.0115736\n-0.0212099\n0.0243261\n\n\n0.0016771\n0.0123125\n0.0122627\n-0.0224550\n0.0258092\n\n\n0.0016808\n0.0124056\n0.0123578\n-0.0226337\n0.0259953\n\n\n\n\n\n\nUsing this model, the data can be effectively modeled and perhaps future values can be forecasted. However, stock market prices are generally impossible to forecast. We can say this model does a decent job at modeling the data and its variance."
  },
  {
    "objectID": "models2.html#fitting-sarimax-model-using-auto.arima",
    "href": "models2.html#fitting-sarimax-model-using-auto.arima",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting SARIMAX model using auto.arima()",
    "text": "Fitting SARIMAX model using auto.arima()\nUsing auto.arima() function here to fit the ARIMAX model. Here we are trying to predict CO2 Emissions using Renewable Energy, Petroleum, Natural gas, and Coal. All variables are time series and the exogenous variables in this case are Renewable Energy, Petroleum, Natural gas, and Coal.\n\n\nCode\n# \"CO2 Emissions\",\"Renewable\",\"Petroleum\",\"Natural Gas\",\"Coal\"\nxreg &lt;- cbind(Rnwbl = dd.ts[, \"Renewable\"],\n              Ptlm = dd.ts[, \"Petroleum\"],\n              Ntlgs = dd.ts[, \"Natural Gas\"],\n              Cl = dd.ts[, \"Coal\"])\n\nfit &lt;- auto.arima(dd.ts[, \"CO2 Emissions\"], xreg = xreg, seasonal= TRUE)\nsummary(fit)\n\n\nSeries: dd.ts[, \"CO2 Emissions\"] \nRegression with ARIMA(4,0,0)(0,1,1)[12] errors \n\nCoefficients:\n         ar1     ar2     ar3     ar4     sma1    Rnwbl    Ptlm   Ntlgs       Cl\n      0.7384  0.0817  0.0343  0.1269  -0.7452  -0.0017  0.0728  0.1218  -0.0065\ns.e.  0.0417  0.0510  0.0512  0.0412   0.0287   0.0166  0.0045  0.0040   0.0069\n\nsigma^2 = 85.53:  log likelihood = -2143.09\nAIC=4306.19   AICc=4306.57   BIC=4349.95\n\nTraining set error measures:\n                     ME     RMSE      MAE         MPE    MAPE      MASE\nTraining set -0.1968328 9.084863 6.881898 -0.03815555 1.18767 0.3113222\n                     ACF1\nTraining set -0.003652219\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(4,0,0)(0,1,1)[12] errors\nQ* = 33.387, df = 19, p-value = 0.02168\n\nModel df: 5.   Total lags used: 24\n\n\nFrom the results above, we know that this is a regression model with SARIMA(4,0,0)(0,1,1)[12]."
  },
  {
    "objectID": "models2.html#fitting-the-model-manually---regression-with-arma-errors",
    "href": "models2.html#fitting-the-model-manually---regression-with-arma-errors",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting the model manually - Regression with ARMA Errors",
    "text": "Fitting the model manually - Regression with ARMA Errors\nHere we first have to fit the linear regression model predicting CO2 Emissions using Renewable Energy, Petroleum, Natural gas, and Coal.\nThen for the residuals, we will fit an ARIMA/SARIMA model.\n\nFirst fit the linear model\n\n\nCode\ndd$`CO2 Emissions` &lt;- ts(dd$`CO2 Emissions`, star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$Renewable &lt;- ts(dd$Renewable,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$Petroleum &lt;-ts(dd$Petroleum,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$`Natural Gas` &lt;-ts(dd$`Natural Gas`,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\ndd$Coal &lt;-ts(dd$Coal,star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n# First fit the linear model\nfit.reg &lt;- lm(`CO2 Emissions` ~ Renewable + Petroleum + `Natural Gas` + Coal, data=dd)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = `CO2 Emissions` ~ Renewable + Petroleum + `Natural Gas` + \n    Coal, data = dd)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-107.596  -27.274   -1.138   24.142  126.391 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.015e+02  2.092e+01  -9.630  &lt; 2e-16 ***\nRenewable     -1.098e-01  1.316e-02  -8.348 4.88e-16 ***\nPetroleum      1.957e-01  6.690e-03  29.252  &lt; 2e-16 ***\n`Natural Gas`  5.652e-02  4.124e-03  13.705  &lt; 2e-16 ***\nCoal           1.088e-01  4.877e-03  22.308  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.52 on 595 degrees of freedom\nMultiple R-squared:  0.7462,    Adjusted R-squared:  0.7445 \nF-statistic: 437.3 on 4 and 595 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nLinear Model Residuals\n\nOriginal Time Series ResidualsFirst DiffFirst Diff and Seasonality Diff\n\n\n\n\nCode\nres.fit &lt;- ts(residuals(fit.reg), star=decimal_date(as.Date(\"1973-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n\nggtsdisplay(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit |&gt; diff() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\nres.fit |&gt; diff() |&gt; diff(lag=12) |&gt; ggtsdisplay()\n\n\n\n\n\n\n\n\n\n\nStationarity and Seasonality Test\n\n\nCode\nres.fit |&gt; diff() |&gt; diff(lag=12) |&gt; adf.test()\n\n\nWarning in adf.test(diff(diff(res.fit), lag = 12)): p-value smaller than\nprinted p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(diff(res.fit), lag = 12)\nDickey-Fuller = -10.178, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nCode\ndiff.res.fit &lt;- res.fit |&gt; diff() |&gt; diff(lag=12) \nisSeasonal(diff.res.fit, test = \"combined\", freq = NA)\n\n\n[1] FALSE\n\n\nAfter first differencing and seasonality differencing, the data is proved to be stationary. Now let’s find the model parameters of the time series linear regression residual data.\n\n\nFind the Model Parameters\n\n\nCode\n#q=1,2, Q=1,2 , p=1,2, P=0,1,2,3\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*200),nrow=200)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n       \n          if(p+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\n\n\nCode\n#q=1,2, Q=1,2 , p=0,1,2, P=0,1,2,3 \n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,data=res.fit) |&gt;\n  drop_na()\n\nminaic &lt;- output[which.min(output$AIC), ]\nminbic &lt;- output[which.min(output$BIC), ]\n\n\n\nParameters with Minimum AICParameters with Minimum BICUsing auto.arima()\n\n\n\n\nCode\nknitr::kable(minaic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n25\n1\n1\n1\n0\n1\n1\n4877.404\n4894.904\n4877.473\n\n\n\n\n\n\n\n\n\nCode\nknitr::kable(minbic)\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n25\n1\n1\n1\n0\n1\n1\n4877.404\n4894.904\n4877.473\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(res.fit)\n\n\nSeries: res.fit \nARIMA(2,0,1)(2,1,1)[12] \n\nCoefficients:\n        ar1      ar2      ma1    sar1    sar2     sma1\n      1.392  -0.4079  -0.7879  0.0674  -0.072  -0.8284\ns.e.  0.079   0.0732   0.0580  0.0506   0.049   0.0304\n\nsigma^2 = 226.2:  log likelihood = -2432.01\nAIC=4878.03   AICc=4878.22   BIC=4908.67\n\n\n\n\n\nBest model from the output is SARIMA(1,1,1)x(0,1,1)[12]. auto.arima() suggested SARIMA(2,0,1)(2,1,1)[12]\n\n\n\nModel diagnostics\nFrom model fitting, we generated 1 model, SARIMA((1,1,1)x(0,1,1)[12]. auto.arima() generated SARIMA(2,0,1) x (2,1,1)[12]. It looks like SARIMA(1,1,1)x(0,1,1)[12] has the lower AIC, BIC, and AICc. Now let’s do two model diagnoses to analyze the result and find the better model to do forecast later.\n\nSARIMA(2,0,1)(2,1,1)[12]\n\n\nCode\nset.seed(1234)\nfit1 &lt;- Arima(res.fit, order=c(2,0,1), seasonal = list(order = c(2,1,1), period = 12))\n\n\n\n\nModel Fitting Visual Results and Residuals\n\n\nCode\nmodel_output &lt;- capture.output(sarima(res.fit,2,0,1,2,1,1,12))\n\n\n\n\n\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(2,0,1), seasonal = list(order = c(2,1,1), period = 12)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,0,1)(2,1,1)[12]\nQ* = 36.154, df = 18, p-value = 0.006743\n\nModel df: 6.   Total lags used: 24\n\n\nThe Ljung-Box test uses the following hypotheses:\nH0: The residuals are independently distributed.\nHA: The residuals are not independently distributed; they exhibit serial correlation.\nIdeally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.\n\n\n\nThere are two significant spikes in the ACF, and the model didn’t fail the Ljung-Box test.\n\n\nModel Fitting\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(2,0,1), seasonal = list(order=c(2,1,1), period=12))\n\n\nSeries: res.fit \nARIMA(2,0,1)(2,1,1)[12] \n\nCoefficients:\n        ar1      ar2      ma1    sar1    sar2     sma1\n      1.392  -0.4079  -0.7879  0.0674  -0.072  -0.8284\ns.e.  0.079   0.0732   0.0580  0.0506   0.049   0.0304\n\nsigma^2 = 226.2:  log likelihood = -2432.01\nAIC=4878.03   AICc=4878.22   BIC=4908.67\n\n\n\n\nModel Output Diagnostics\n\n\nCode\ncat(model_output[65:100], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1    sar1     sar2     sma1  constant\n      1.3922  -0.4080  -0.7880  0.0674  -0.0721  -0.8284    0.0019\ns.e.  0.0791   0.0733   0.0582  0.0506   0.0490   0.0304    0.1239\n\nsigma^2 estimated as 223.9:  log likelihood = -2432.01,  aic = 4880.03\n\n$degrees_of_freedom\n[1] 581\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        1.3922 0.0791  17.6042  0.0000\nar2       -0.4080 0.0733  -5.5656  0.0000\nma1       -0.7880 0.0582 -13.5489  0.0000\nsar1       0.0674 0.0506   1.3319  0.1834\nsar2      -0.0721 0.0490  -1.4715  0.1417\nsma1      -0.8284 0.0304 -27.2535  0.0000\nconstant   0.0019 0.1239   0.0154  0.9877\n\n$AIC\n[1] 8.29937\n\n$AICc\n[1] 8.299698\n\n$BIC\n[1] 8.358917\n\n\n\n\nCode\nsummary(fit1)\n\n\nSeries: res.fit \nARIMA(2,0,1)(2,1,1)[12] \n\nCoefficients:\n        ar1      ar2      ma1    sar1    sar2     sma1\n      1.392  -0.4079  -0.7879  0.0674  -0.072  -0.8284\ns.e.  0.079   0.0732   0.0580  0.0506   0.049   0.0304\n\nsigma^2 = 226.2:  log likelihood = -2432.01\nAIC=4878.03   AICc=4878.22   BIC=4908.67\n\nTraining set error measures:\n                      ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.006210686 14.81186 11.34788 -11.06374 110.0339 0.6029853\n                    ACF1\nTraining set -0.01093927\n\n\n\n\n\nSARIMA(1,1,1)x(0,1,1)[12]\n\n\nCode\nfit2 &lt;- Arima(res.fit , order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12))\n\n\n\nModel Fitting Visual Results and Residuals\n\n\nCode\nmodel_output2 &lt;- capture.output(sarima(res.fit,1,1,1,0,1,1,12))\n\n\n\n\n\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12)) |&gt;\n  residuals() |&gt; ggtsdisplay()\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,1)(0,1,1)[12]\nQ* = 45.782, df = 21, p-value = 0.001366\n\nModel df: 3.   Total lags used: 24\n\n\nThere is one significant spike in the ACF, and the model didn’t fail the Ljung-Box test.\n\n\nModel Fitting\n\n\nCode\nres.fit |&gt;\n  Arima(order=c(1,1,1), seasonal = list(order = c(0,1,1), period = 12))\n\n\nSeries: res.fit \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4485  -0.8254  -0.8340\ns.e.  0.0637   0.0409   0.0232\n\nsigma^2 = 229.8:  log likelihood = -2434.7\nAIC=4877.4   AICc=4877.47   BIC=4894.9\n\n\n\n\nModel Output Diagnostics\n\n\nCode\ncat(model_output2[30:61], model_output2[length(model_output2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sma1\n      0.4485  -0.8254  -0.8340\ns.e.  0.0637   0.0409   0.0232\n\nsigma^2 estimated as 228.6:  log likelihood = -2434.7,  aic = 4877.4\n\n$degrees_of_freedom\n[1] 584\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.4485 0.0637   7.0433       0\nma1   -0.8254 0.0409 -20.2006       0\nsma1  -0.8340 0.0232 -35.8717       0\n\n$AIC\n[1] 8.309036\n\n$AICc\n[1] 8.309106\n\n$BIC\n[1] 8.338849\n\n\n\n\nCode\nsummary(fit2)\n\n\nSeries: res.fit \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4485  -0.8254  -0.8340\ns.e.  0.0637   0.0409   0.0232\n\nsigma^2 = 229.8:  log likelihood = -2434.7\nAIC=4877.4   AICc=4877.47   BIC=4894.9\n\nTraining set error measures:\n                     ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -0.2983109 14.95631 11.52741 -17.96732 111.7469 0.6125249\n                    ACF1\nTraining set -0.01297584\n\n\n\n\n\n\nModel Selection\nFrom two model diagnostics above, both SARIMA(1,1,1)(0,1,1)[12] and SARIMA(1,1,1) x (2,1,2)[12] model have similar number of spikes in the ACF and PACF plots of its residuals. All the training set error measures of the two models are similar. SARIMA(1,1,1)(0,1,1)[12] model has a slightly smaller sigma squared which means it has smaller variance. The estimators with a smaller variance is more efficient.\n\n\nCross Validation\n\n\nCode\nn=length(res.fit)\nk=120\n \n #n-k=480; 480/12=40;\n \nrmse1 &lt;- matrix(NA, 40,12)\nrmse2 &lt;- matrix(NA,40,12)\nrmse3 &lt;- matrix(NA,40,12)\n\nst &lt;- tsp(res.fit)[1]+(k-1)/12 \n\nfor(i in 1:10)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(res.fit, end=st + i-1)\n  xtest &lt;- window(res.fit, start=st + (i-1) + 1/12, end=st + i)\n  \n  #ARIMA(1,1,2)x(0,1,0)[12]. auto.arima() - ARIMA(2,0,1)(2,1,1)[12]\n  \n  fit &lt;- Arima(xtrain, order=c(2,0,1), seasonal=list(order=c(2,1,1), period=12),\n                include.drift=TRUE, method=\"CSS\")\n  fcast &lt;- forecast(fit, h=40)\n  \n  fit2 &lt;- Arima(xtrain, order=c(1,1,1), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"CSS\")\n  fcast2 &lt;- forecast(fit2, h=40)\n  \n  fit3 &lt;- Arima(xtrain, order=c(2,1,1), seasonal=list(order=c(2,1,1), period=12),\n                include.drift=TRUE, method=\"CSS\")\n  fcast3 &lt;- forecast(fit3, h=40)\n  \n\n  rmse1[i,1:length(xtest)]  &lt;- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] &lt;- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] &lt;- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:12, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:12, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\nThis fit is good based on low RMSE: SARIMA(2,0,1)x(2,1,1)[12]\n\n\nModel Fitting - SARIMA(2,0,1)x(2,1,1)[12]\n\n\nCode\nxreg &lt;- cbind(RNWBL = dd[, \"Renewable\"],\n              PTRLM = dd[, \"Petroleum\"],\n              NTRLG = dd[, \"Natural Gas\"],\n              CL = dd[, \"Coal\"])\n\n\nfit &lt;- Arima(dd$`CO2 Emissions`,order=c(2,0,1),seasonal = c(2,1,1),xreg=xreg,method=\"CSS\")\nsummary(fit)\n\n\nSeries: dd$`CO2 Emissions` \nRegression with ARIMA(2,0,1)(2,1,1)[12] errors \n\nCoefficients:\n         ar1      ar2      ma1     sar1     sar2     sma1    RNWBL   PTRLM\n      1.5783  -0.5791  -0.8522  -0.0893  -0.1104  -0.6919  -0.0009  0.0733\ns.e.  0.0731   0.0725   0.0513   0.0578   0.0509   0.0446   0.0164  0.0047\n       NTRLG       CL\n      0.1223  -0.0084\ns.e.  0.0041   0.0069\n\nsigma^2 = 83.51:  log likelihood = -2143.52\n\nTraining set error measures:\n                     ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set -0.3175613 8.969186 6.579235 -0.05707093 1.127818 0.2976303\n                    ACF1\nTraining set -0.01008774"
  },
  {
    "objectID": "models2.html#forecast---sarima211x21112",
    "href": "models2.html#forecast---sarima211x21112",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecast - SARIMA(2,1,1)x(2,1,1)[12]",
    "text": "Forecast - SARIMA(2,1,1)x(2,1,1)[12]\nIn order to forecast CO2 Emissions variable, or the whole fit, we need to have forecasts of Renewable Energy, Petroleum, Natural gas, and Coal.\nHere we will be using auto.arima() to fit the Renewable Energy, Petroleum, Natural gas, and Coal variables.\n\nFitting SARIMA model to different energy consumption variables\n\nRenewable Energy\n\n\n\n\nCode\nRNWBL_fit &lt;- auto.arima(dd$Renewable) #fiting an ARIMA model to the CO2 emissions variable\nsummary(RNWBL_fit)\n\n\nSeries: dd$Renewable \nARIMA(0,1,2)(1,1,2)[12] \n\nCoefficients:\n          ma1      ma2    sar1     sma1    sma2\n      -0.2496  -0.1966  0.3548  -1.1398  0.2403\ns.e.   0.0408   0.0414  0.5676   0.5770  0.4817\n\nsigma^2 = 534.4:  log likelihood = -2681.06\nAIC=5374.11   AICc=5374.26   BIC=5400.36\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.2969799 22.76701 17.49176 -0.0738994 3.172679 0.4860948\n                    ACF1\nTraining set 0.006186365\n\n\n\n\nCode\nfRNWBL &lt;- forecast(RNWBL_fit, h=60)\n\n\n\nPetroleum\n\n\nCode\nPTRLM_fit &lt;- auto.arima(dd$Petroleum) #fiting an ARIMA model to the petroleum variable\nsummary(PTRLM_fit)\n\n\nSeries: dd$Petroleum \nARIMA(3,0,2)(2,1,1)[12] \n\nCoefficients:\n          ar1     ar2     ar3     ma1     ma2    sar1     sar2     sma1\n      -0.0239  0.2107  0.6388  0.7031  0.4326  0.0213  -0.0886  -0.8043\ns.e.   0.1010  0.0813  0.1060  0.1306  0.1478  0.0558   0.0511   0.0355\n\nsigma^2 = 7153:  log likelihood = -3446.92\nAIC=6911.83   AICc=6912.14   BIC=6951.22\n\nTraining set error measures:\n                   ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set 1.419845 83.15279 59.65203 -0.02073471 2.058544 0.5540171\n                    ACF1\nTraining set -0.03071941\n\n\n\n\nCode\nfPTRLM &lt;- forecast(PTRLM_fit, h=60)\n\n\n\n\nNatural Gas\n\n\nCode\nNTRLG_fit &lt;- auto.arima(dd$`Natural Gas`) #fiting an ARIMA model to the petroleum variable\nsummary(NTRLG_fit)\n\n\nSeries: dd$`Natural Gas` \nARIMA(2,1,1)(1,1,2)[12] \n\nCoefficients:\n         ar1      ar2      ma1     sar1     sma1     sma2\n      0.4262  -0.0133  -0.9085  -0.4149  -0.2781  -0.4480\ns.e.  0.0475   0.0455   0.0236   0.1921   0.1801   0.1392\n\nsigma^2 = 9115:  log likelihood = -3512.73\nAIC=7039.46   AICc=7039.66   BIC=7070.09\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 6.345775 93.94888 69.62737 0.09510603 3.560609 0.6725354\n                     ACF1\nTraining set -0.005717725\n\n\n\n\nCode\nfNTRLG &lt;- forecast(NTRLG_fit, h=60)\n\n\n\n\nCoal\n\n\nCode\nCL_fit &lt;- auto.arima(dd$Coal) #fiting an ARIMA model to the petroleum variable\nsummary(CL_fit)\n\n\nSeries: dd$Coal \nARIMA(2,1,2)(2,1,1)[12] \n\nCoefficients:\n          ar1     ar2      ma1      ma2    sar1     sar2     sma1\n      -0.1566  0.4381  -0.1342  -0.6282  0.0155  -0.1504  -0.7816\ns.e.   0.5999  0.2635   0.5924   0.4288  0.0520   0.0481   0.0331\n\nsigma^2 = 2982:  log likelihood = -3184.71\nAIC=6385.42   AICc=6385.67   BIC=6420.43\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -1.378761 53.69276 39.66915 -0.1088342 2.908254 0.5045871\n                     ACF1\nTraining set -0.001052996\n\n\n\n\nCode\nfCL &lt;- forecast(CL_fit,  h=60)\n\n\n\n\n\n\n\n\nCode\nfxreg &lt;- cbind(RNWBL = fRNWBL$mean,\n               PTRLM = fPTRLM$mean,\n               NTRLG = fNTRLG$mean,\n               CL = fCL$mean)\n\nfcast &lt;- forecast(fit, xreg=fxreg) #fimp$mean gives the forecasted values\ng &lt;- autoplot(fcast) + \n  xlab(\"Year\") +\n  ylab(\"Million Metric Tons of Carbon Dioxide\") +\n  ggtitle(\"CO2 Emissions Forecast from SARIMAX Model\")\nggplotly(g)"
  },
  {
    "objectID": "deep_learning.html#train-and-test-plot",
    "href": "deep_learning.html#train-and-test-plot",
    "title": "Deep Learning for TS",
    "section": "Train and Test Plot",
    "text": "Train and Test Plot\n\n\nCode\n## Test-train split ##\n# Parameter split_percent defines the ratio of training examples\n\ndef get_train_test(data, split_percent=0.9):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n    # Point for splitting data into train and test\n\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\ntrain_data, test_data, data = get_train_test(X)\n#print(train_data.shape)\n#print(test_data.shape)\n## Visualize: training-test split ##\n# SINGLE SERIES\n\nt1=[*range(0,len(train_data))]\n\nt2=len(train_data)+np.array([*range(0, len(test_data))])\n\nplotly_line_plot([t1, t2], [train_data, test_data], title=\"Train-test Split: CO2 Emissions per month since 1973-01\", names=[\"Train Data\", \"Test Data\"])"
  },
  {
    "objectID": "deep_learning.html#co2-emissions-partition-plot",
    "href": "deep_learning.html#co2-emissions-partition-plot",
    "title": "Deep Learning for TS",
    "section": "CO2 Emissions Partition Plot",
    "text": "CO2 Emissions Partition Plot\n\n\nCode\n## Re-format data into required shape ##\n# PREPARE THE INPUT X AND TARGET Y\n\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind,X,Y_ind,Y #use for plotting later\n\n    # INDICES OF TARGET ARRAY\n\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps); #print(Y_ind); exit()\n    Y = dat[Y_ind]\n    # PREPARE X\n\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    del X_ind[::time_steps] # if time_steps=10 remove every 10th entry\n\n    X = dat[X_ind];\n    # PLOT\n\n    if(plot_data_partition):\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show();\n    # RESHAPE INTO KERAS FORMAT\n\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n    return X1, Y\n\n# PARTITION DATA\n\np=3  # simpilar to AR(p) given time_steps data points, predict time_steps+1 point (make prediction one month in future)\n\ntestX, testY = get_XY(test_data, p)\n\ntrainX, trainY = get_XY(train_data, p)\n#print(testX.shape,testY.shape)\n#print(trainX.shape,trainY.shape)\n#print(type(trainX))\n## Visualize ##\n## Build list\n\ntmp1=[]; tmp2=[]; tmp3=[]; count=0\n\nfor i in range(0,trainX.shape[0]):\n    # tmp1.append()\n    tmp1.append(count+np.array([*range(0,trainX[i,:,0].shape[0])]))\n    tmp1.append([count+trainX[i,:,0].shape[0]]); #print(([count+trainX[i,:,0].shape[0]]))\n    # tmp1.append([count+trainX[i,:,0].shape[0]+1])\n    tmp2.append(trainX[i,:,0])\n    tmp2.append([trainY[i]]); #print([trainY[i]])\n    # tmp2.append([trainY[i]])\n    count+=trainX[i,:,0].shape[0]+1\n\n    # print(i,trainX[i,:,0].shape)\n# print(tmp1)\n# print(tmp2)\n\ndef plotly_line_plot2(t, y, x_label=\"t: time (months)\", y_label=\"y(t): Million Metric Tons of Carbon Dioxide\", title=\"CO2 Emissions Partition Plot\"):\n    # GENERATE PLOTLY FIGURE\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=t[0], y=y[0]))\n    for i in range(1, len(y)):\n        if len(t[i]) == 1:\n            # print(t[i],y[i])\n            fig.add_trace(go.Scatter(x=t[i], y=y[i]))\n        else:\n            fig.add_trace(go.Scatter(x=t[i], y=y[i],))\n    fig.update_layout(\n        font=dict(\n            family=\"Palatino\",\n            size=14\n\n        ),\n        title=title,\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"seaborn\",\n        showlegend=True\n\n    )\n    fig.show()\n\nplotly_line_plot2(tmp1, tmp2, title=\"Train-test Split with Timestamp: CO2 Emissions per month since 1973-01\")"
  },
  {
    "objectID": "models2.html#forecast---sarima201x21112",
    "href": "models2.html#forecast---sarima201x21112",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecast - SARIMA(2,0,1)x(2,1,1)[12]",
    "text": "Forecast - SARIMA(2,0,1)x(2,1,1)[12]\nIn order to forecast CO2 Emissions variable, or the whole fit, we need to have forecasts of Renewable Energy, Petroleum, Natural gas, and Coal.\nHere we will be using auto.arima() to fit the Renewable Energy, Petroleum, Natural gas, and Coal variables.\n\nFitting SARIMA model to different energy consumption variables\n\nRenewable EnergyPetroleumNatural GasCoal\n\n\n\n\nCode\nRNWBL_fit &lt;- auto.arima(dd$Renewable) #fiting an ARIMA model to the CO2 emissions variable\nsummary(RNWBL_fit)\n\n\nSeries: dd$Renewable \nARIMA(0,1,2)(1,1,2)[12] \n\nCoefficients:\n          ma1      ma2    sar1     sma1    sma2\n      -0.2496  -0.1966  0.3548  -1.1398  0.2403\ns.e.   0.0408   0.0414  0.5676   0.5770  0.4817\n\nsigma^2 = 534.4:  log likelihood = -2681.06\nAIC=5374.11   AICc=5374.26   BIC=5400.36\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.2969799 22.76701 17.49176 -0.0738994 3.172679 0.4860948\n                    ACF1\nTraining set 0.006186365\n\n\n\n\nCode\nfRNWBL &lt;- forecast(RNWBL_fit, h=60)\n\n\n\n\n\n\nCode\nPTRLM_fit &lt;- auto.arima(dd$Petroleum) #fiting an ARIMA model to the petroleum variable\nsummary(PTRLM_fit)\n\n\nSeries: dd$Petroleum \nARIMA(3,0,2)(2,1,1)[12] \n\nCoefficients:\n          ar1     ar2     ar3     ma1     ma2    sar1     sar2     sma1\n      -0.0239  0.2107  0.6388  0.7031  0.4326  0.0213  -0.0886  -0.8043\ns.e.   0.1010  0.0813  0.1060  0.1306  0.1478  0.0558   0.0511   0.0355\n\nsigma^2 = 7153:  log likelihood = -3446.92\nAIC=6911.83   AICc=6912.14   BIC=6951.22\n\nTraining set error measures:\n                   ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set 1.419845 83.15279 59.65203 -0.02073471 2.058544 0.5540171\n                    ACF1\nTraining set -0.03071941\n\n\n\n\nCode\nfPTRLM &lt;- forecast(PTRLM_fit, h=60)\n\n\n\n\n\n\nCode\nNTRLG_fit &lt;- auto.arima(dd$`Natural Gas`) #fiting an ARIMA model to the petroleum variable\nsummary(NTRLG_fit)\n\n\nSeries: dd$`Natural Gas` \nARIMA(2,1,1)(1,1,2)[12] \n\nCoefficients:\n         ar1      ar2      ma1     sar1     sma1     sma2\n      0.4262  -0.0133  -0.9085  -0.4149  -0.2781  -0.4480\ns.e.  0.0475   0.0455   0.0236   0.1921   0.1801   0.1392\n\nsigma^2 = 9115:  log likelihood = -3512.73\nAIC=7039.46   AICc=7039.66   BIC=7070.09\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 6.345775 93.94888 69.62737 0.09510603 3.560609 0.6725354\n                     ACF1\nTraining set -0.005717725\n\n\n\n\nCode\nfNTRLG &lt;- forecast(NTRLG_fit, h=60)\n\n\n\n\n\n\nCode\nCL_fit &lt;- auto.arima(dd$Coal) #fiting an ARIMA model to the petroleum variable\nsummary(CL_fit)\n\n\nSeries: dd$Coal \nARIMA(2,1,2)(2,1,1)[12] \n\nCoefficients:\n          ar1     ar2      ma1      ma2    sar1     sar2     sma1\n      -0.1566  0.4381  -0.1342  -0.6282  0.0155  -0.1504  -0.7816\ns.e.   0.5999  0.2635   0.5924   0.4288  0.0520   0.0481   0.0331\n\nsigma^2 = 2982:  log likelihood = -3184.71\nAIC=6385.42   AICc=6385.67   BIC=6420.43\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -1.378761 53.69276 39.66915 -0.1088342 2.908254 0.5045871\n                     ACF1\nTraining set -0.001052996\n\n\n\n\nCode\nfCL &lt;- forecast(CL_fit,  h=60)\n\n\n\n\n\n\n\nCode\nfxreg &lt;- cbind(RNWBL = fRNWBL$mean,\n               PTRLM = fPTRLM$mean,\n               NTRLG = fNTRLG$mean,\n               CL = fCL$mean)\n\nfcast &lt;- forecast(fit, xreg=fxreg) #fimp$mean gives the forecasted values\ng &lt;- autoplot(fcast) + \n  xlab(\"Year\") +\n  ylab(\"Million Metric Tons of Carbon Dioxide\") +\n  ggtitle(\"CO2 Emissions Forecast from SARIMAX Model\")\nggplotly(g)"
  }
]